{"api": "2.0", "content": {"html": "<h1 id='0' style='font-size:22px'>Sequence to Sequence Learning<br>with Neural Networks</h1>\n<table id='1' style='font-size:18px'><thead></thead><tbody><tr><td>Ilya Sutskever</td><td>Oriol Vinyals</td><td>Quoc V. Le</td></tr><tr><td>Google</td><td>Google</td><td>Google</td></tr><tr><td>ilyasu@google .com</td><td>vinyal s@google · com</td><td>qvl @google.com</td></tr></tbody></table>\n<p id='2' data-category='paragraph' style='font-size:20px'>Abstract</p>\n<p id='3' data-category='paragraph' style='font-size:16px'>Deep Neural Networks (DNNs) are powerful models that have achieved excel-<br>lent performance on difficult learning tasks. Although DNNs work well whenever<br>large labeled training sets are available, they cannot be used to map sequences to<br>sequences. In this paper, we present a general end-to-end approach to sequence<br>learning that makes minimal assumptions on the sequence structure. Our method<br>uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence<br>to a vector of a fixed dimensionality, and then another deep LSTM to decode the<br>target sequence from the vector. Our main result is that on an English to French<br>translation task from the WMT' 14 dataset, the translations produced by the LSTM<br>achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU<br>score was penalized on out-of-vocabulary words. Additionally, the LSTM did not<br>have difficulty on long sentences. For comparison, a phrase-based SMT system<br>achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM<br>to rerank the 1000 hypotheses produced by the aforementioned SMT system, its<br>BLEU score increases to 36.5, which is close to the previous best result on this<br>task. The LSTM also learned sensible phrase and sentence representations that<br>are sensitive to word order and are relatively invariant to the active and the pas-<br>sive voice. Finally, we found that reversing the order of the words in all source<br>sentences (but not target sentences) improved the LSTM's performance markedly,<br>because doing so introduced many short term dependencies between the source<br>and the target sentence which made the optimization problem easier.</p>\n<p id='4' data-category='paragraph' style='font-size:18px'>1 Introduction</p>\n<p id='5' data-category='paragraph' style='font-size:16px'>Deep Neural Networks (DNNs) are extremely powerful machine learning models that achieve ex-<br>cellent performance on difficult problems such as speech recognition [13, 7] and visual object recog-<br>nition [19, 6, 21, 20]. DNNs are powerful because they can perform arbitrary parallel computation<br>for a modest number of steps. A surprising example of the power of DNNs is their ability to sort<br>N N-bit numbers using only 2 hidden layers of quadratic size [27]. So, while neural networks are<br>related to conventional statistical models, they learn an intricate computation. Furthermore, large<br>DNNs can be trained with supervised backpropagation whenever the labeled training set has enough<br>information to specify the network's parameters. Thus, if there exists a parameter setting of a large<br>DNN that achieves good results (for example, because humans can solve the task very rapidly),<br>supervised backpropagation will find these parameters and solve the problem.</p>\n<br><p id='6' data-category='paragraph' style='font-size:16px'>Despite their flexibility and power, DNNs can only be applied to problems whose inputs and targets<br>can be sensibly encoded with vectors of fixed dimensionality. It is a significant limitation, since<br>many important problems are best expressed with sequences whose lengths are not known a-priori.<br>For example, speech recognition and machine translation are sequential problems. Likewise, ques-<br>tion answering can also be seen as mapping a sequence of words representing the question to a</p>\n<br><header id='7' style='font-size:14px'>2014<br>Dec<br>14<br>[cs.CL]<br>arXiv:1409.3215v3</header>\n<footer id='8' style='font-size:14px'>1</footer>\n<p id='9' data-category='paragraph' style='font-size:16px'>sequence of words representing the answer. It is therefore clear that a domain-independent method<br>that learns to map sequences to sequences would be useful.</p>\n<br><p id='10' data-category='paragraph' style='font-size:20px'>Sequences pose a challenge for DNNs because they require that the dimensionality of the inputs and<br>outputs is known and fixed. In this paper, we show that a straightforward application of the Long<br>Short-Term Memory (LSTM) architecture [16] can solve general sequence to sequence problems.<br>The idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large fixed-<br>dimensional vector representation, and then to use another LSTM to extract the output sequence<br>from that vector (fig. 1). The second LSTM is essentially a recurrent neural network language model<br>[28, 23, 30] except that it is conditioned on the input sequence. The LSTM's ability to successfully<br>learn on data with long range temporal dependencies makes it a natural choice for this application<br>due to the considerable time lag between the inputs and their corresponding outputs (fig. 1).</p>\n<br><p id='11' data-category='paragraph' style='font-size:20px'>There have been a number of related attempts to address the general sequence to sequence learning<br>problem with neural networks. Our approach is closely related to Kalchbrenner and Blunsom [18]<br>who were the first to map the entire input sentence to vector, and is related to Cho et al. [5] although<br>the latter was used only for rescoring hypotheses produced by a phrase-based system. Graves [10]<br>introduced a novel differentiable attention mechanism that allows neural networks to focus on dif-<br>ferent parts of their input, and an elegant variant of this idea was successfully applied to machine<br>translation by Bahdanau et al. [2]. The Connectionist Sequence Classification is another popular<br>technique for mapping sequences to sequences with neural networks, but it assumes a monotonic<br>alignment between the inputs and the outputs [1 1].</p>\n<figure id='12'><img style='font-size:16px' alt=\"W X Y Z <EOS>\nT T ↑\nA B C <EOS> w X Y Z\" data-coord=\"top-left:(257,674); bottom-right:(1019,837)\" /></figure>\n<p id='13' data-category='paragraph' style='font-size:14px'>Figure 1: Our model reads an input sentence \"ABC\" and produces \"WXYZ\" as the output sentence. The<br>model stops making predictions after outputting the end-of-sentence token. Note that the LSTM reads the<br>input sentence in reverse, because doing so introduces many short term dependencies in the data that make the<br>optimization problem much easier.</p>\n<p id='14' data-category='paragraph' style='font-size:20px'>The main result of this work is the following. On the WMT' 14 English to French translation task,<br>we obtained a BLEU score of 34.81 by directly extracting translations from an ensemble of 5 deep<br>LSTMs (with 384M parameters and 8,000 dimensional state each) using a simple left-to-right beam-<br>search decoder. This is by far the best result achieved by direct translation with large neural net-<br>works. For comparison, the BLEU score of an SMT baseline on this dataset is 33.30 [29]. The 34.81<br>BLEU score was achieved by an LSTM with a vocabulary of 80k words, so the score was penalized<br>whenever the reference translation contained a word not covered by these 80k. This result shows<br>that a relatively unoptimized small-vocabulary neural network architecture which has much room<br>for improvement outperforms a phrase-based SMT system.</p>\n<br><p id='15' data-category='paragraph' style='font-size:20px'>Finally, we used the LSTM to rescore the publicly available 1000-best lists of the SMT baseline on<br>the same task [29]. By doing so, we obtained a BLEU score of 36.5, which improves the baseline by<br>3.2 BLEU points and is close to the previous best published result on this task (which is 37.0 [9]).</p>\n<br><p id='16' data-category='paragraph' style='font-size:20px'>Surprisingly, the LSTM did not suffer on very long sentences, despite the recent experience of other<br>researchers with related architectures [26]. We were able to do well on long sentences because we<br>reversed the order of words in the source sentence but not the target sentences in the training and test<br>set. By doing so, we introduced many short term dependencies that made the optimization problem<br>much simpler (see sec. 2 and 3.3). As a result, SGD could learn LSTMs that had no trouble with<br>long sentences. The simple trick of reversing the words in the source sentence is one of the key<br>technical contributions of this work.</p>\n<p id='17' data-category='paragraph' style='font-size:16px'>A useful property of the LSTM is that it learns to map an input sentence of variable length into<br>a fixed-dimensional vector representation. Given that translations tend to be paraphrases of the<br>source sentences, the translation objective encourages the LSTM to find sentence representations<br>that capture their meaning, as sentences with similar meanings are close to each other while different</p>\n<footer id='18' style='font-size:20px'>2</footer>\n<p id='19' data-category='paragraph' style='font-size:14px'>sentences meanings will be far. A qualitative evaluation supports this claim, showing that our model<br>is aware of word order and is fairly invariant to the active and passive voice.</p>\n<p id='20' data-category='paragraph' style='font-size:18px'>2 The model</p>\n<p id='21' data-category='paragraph' style='font-size:14px'>The Recurrent Neural Network (RNN) [31, 28] is a natural generalization of feedforward neural<br>networks to sequences. Given a sequence of inputs (x1, . · .,XT), a standard RNN computes a<br>sequence of outputs (y1,· · . , YT) by iterating the following equation:</p>\n<p id='22' data-category='equation'>$$\\begin{array}{l l l}{{h_{t}}}&{{=}}&{{\\mathrm{sigm}\\left(W^{\\mathrm{hx}}x_{t}+W^{\\mathrm{hh}}h_{t-1}\\right)}}\\\\ {{y_{t}}}&{{=}}&{{W^{\\mathrm{yh}}h_{t}}}\\end{array}$$</p>\n<br><p id='23' data-category='paragraph' style='font-size:14px'>The RNN can easily map sequences to sequences whenever the alignment between the inputs the<br>outputs is known ahead of time. However, it is not clear how to apply an RNN to problems whose<br>input and the output sequences have different lengths with complicated and non-monotonic relation-<br>ships.</p>\n<br><p id='24' data-category='paragraph' style='font-size:14px'>The simplest strategy for general sequence learning is to map the input sequence to a fixed-sized<br>vector using one RNN, and then to map the vector to the target sequence with another RNN (this<br>approach has also been taken by Cho et al. [5]). While it could work in principle since the RNN is<br>provided with all the relevant information, it would be difficult to train the RNNs due to the resulting<br>long term dependencies (figure 1) [14, 4, 16, 15]. However, the Long Short-Term Memory (LSTM)<br>[16] is known to learn problems with long range temporal dependencies, so an LSTM may succeed<br>in this setting.</p>\n<p id='25' data-category='paragraph' style='font-size:14px'>The goal of the LSTM is to estimate the conditional probability p(y1, . : , YT' x1,···, xT) where<br>(x1, · · · , xT) is an input sequence and Y1, · · · , YT' is its corresponding output sequence whose length<br>T' may differ from T. The LSTM computes this conditional probability by first obtaining the fixed-<br>dimensional representation v of the input sequence (x1,.. · , xT) given by the last hidden state of the<br>LSTM, and then computing the probability of y1, · · · , YT' with a standard LSTM-LM formulation<br>whose initial hidden state is set to the representation v of x1, · · · , XT:</p>\n<p id='26' data-category='equation'>$$p(y_{1},\\cdot\\cdot\\cdot,y_{T^{\\prime}}|x_{1},\\cdot\\cdot\\cdot,x_{T})=\\prod_{t=1}^{T^{\\prime}}p(y_{t}|v,y_{1},\\cdot\\cdot\\cdot,y_{t-1})$$</p>\n<br><caption id='27' style='font-size:20px'>(1)</caption>\n<p id='28' data-category='paragraph' style='font-size:14px'>In this equation, each p(yt|v, y1, · · · , yt-1) distribution is represented with a softmax over all the<br>words in the vocabulary. We use the LSTM formulation from Graves [10]. Note that we require that<br>each sentence ends with a special end-of-sentence symbol \"<EOS>\", which enables the model to<br>define a distribution over sequences of all possible lengths. The overall scheme is outlined in figure<br>· <EOS>\" and then uses<br>1, where the shown LSTM computes the representation of \"A\", \"B\" , \"C\" ,<br>this representation to compute the probability of \"W\", \"X\", \"Y\" \"Z\" , <EOS>\"<br>\"<br>,</p>\n<br><p id='29' data-category='paragraph' style='font-size:14px'>Our actual models differ from the above description in three important ways. First, we used two<br>different LSTMs: one for the input sequence and another for the output sequence, because doing<br>so increases the number model parameters at negligible computational cost and makes it natural to<br>train the LSTM on multiple language pairs simultaneously [18]. Second, we found that deep LSTMs<br>significantly outperformed shallow LSTMs, so we chose an LSTM with four layers. Third, we found<br>it extremely valuable to reverse the order of the words of the input sentence. So for example, instead<br>of mapping the sentence a, b, c to the sentence �, �, % the LSTM is asked to map c, b, a to �, �, %<br>where �, �, づ is the translation of a, b, c. This way, a is in close proximity to �, bis fairly close to �,<br>and so on, a fact that makes it easy for SGD to \"establish communication\" between the input and the<br>output. We found this simple data transformation to greatly improve the performance of the LSTM.</p>\n<p id='30' data-category='paragraph' style='font-size:22px'>3 Experiments</p>\n<p id='31' data-category='paragraph' style='font-size:14px'>We applied our method to the WMT' 14 English to French MT task in two ways. We used it to<br>directly translate the input sentence without using a reference SMT system and we it to rescore the<br>n-best lists of an SMT baseline. We report the accuracy of these translation methods, present sample<br>translations, and visualize the resulting sentence representation.</p>\n<footer id='32' style='font-size:14px'>3</footer>\n<p id='33' data-category='paragraph' style='font-size:14px'>3.1 Dataset details</p>\n<p id='34' data-category='paragraph' style='font-size:14px'>We used the WMT' 14 English to French dataset. We trained our models on a subset of 12M sen-<br>tences consisting of 348M French words and 304M English words, which is a clean \"selected\"<br>subset from [29]. We chose this translation task and this specific training set subset because of the<br>public availability of a tokenized training and test set together with 1000-best lists from the baseline<br>SMT [29].</p>\n<p id='35' data-category='paragraph' style='font-size:14px'>As typical neural language models rely on a vector representation for each word, we used a fixed<br>vocabulary for both languages. We used 160,000 of the most frequent words for the source language<br>and 80,000 of the most frequent words for the target language. Every out-of-vocabulary word was<br>replaced with a special \"UNK\" token.</p>\n<p id='36' data-category='paragraph' style='font-size:18px'>3.2 Decoding and Rescoring</p>\n<p id='37' data-category='paragraph' style='font-size:14px'>The core of our experiments involved training a large deep LSTM on many sentence pairs. We<br>trained it by maximizing the log probability of a correct translation T given the source sentence S,<br>so the training objective is</p>\n<br><p id='38' data-category='equation'>$$\\textstyle{1/|{\\cal G}|}_{(T,S)\\in{\\cal S}}\\log p(T|S)$$</p>\n<br><p id='39' data-category='paragraph' style='font-size:16px'>where s is the training set. Once training is complete, we produce translations by finding the most<br>likely translation according to the LSTM:</p>\n<p id='40' data-category='equation'>$${\\hat{T}}=\\arg\\operatorname*{max}_{T}p(T|S)$$</p>\n<br><caption id='41' style='font-size:20px'>(2)</caption>\n<p id='42' data-category='paragraph' style='font-size:14px'>We search for the most likely translation using a simple left-to-right beam search decoder which<br>maintains a small number B of partial hypotheses, where a partial hypothesis is a prefix of some<br>translation. At each timestep we extend each partial hypothesis in the beam with every possible<br>word in the vocabulary. This greatly increases the number of the hypotheses so we discard all but<br>the B most likely hypotheses according to the model's log probability. As soon as the \"<EOS>\"<br>symbol is appended to a hypothesis, it is removed from the beam and is added to the set of complete<br>hypotheses. While this decoder is approximate, it is simple to implement. Interestingly, our system<br>performs well even with a beam size of 1, and a beam of size 2 provides most of the benefits of beam<br>search (Table 1).</p>\n<br><p id='43' data-category='paragraph' style='font-size:14px'>We also used the LSTM to rescore the 1000-best lists produced by the baseline system [29]. To<br>rescore an n-best list, we computed the log probability of every hypothesis with our LSTM and took<br>an even average with their score and the LSTM's score.</p>\n<h1 id='44' style='font-size:16px'>3.3 Reversing the Source Sentences</h1>\n<p id='45' data-category='paragraph' style='font-size:14px'>While the LSTM is capable of solving problems with long term dependencies, we discovered that<br>the LSTM learns much better when the source sentences are reversed (the target sentences are not<br>reversed). By doing so, the LSTM's test perplexity dropped from 5.8 to 4.7, and the test BLEU<br>scores of its decoded translations increased from 25.9 to 30.6.</p>\n<p id='46' data-category='paragraph' style='font-size:14px'>While we do not have a complete explanation to this phenomenon, we believe that it is caused by<br>the introduction of many short term dependencies to the dataset. Normally, when we concatenate a<br>source sentence with a target sentence, each word in the source sentence is far from its corresponding<br>word in the target sentence. As a result, the problem has a large \"minimal time lag\" [17]. By<br>reversing the words in the source sentence, the average distance between corresponding words in<br>the source and target language is unchanged. However, the first few words in the source language<br>are now very close to the first few words in the target language, so the problem's minimal time lag is<br>greatly reduced. Thus, backpropagation has an easier time \"establishing communication\" between<br>the source sentence and the target sentence, which in turn results in substantially improved overall<br>performance.</p>\n<p id='47' data-category='paragraph' style='font-size:14px'>Initially, we believed that reversing the input sentences would only lead to more confident predic-<br>tions in the early parts of the target sentence and to less confident predictions in the later parts. How-<br>ever, LSTMs trained on reversed source sentences did much better on long sentences than LSTMs</p>\n<footer id='48' style='font-size:14px'>4</footer>\n<p id='49' data-category='paragraph' style='font-size:16px'>trained on the raw source sentences (see sec. 3.7), which suggests that reversing the input sentences<br>results in LSTMs with better memory utilization.</p>\n<p id='50' data-category='paragraph' style='font-size:22px'>3.4 Training details</p>\n<p id='51' data-category='paragraph' style='font-size:18px'>We found that the LSTM models are fairly easy to train. We used deep LSTMs with 4 layers,<br>with 1000 cells at each layer and 1000 dimensional word embeddings, with an input vocabulary<br>of 160,000 and an output vocabulary of 80,000. Thus the deep LSTM uses 8000 real numbers to<br>represent a sentence. We found deep LSTMs to significantly outperform shallow LSTMs, where<br>each additional layer reduced perplexity by nearly 10%, possibly due to their much larger hidden<br>state. We used a naive softmax over 80,000 words at each output. The resulting LSTM has 384M<br>parameters of which 64M are pure recurrent connections (32M for the \"encoder\" LSTM and 32M<br>for the \"decoder\" LSTM). The complete training details are given below:</p>\n<p id='52' data-category='list' style='font-size:18px'>· We initialized all of the LSTM's parameters with the uniform distribution between -0.08<br>and 0.08<br>· We used stochastic gradient descent without momentum, with a fixed learning rate of 0.7.<br>After 5 epochs, we begun halving the learning rate every half epoch. We trained our models<br>for a total of 7.5 epochs.<br>· We used batches of 128 sequences for the gradient and divided it the size of the batch<br>(namely, 128).<br>· Although LSTMs tend to not suffer from the vanishing gradient problem, they can have<br>exploding gradients. Thus we enforced a hard constraint on the norm of the gradient [10,<br>25] by scaling it when its norm exceeded a threshold. For each training batch, we compute<br>s = 1191|2, where g is the gradient divided by 128. If s > 5, we set g = 5g.<br>s<br>· Different sentences have different lengths. Most sentences are short (e.g., length 20-30)<br>but some sentences are long (e.g., length > 100), so a minibatch of 128 randomly chosen<br>training sentences will have many short sentences and few long sentences, and as a result,<br>much of the computation in the minibatch is wasted. To address this problem, we made sure<br>that all sentences in a minibatch are roughly of the same length, yielding a 2x speedup.</p>\n<h1 id='53' style='font-size:20px'>3.5 Parallelization</h1>\n<p id='54' data-category='paragraph' style='font-size:16px'>A C++ implementation of deep LSTM with the configuration from the previous section on a sin-<br>gle GPU processes a speed of approximately 1,700 words per second. This was too slow for our<br>purposes, so we parallelized our model using an 8-GPU machine. Each layer of the LSTM was<br>executed on a different GPU and communicated its activations to the next GPU / layer as soon as<br>they were computed. Our models have 4 layers of LSTMs, each of which resides on a separate<br>GPU. The remaining 4 GPUs were used to parallelize the softmax, so each GPU was responsible<br>for multiplying by a 1000 x 20000 matrix. The resulting implementation achieved a speed of 6,300<br>(both English and French) words per second with a minibatch size of 128. Training took about a ten<br>days with this implementation.</p>\n<p id='55' data-category='paragraph' style='font-size:22px'>3.6 Experimental Results</p>\n<p id='56' data-category='paragraph' style='font-size:16px'>We used the cased BLEU score [24] to evaluate the quality of our translations. We computed our<br>BLEU scores using multi-bleu · pl 1 the tokenized predictions and ground truth. This way<br>on<br>of evaluating the BELU score is consistent with [5] and [2], and reproduces the 33.3 score of [29].<br>However, if we evaluate the best WMT' 14 system [9] (whose predictions can be downloaded from<br>statmt · org \\matrix) in this manner, we get 37.0, which is greater than the 35.8 reported by<br>statmt · org \\matrix.</p>\n<br><p id='57' data-category='paragraph' style='font-size:16px'>The results are presented in tables 1 and 2. Our best results are obtained with an ensemble of LSTMs<br>that differ in their random initializations and in the random order of minibatches. While the decoded<br>translations of the LSTM ensemble do not outperform the best WMT' 14 system, it is the first time<br>that a pure neural translation system outperforms a phrase-based SMT baseline on a large scale MT</p>\n<p id='58' data-category='footnote' style='font-size:14px'>1There several variants of the BLEU score, and each variant is defined with a perl script.</p>\n<footer id='59' style='font-size:18px'>5</footer>", "markdown": "# Sequence to Sequence Learning\nwith Neural Networks\n\n| Ilya Sutskever | Oriol Vinyals | Quoc V. Le |\n| --- | --- | --- |\n| Google | Google | Google |\n| ilyasu@google .com | vinyal s@google · com | qvl @google.com |\n\n\nAbstract\n\nDeep Neural Networks (DNNs) are powerful models that have achieved excel-\nlent performance on difficult learning tasks. Although DNNs work well whenever\nlarge labeled training sets are available, they cannot be used to map sequences to\nsequences. In this paper, we present a general end-to-end approach to sequence\nlearning that makes minimal assumptions on the sequence structure. Our method\nuses a multilayered Long Short-Term Memory (LSTM) to map the input sequence\nto a vector of a fixed dimensionality, and then another deep LSTM to decode the\ntarget sequence from the vector. Our main result is that on an English to French\ntranslation task from the WMT' 14 dataset, the translations produced by the LSTM\nachieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU\nscore was penalized on out-of-vocabulary words. Additionally, the LSTM did not\nhave difficulty on long sentences. For comparison, a phrase-based SMT system\nachieves a BLEU score of 33.3 on the same dataset. When we used the LSTM\nto rerank the 1000 hypotheses produced by the aforementioned SMT system, its\nBLEU score increases to 36.5, which is close to the previous best result on this\ntask. The LSTM also learned sensible phrase and sentence representations that\nare sensitive to word order and are relatively invariant to the active and the pas-\nsive voice. Finally, we found that reversing the order of the words in all source\nsentences (but not target sentences) improved the LSTM's performance markedly,\nbecause doing so introduced many short term dependencies between the source\nand the target sentence which made the optimization problem easier.\n\n1 Introduction\n\nDeep Neural Networks (DNNs) are extremely powerful machine learning models that achieve ex-\ncellent performance on difficult problems such as speech recognition [13, 7] and visual object recog-\nnition [19, 6, 21, 20]. DNNs are powerful because they can perform arbitrary parallel computation\nfor a modest number of steps. A surprising example of the power of DNNs is their ability to sort\nN N-bit numbers using only 2 hidden layers of quadratic size [27]. So, while neural networks are\nrelated to conventional statistical models, they learn an intricate computation. Furthermore, large\nDNNs can be trained with supervised backpropagation whenever the labeled training set has enough\ninformation to specify the network's parameters. Thus, if there exists a parameter setting of a large\nDNN that achieves good results (for example, because humans can solve the task very rapidly),\nsupervised backpropagation will find these parameters and solve the problem.\n\nDespite their flexibility and power, DNNs can only be applied to problems whose inputs and targets\ncan be sensibly encoded with vectors of fixed dimensionality. It is a significant limitation, since\nmany important problems are best expressed with sequences whose lengths are not known a-priori.\nFor example, speech recognition and machine translation are sequential problems. Likewise, ques-\ntion answering can also be seen as mapping a sequence of words representing the question to a\n\n2014\nDec\n14\n[cs.CL]\narXiv:1409.3215v3\n\n1\n\nsequence of words representing the answer. It is therefore clear that a domain-independent method\nthat learns to map sequences to sequences would be useful.\n\nSequences pose a challenge for DNNs because they require that the dimensionality of the inputs and\noutputs is known and fixed. In this paper, we show that a straightforward application of the Long\nShort-Term Memory (LSTM) architecture [16] can solve general sequence to sequence problems.\nThe idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large fixed-\ndimensional vector representation, and then to use another LSTM to extract the output sequence\nfrom that vector (fig. 1). The second LSTM is essentially a recurrent neural network language model\n[28, 23, 30] except that it is conditioned on the input sequence. The LSTM's ability to successfully\nlearn on data with long range temporal dependencies makes it a natural choice for this application\ndue to the considerable time lag between the inputs and their corresponding outputs (fig. 1).\n\nThere have been a number of related attempts to address the general sequence to sequence learning\nproblem with neural networks. Our approach is closely related to Kalchbrenner and Blunsom [18]\nwho were the first to map the entire input sentence to vector, and is related to Cho et al. [5] although\nthe latter was used only for rescoring hypotheses produced by a phrase-based system. Graves [10]\nintroduced a novel differentiable attention mechanism that allows neural networks to focus on dif-\nferent parts of their input, and an elegant variant of this idea was successfully applied to machine\ntranslation by Bahdanau et al. [2]. The Connectionist Sequence Classification is another popular\ntechnique for mapping sequences to sequences with neural networks, but it assumes a monotonic\nalignment between the inputs and the outputs [1 1].\n\n![image](/image/placeholder)\nW X Y Z <EOS>\nT T ↑\nA B C <EOS> w X Y Z\n\nFigure 1: Our model reads an input sentence \"ABC\" and produces \"WXYZ\" as the output sentence. The\nmodel stops making predictions after outputting the end-of-sentence token. Note that the LSTM reads the\ninput sentence in reverse, because doing so introduces many short term dependencies in the data that make the\noptimization problem much easier.\n\nThe main result of this work is the following. On the WMT' 14 English to French translation task,\nwe obtained a BLEU score of 34.81 by directly extracting translations from an ensemble of 5 deep\nLSTMs (with 384M parameters and 8,000 dimensional state each) using a simple left-to-right beam-\nsearch decoder. This is by far the best result achieved by direct translation with large neural net-\nworks. For comparison, the BLEU score of an SMT baseline on this dataset is 33.30 [29]. The 34.81\nBLEU score was achieved by an LSTM with a vocabulary of 80k words, so the score was penalized\nwhenever the reference translation contained a word not covered by these 80k. This result shows\nthat a relatively unoptimized small-vocabulary neural network architecture which has much room\nfor improvement outperforms a phrase-based SMT system.\n\nFinally, we used the LSTM to rescore the publicly available 1000-best lists of the SMT baseline on\nthe same task [29]. By doing so, we obtained a BLEU score of 36.5, which improves the baseline by\n3.2 BLEU points and is close to the previous best published result on this task (which is 37.0 [9]).\n\nSurprisingly, the LSTM did not suffer on very long sentences, despite the recent experience of other\nresearchers with related architectures [26]. We were able to do well on long sentences because we\nreversed the order of words in the source sentence but not the target sentences in the training and test\nset. By doing so, we introduced many short term dependencies that made the optimization problem\nmuch simpler (see sec. 2 and 3.3). As a result, SGD could learn LSTMs that had no trouble with\nlong sentences. The simple trick of reversing the words in the source sentence is one of the key\ntechnical contributions of this work.\n\nA useful property of the LSTM is that it learns to map an input sentence of variable length into\na fixed-dimensional vector representation. Given that translations tend to be paraphrases of the\nsource sentences, the translation objective encourages the LSTM to find sentence representations\nthat capture their meaning, as sentences with similar meanings are close to each other while different\n\n2\n\nsentences meanings will be far. A qualitative evaluation supports this claim, showing that our model\nis aware of word order and is fairly invariant to the active and passive voice.\n\n2 The model\n\nThe Recurrent Neural Network (RNN) [31, 28] is a natural generalization of feedforward neural\nnetworks to sequences. Given a sequence of inputs (x1, . · .,XT), a standard RNN computes a\nsequence of outputs (y1,· · . , YT) by iterating the following equation:\n\n$$\\begin{array}{l l l}{{h_{t}}}&{{=}}&{{\\mathrm{sigm}\\left(W^{\\mathrm{hx}}x_{t}+W^{\\mathrm{hh}}h_{t-1}\\right)}}\\\\ {{y_{t}}}&{{=}}&{{W^{\\mathrm{yh}}h_{t}}}\\end{array}$$\n\nThe RNN can easily map sequences to sequences whenever the alignment between the inputs the\noutputs is known ahead of time. However, it is not clear how to apply an RNN to problems whose\ninput and the output sequences have different lengths with complicated and non-monotonic relation-\nships.\n\nThe simplest strategy for general sequence learning is to map the input sequence to a fixed-sized\nvector using one RNN, and then to map the vector to the target sequence with another RNN (this\napproach has also been taken by Cho et al. [5]). While it could work in principle since the RNN is\nprovided with all the relevant information, it would be difficult to train the RNNs due to the resulting\nlong term dependencies (figure 1) [14, 4, 16, 15]. However, the Long Short-Term Memory (LSTM)\n[16] is known to learn problems with long range temporal dependencies, so an LSTM may succeed\nin this setting.\n\nThe goal of the LSTM is to estimate the conditional probability p(y1, . : , YT' x1,···, xT) where\n(x1, · · · , xT) is an input sequence and Y1, · · · , YT' is its corresponding output sequence whose length\nT' may differ from T. The LSTM computes this conditional probability by first obtaining the fixed-\ndimensional representation v of the input sequence (x1,.. · , xT) given by the last hidden state of the\nLSTM, and then computing the probability of y1, · · · , YT' with a standard LSTM-LM formulation\nwhose initial hidden state is set to the representation v of x1, · · · , XT:\n\n$$p(y_{1},\\cdot\\cdot\\cdot,y_{T^{\\prime}}|x_{1},\\cdot\\cdot\\cdot,x_{T})=\\prod_{t=1}^{T^{\\prime}}p(y_{t}|v,y_{1},\\cdot\\cdot\\cdot,y_{t-1})$$\n\n(1)\n\nIn this equation, each p(yt|v, y1, · · · , yt-1) distribution is represented with a softmax over all the\nwords in the vocabulary. We use the LSTM formulation from Graves [10]. Note that we require that\neach sentence ends with a special end-of-sentence symbol \"<EOS>\", which enables the model to\ndefine a distribution over sequences of all possible lengths. The overall scheme is outlined in figure\n· <EOS>\" and then uses\n1, where the shown LSTM computes the representation of \"A\", \"B\" , \"C\" ,\nthis representation to compute the probability of \"W\", \"X\", \"Y\" \"Z\" , <EOS>\"\n\"\n,\n\nOur actual models differ from the above description in three important ways. First, we used two\ndifferent LSTMs: one for the input sequence and another for the output sequence, because doing\nso increases the number model parameters at negligible computational cost and makes it natural to\ntrain the LSTM on multiple language pairs simultaneously [18]. Second, we found that deep LSTMs\nsignificantly outperformed shallow LSTMs, so we chose an LSTM with four layers. Third, we found\nit extremely valuable to reverse the order of the words of the input sentence. So for example, instead\nof mapping the sentence a, b, c to the sentence �, �, % the LSTM is asked to map c, b, a to �, �, %\nwhere �, �, づ is the translation of a, b, c. This way, a is in close proximity to �, bis fairly close to �,\nand so on, a fact that makes it easy for SGD to \"establish communication\" between the input and the\noutput. We found this simple data transformation to greatly improve the performance of the LSTM.\n\n3 Experiments\n\nWe applied our method to the WMT' 14 English to French MT task in two ways. We used it to\ndirectly translate the input sentence without using a reference SMT system and we it to rescore the\nn-best lists of an SMT baseline. We report the accuracy of these translation methods, present sample\ntranslations, and visualize the resulting sentence representation.\n\n3\n\n3.1 Dataset details\n\nWe used the WMT' 14 English to French dataset. We trained our models on a subset of 12M sen-\ntences consisting of 348M French words and 304M English words, which is a clean \"selected\"\nsubset from [29]. We chose this translation task and this specific training set subset because of the\npublic availability of a tokenized training and test set together with 1000-best lists from the baseline\nSMT [29].\n\nAs typical neural language models rely on a vector representation for each word, we used a fixed\nvocabulary for both languages. We used 160,000 of the most frequent words for the source language\nand 80,000 of the most frequent words for the target language. Every out-of-vocabulary word was\nreplaced with a special \"UNK\" token.\n\n3.2 Decoding and Rescoring\n\nThe core of our experiments involved training a large deep LSTM on many sentence pairs. We\ntrained it by maximizing the log probability of a correct translation T given the source sentence S,\nso the training objective is\n\n$$\\textstyle{1/|{\\cal G}|}_{(T,S)\\in{\\cal S}}\\log p(T|S)$$\n\nwhere s is the training set. Once training is complete, we produce translations by finding the most\nlikely translation according to the LSTM:\n\n$${\\hat{T}}=\\arg\\operatorname*{max}_{T}p(T|S)$$\n\n(2)\n\nWe search for the most likely translation using a simple left-to-right beam search decoder which\nmaintains a small number B of partial hypotheses, where a partial hypothesis is a prefix of some\ntranslation. At each timestep we extend each partial hypothesis in the beam with every possible\nword in the vocabulary. This greatly increases the number of the hypotheses so we discard all but\nthe B most likely hypotheses according to the model's log probability. As soon as the \"<EOS>\"\nsymbol is appended to a hypothesis, it is removed from the beam and is added to the set of complete\nhypotheses. While this decoder is approximate, it is simple to implement. Interestingly, our system\nperforms well even with a beam size of 1, and a beam of size 2 provides most of the benefits of beam\nsearch (Table 1).\n\nWe also used the LSTM to rescore the 1000-best lists produced by the baseline system [29]. To\nrescore an n-best list, we computed the log probability of every hypothesis with our LSTM and took\nan even average with their score and the LSTM's score.\n\n# 3.3 Reversing the Source Sentences\n\nWhile the LSTM is capable of solving problems with long term dependencies, we discovered that\nthe LSTM learns much better when the source sentences are reversed (the target sentences are not\nreversed). By doing so, the LSTM's test perplexity dropped from 5.8 to 4.7, and the test BLEU\nscores of its decoded translations increased from 25.9 to 30.6.\n\nWhile we do not have a complete explanation to this phenomenon, we believe that it is caused by\nthe introduction of many short term dependencies to the dataset. Normally, when we concatenate a\nsource sentence with a target sentence, each word in the source sentence is far from its corresponding\nword in the target sentence. As a result, the problem has a large \"minimal time lag\" [17]. By\nreversing the words in the source sentence, the average distance between corresponding words in\nthe source and target language is unchanged. However, the first few words in the source language\nare now very close to the first few words in the target language, so the problem's minimal time lag is\ngreatly reduced. Thus, backpropagation has an easier time \"establishing communication\" between\nthe source sentence and the target sentence, which in turn results in substantially improved overall\nperformance.\n\nInitially, we believed that reversing the input sentences would only lead to more confident predic-\ntions in the early parts of the target sentence and to less confident predictions in the later parts. How-\never, LSTMs trained on reversed source sentences did much better on long sentences than LSTMs\n\n4\n\ntrained on the raw source sentences (see sec. 3.7), which suggests that reversing the input sentences\nresults in LSTMs with better memory utilization.\n\n3.4 Training details\n\nWe found that the LSTM models are fairly easy to train. We used deep LSTMs with 4 layers,\nwith 1000 cells at each layer and 1000 dimensional word embeddings, with an input vocabulary\nof 160,000 and an output vocabulary of 80,000. Thus the deep LSTM uses 8000 real numbers to\nrepresent a sentence. We found deep LSTMs to significantly outperform shallow LSTMs, where\neach additional layer reduced perplexity by nearly 10%, possibly due to their much larger hidden\nstate. We used a naive softmax over 80,000 words at each output. The resulting LSTM has 384M\nparameters of which 64M are pure recurrent connections (32M for the \"encoder\" LSTM and 32M\nfor the \"decoder\" LSTM). The complete training details are given below:\n\n- · We initialized all of the LSTM's parameters with the uniform distribution between -0.08\n- and 0.08\n- · We used stochastic gradient descent without momentum, with a fixed learning rate of 0.7.\n- After 5 epochs, we begun halving the learning rate every half epoch. We trained our models\n- for a total of 7.5 epochs.\n- · We used batches of 128 sequences for the gradient and divided it the size of the batch\n- (namely, 128).\n- · Although LSTMs tend to not suffer from the vanishing gradient problem, they can have\n- exploding gradients. Thus we enforced a hard constraint on the norm of the gradient [10,\n- 25] by scaling it when its norm exceeded a threshold. For each training batch, we compute\n- s = 1191|2, where g is the gradient divided by 128. If s > 5, we set g = 5g.\n- s\n- · Different sentences have different lengths. Most sentences are short (e.g., length 20-30)\n- but some sentences are long (e.g., length > 100), so a minibatch of 128 randomly chosen\n- training sentences will have many short sentences and few long sentences, and as a result,\n- much of the computation in the minibatch is wasted. To address this problem, we made sure\n- that all sentences in a minibatch are roughly of the same length, yielding a 2x speedup.\n\n\n# 3.5 Parallelization\n\nA C++ implementation of deep LSTM with the configuration from the previous section on a sin-\ngle GPU processes a speed of approximately 1,700 words per second. This was too slow for our\npurposes, so we parallelized our model using an 8-GPU machine. Each layer of the LSTM was\nexecuted on a different GPU and communicated its activations to the next GPU / layer as soon as\nthey were computed. Our models have 4 layers of LSTMs, each of which resides on a separate\nGPU. The remaining 4 GPUs were used to parallelize the softmax, so each GPU was responsible\nfor multiplying by a 1000 x 20000 matrix. The resulting implementation achieved a speed of 6,300\n(both English and French) words per second with a minibatch size of 128. Training took about a ten\ndays with this implementation.\n\n3.6 Experimental Results\n\nWe used the cased BLEU score [24] to evaluate the quality of our translations. We computed our\nBLEU scores using multi-bleu · pl 1 the tokenized predictions and ground truth. This way\non\nof evaluating the BELU score is consistent with [5] and [2], and reproduces the 33.3 score of [29].\nHowever, if we evaluate the best WMT' 14 system [9] (whose predictions can be downloaded from\nstatmt · org \\matrix) in this manner, we get 37.0, which is greater than the 35.8 reported by\nstatmt · org \\matrix.\n\nThe results are presented in tables 1 and 2. Our best results are obtained with an ensemble of LSTMs\nthat differ in their random initializations and in the random order of minibatches. While the decoded\ntranslations of the LSTM ensemble do not outperform the best WMT' 14 system, it is the first time\nthat a pure neural translation system outperforms a phrase-based SMT baseline on a large scale MT\n\n1There several variants of the BLEU score, and each variant is defined with a perl script.\n\n5", "text": "Sequence to Sequence Learning\nwith Neural Networks\nIlya Sutskever Oriol Vinyals Quoc V. Le\n Google Google Google\n ilyasu@google .com vinyal s@google · com qvl @google.com\nAbstract\nDeep Neural Networks (DNNs) are powerful models that have achieved excel-\nlent performance on difficult learning tasks. Although DNNs work well whenever\nlarge labeled training sets are available, they cannot be used to map sequences to\nsequences. In this paper, we present a general end-to-end approach to sequence\nlearning that makes minimal assumptions on the sequence structure. Our method\nuses a multilayered Long Short-Term Memory (LSTM) to map the input sequence\nto a vector of a fixed dimensionality, and then another deep LSTM to decode the\ntarget sequence from the vector. Our main result is that on an English to French\ntranslation task from the WMT' 14 dataset, the translations produced by the LSTM\nachieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU\nscore was penalized on out-of-vocabulary words. Additionally, the LSTM did not\nhave difficulty on long sentences. For comparison, a phrase-based SMT system\nachieves a BLEU score of 33.3 on the same dataset. When we used the LSTM\nto rerank the 1000 hypotheses produced by the aforementioned SMT system, its\nBLEU score increases to 36.5, which is close to the previous best result on this\ntask. The LSTM also learned sensible phrase and sentence representations that\nare sensitive to word order and are relatively invariant to the active and the pas-\nsive voice. Finally, we found that reversing the order of the words in all source\nsentences (but not target sentences) improved the LSTM's performance markedly,\nbecause doing so introduced many short term dependencies between the source\nand the target sentence which made the optimization problem easier.\n1 Introduction\nDeep Neural Networks (DNNs) are extremely powerful machine learning models that achieve ex-\ncellent performance on difficult problems such as speech recognition [13, 7] and visual object recog-\nnition [19, 6, 21, 20]. DNNs are powerful because they can perform arbitrary parallel computation\nfor a modest number of steps. A surprising example of the power of DNNs is their ability to sort\nN N-bit numbers using only 2 hidden layers of quadratic size [27]. So, while neural networks are\nrelated to conventional statistical models, they learn an intricate computation. Furthermore, large\nDNNs can be trained with supervised backpropagation whenever the labeled training set has enough\ninformation to specify the network's parameters. Thus, if there exists a parameter setting of a large\nDNN that achieves good results (for example, because humans can solve the task very rapidly),\nsupervised backpropagation will find these parameters and solve the problem.\nDespite their flexibility and power, DNNs can only be applied to problems whose inputs and targets\ncan be sensibly encoded with vectors of fixed dimensionality. It is a significant limitation, since\nmany important problems are best expressed with sequences whose lengths are not known a-priori.\nFor example, speech recognition and machine translation are sequential problems. Likewise, ques-\ntion answering can also be seen as mapping a sequence of words representing the question to a\n2014\nDec\n14\n[cs.CL]\narXiv:1409.3215v3\n1\nsequence of words representing the answer. It is therefore clear that a domain-independent method\nthat learns to map sequences to sequences would be useful.\nSequences pose a challenge for DNNs because they require that the dimensionality of the inputs and\noutputs is known and fixed. In this paper, we show that a straightforward application of the Long\nShort-Term Memory (LSTM) architecture [16] can solve general sequence to sequence problems.\nThe idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large fixed-\ndimensional vector representation, and then to use another LSTM to extract the output sequence\nfrom that vector (fig. 1). The second LSTM is essentially a recurrent neural network language model\n[28, 23, 30] except that it is conditioned on the input sequence. The LSTM's ability to successfully\nlearn on data with long range temporal dependencies makes it a natural choice for this application\ndue to the considerable time lag between the inputs and their corresponding outputs (fig. 1).\nThere have been a number of related attempts to address the general sequence to sequence learning\nproblem with neural networks. Our approach is closely related to Kalchbrenner and Blunsom [18]\nwho were the first to map the entire input sentence to vector, and is related to Cho et al. [5] although\nthe latter was used only for rescoring hypotheses produced by a phrase-based system. Graves [10]\nintroduced a novel differentiable attention mechanism that allows neural networks to focus on dif-\nferent parts of their input, and an elegant variant of this idea was successfully applied to machine\ntranslation by Bahdanau et al. [2]. The Connectionist Sequence Classification is another popular\ntechnique for mapping sequences to sequences with neural networks, but it assumes a monotonic\nalignment between the inputs and the outputs [1 1].\nW X Y Z <EOS>\nT T ↑\nA B C <EOS> w X Y Z\nFigure 1: Our model reads an input sentence \"ABC\" and produces \"WXYZ\" as the output sentence. The\nmodel stops making predictions after outputting the end-of-sentence token. Note that the LSTM reads the\ninput sentence in reverse, because doing so introduces many short term dependencies in the data that make the\noptimization problem much easier.\nThe main result of this work is the following. On the WMT' 14 English to French translation task,\nwe obtained a BLEU score of 34.81 by directly extracting translations from an ensemble of 5 deep\nLSTMs (with 384M parameters and 8,000 dimensional state each) using a simple left-to-right beam-\nsearch decoder. This is by far the best result achieved by direct translation with large neural net-\nworks. For comparison, the BLEU score of an SMT baseline on this dataset is 33.30 [29]. The 34.81\nBLEU score was achieved by an LSTM with a vocabulary of 80k words, so the score was penalized\nwhenever the reference translation contained a word not covered by these 80k. This result shows\nthat a relatively unoptimized small-vocabulary neural network architecture which has much room\nfor improvement outperforms a phrase-based SMT system.\nFinally, we used the LSTM to rescore the publicly available 1000-best lists of the SMT baseline on\nthe same task [29]. By doing so, we obtained a BLEU score of 36.5, which improves the baseline by\n3.2 BLEU points and is close to the previous best published result on this task (which is 37.0 [9]).\nSurprisingly, the LSTM did not suffer on very long sentences, despite the recent experience of other\nresearchers with related architectures [26]. We were able to do well on long sentences because we\nreversed the order of words in the source sentence but not the target sentences in the training and test\nset. By doing so, we introduced many short term dependencies that made the optimization problem\nmuch simpler (see sec. 2 and 3.3). As a result, SGD could learn LSTMs that had no trouble with\nlong sentences. The simple trick of reversing the words in the source sentence is one of the key\ntechnical contributions of this work.\nA useful property of the LSTM is that it learns to map an input sentence of variable length into\na fixed-dimensional vector representation. Given that translations tend to be paraphrases of the\nsource sentences, the translation objective encourages the LSTM to find sentence representations\nthat capture their meaning, as sentences with similar meanings are close to each other while different\n2\nsentences meanings will be far. A qualitative evaluation supports this claim, showing that our model\nis aware of word order and is fairly invariant to the active and passive voice.\n2 The model\nThe Recurrent Neural Network (RNN) [31, 28] is a natural generalization of feedforward neural\nnetworks to sequences. Given a sequence of inputs (x1, . · .,XT), a standard RNN computes a\nsequence of outputs (y1,· · . , YT) by iterating the following equation:\nht = sigm (Whx Xt + Whh ht-1) \nyt = Wyhht\nThe RNN can easily map sequences to sequences whenever the alignment between the inputs the\noutputs is known ahead of time. However, it is not clear how to apply an RNN to problems whose\ninput and the output sequences have different lengths with complicated and non-monotonic relation-\nships.\nThe simplest strategy for general sequence learning is to map the input sequence to a fixed-sized\nvector using one RNN, and then to map the vector to the target sequence with another RNN (this\napproach has also been taken by Cho et al. [5]). While it could work in principle since the RNN is\nprovided with all the relevant information, it would be difficult to train the RNNs due to the resulting\nlong term dependencies (figure 1) [14, 4, 16, 15]. However, the Long Short-Term Memory (LSTM)\n[16] is known to learn problems with long range temporal dependencies, so an LSTM may succeed\nin this setting.\nThe goal of the LSTM is to estimate the conditional probability p(y1, . : , YT' x1,···, xT) where\n(x1, · · · , xT) is an input sequence and Y1, · · · , YT' is its corresponding output sequence whose length\nT' may differ from T. The LSTM computes this conditional probability by first obtaining the fixed-\ndimensional representation v of the input sequence (x1,.. · , xT) given by the last hidden state of the\nLSTM, and then computing the probability of y1, · · · , YT' with a standard LSTM-LM formulation\nwhose initial hidden state is set to the representation v of x1, · · · , XT:\nT' \np(y1, . ··· YT' |X1,...,XT) = II p(yt|v, y1, · ··· yt-1) \nt=1\n(1)\nIn this equation, each p(yt|v, y1, · · · , yt-1) distribution is represented with a softmax over all the\nwords in the vocabulary. We use the LSTM formulation from Graves [10]. Note that we require that\neach sentence ends with a special end-of-sentence symbol \"<EOS>\", which enables the model to\ndefine a distribution over sequences of all possible lengths. The overall scheme is outlined in figure\n· <EOS>\" and then uses\n1, where the shown LSTM computes the representation of \"A\", \"B\" , \"C\" ,\nthis representation to compute the probability of \"W\", \"X\", \"Y\" \"Z\" , <EOS>\"\n\"\n,\nOur actual models differ from the above description in three important ways. First, we used two\ndifferent LSTMs: one for the input sequence and another for the output sequence, because doing\nso increases the number model parameters at negligible computational cost and makes it natural to\ntrain the LSTM on multiple language pairs simultaneously [18]. Second, we found that deep LSTMs\nsignificantly outperformed shallow LSTMs, so we chose an LSTM with four layers. Third, we found\nit extremely valuable to reverse the order of the words of the input sentence. So for example, instead\nof mapping the sentence a, b, c to the sentence �, �, % the LSTM is asked to map c, b, a to �, �, %\nwhere �, �, づ is the translation of a, b, c. This way, a is in close proximity to �, bis fairly close to �,\nand so on, a fact that makes it easy for SGD to \"establish communication\" between the input and the\noutput. We found this simple data transformation to greatly improve the performance of the LSTM.\n3 Experiments\nWe applied our method to the WMT' 14 English to French MT task in two ways. We used it to\ndirectly translate the input sentence without using a reference SMT system and we it to rescore the\nn-best lists of an SMT baseline. We report the accuracy of these translation methods, present sample\ntranslations, and visualize the resulting sentence representation.\n3\n3.1 Dataset details\nWe used the WMT' 14 English to French dataset. We trained our models on a subset of 12M sen-\ntences consisting of 348M French words and 304M English words, which is a clean \"selected\"\nsubset from [29]. We chose this translation task and this specific training set subset because of the\npublic availability of a tokenized training and test set together with 1000-best lists from the baseline\nSMT [29].\nAs typical neural language models rely on a vector representation for each word, we used a fixed\nvocabulary for both languages. We used 160,000 of the most frequent words for the source language\nand 80,000 of the most frequent words for the target language. Every out-of-vocabulary word was\nreplaced with a special \"UNK\" token.\n3.2 Decoding and Rescoring\nThe core of our experiments involved training a large deep LSTM on many sentence pairs. We\ntrained it by maximizing the log probability of a correct translation T given the source sentence S,\nso the training objective is\n1/S � log p(T|S) \n(T,S)ES\nwhere s is the training set. Once training is complete, we produce translations by finding the most\nlikely translation according to the LSTM:\nT = arg max p(T|S) \nT\n(2)\nWe search for the most likely translation using a simple left-to-right beam search decoder which\nmaintains a small number B of partial hypotheses, where a partial hypothesis is a prefix of some\ntranslation. At each timestep we extend each partial hypothesis in the beam with every possible\nword in the vocabulary. This greatly increases the number of the hypotheses so we discard all but\nthe B most likely hypotheses according to the model's log probability. As soon as the \"<EOS>\"\nsymbol is appended to a hypothesis, it is removed from the beam and is added to the set of complete\nhypotheses. While this decoder is approximate, it is simple to implement. Interestingly, our system\nperforms well even with a beam size of 1, and a beam of size 2 provides most of the benefits of beam\nsearch (Table 1).\nWe also used the LSTM to rescore the 1000-best lists produced by the baseline system [29]. To\nrescore an n-best list, we computed the log probability of every hypothesis with our LSTM and took\nan even average with their score and the LSTM's score.\n3.3 Reversing the Source Sentences\nWhile the LSTM is capable of solving problems with long term dependencies, we discovered that\nthe LSTM learns much better when the source sentences are reversed (the target sentences are not\nreversed). By doing so, the LSTM's test perplexity dropped from 5.8 to 4.7, and the test BLEU\nscores of its decoded translations increased from 25.9 to 30.6.\nWhile we do not have a complete explanation to this phenomenon, we believe that it is caused by\nthe introduction of many short term dependencies to the dataset. Normally, when we concatenate a\nsource sentence with a target sentence, each word in the source sentence is far from its corresponding\nword in the target sentence. As a result, the problem has a large \"minimal time lag\" [17]. By\nreversing the words in the source sentence, the average distance between corresponding words in\nthe source and target language is unchanged. However, the first few words in the source language\nare now very close to the first few words in the target language, so the problem's minimal time lag is\ngreatly reduced. Thus, backpropagation has an easier time \"establishing communication\" between\nthe source sentence and the target sentence, which in turn results in substantially improved overall\nperformance.\nInitially, we believed that reversing the input sentences would only lead to more confident predic-\ntions in the early parts of the target sentence and to less confident predictions in the later parts. How-\never, LSTMs trained on reversed source sentences did much better on long sentences than LSTMs\n4\ntrained on the raw source sentences (see sec. 3.7), which suggests that reversing the input sentences\nresults in LSTMs with better memory utilization.\n3.4 Training details\nWe found that the LSTM models are fairly easy to train. We used deep LSTMs with 4 layers,\nwith 1000 cells at each layer and 1000 dimensional word embeddings, with an input vocabulary\nof 160,000 and an output vocabulary of 80,000. Thus the deep LSTM uses 8000 real numbers to\nrepresent a sentence. We found deep LSTMs to significantly outperform shallow LSTMs, where\neach additional layer reduced perplexity by nearly 10%, possibly due to their much larger hidden\nstate. We used a naive softmax over 80,000 words at each output. The resulting LSTM has 384M\nparameters of which 64M are pure recurrent connections (32M for the \"encoder\" LSTM and 32M\nfor the \"decoder\" LSTM). The complete training details are given below:\n· We initialized all of the LSTM's parameters with the uniform distribution between -0.08\nand 0.08\n· We used stochastic gradient descent without momentum, with a fixed learning rate of 0.7.\nAfter 5 epochs, we begun halving the learning rate every half epoch. We trained our models\nfor a total of 7.5 epochs.\n· We used batches of 128 sequences for the gradient and divided it the size of the batch\n(namely, 128).\n· Although LSTMs tend to not suffer from the vanishing gradient problem, they can have\nexploding gradients. Thus we enforced a hard constraint on the norm of the gradient [10,\n25] by scaling it when its norm exceeded a threshold. For each training batch, we compute\ns = 1191|2, where g is the gradient divided by 128. If s > 5, we set g = 5g.\ns\n· Different sentences have different lengths. Most sentences are short (e.g., length 20-30)\nbut some sentences are long (e.g., length > 100), so a minibatch of 128 randomly chosen\ntraining sentences will have many short sentences and few long sentences, and as a result,\nmuch of the computation in the minibatch is wasted. To address this problem, we made sure\nthat all sentences in a minibatch are roughly of the same length, yielding a 2x speedup.\n3.5 Parallelization\nA C++ implementation of deep LSTM with the configuration from the previous section on a sin-\ngle GPU processes a speed of approximately 1,700 words per second. This was too slow for our\npurposes, so we parallelized our model using an 8-GPU machine. Each layer of the LSTM was\nexecuted on a different GPU and communicated its activations to the next GPU / layer as soon as\nthey were computed. Our models have 4 layers of LSTMs, each of which resides on a separate\nGPU. The remaining 4 GPUs were used to parallelize the softmax, so each GPU was responsible\nfor multiplying by a 1000 x 20000 matrix. The resulting implementation achieved a speed of 6,300\n(both English and French) words per second with a minibatch size of 128. Training took about a ten\ndays with this implementation.\n3.6 Experimental Results\nWe used the cased BLEU score [24] to evaluate the quality of our translations. We computed our\nBLEU scores using multi-bleu · pl 1 the tokenized predictions and ground truth. This way\non\nof evaluating the BELU score is consistent with [5] and [2], and reproduces the 33.3 score of [29].\nHowever, if we evaluate the best WMT' 14 system [9] (whose predictions can be downloaded from\nstatmt · org \\matrix) in this manner, we get 37.0, which is greater than the 35.8 reported by\nstatmt · org \\matrix.\nThe results are presented in tables 1 and 2. Our best results are obtained with an ensemble of LSTMs\nthat differ in their random initializations and in the random order of minibatches. While the decoded\ntranslations of the LSTM ensemble do not outperform the best WMT' 14 system, it is the first time\nthat a pure neural translation system outperforms a phrase-based SMT baseline on a large scale MT\n1There several variants of the BLEU score, and each variant is defined with a perl script.\n5"}, "elements": [{"category": "heading1", "content": {"html": "<h1 id='0' style='font-size:22px'>Sequence to Sequence Learning<br>with Neural Networks</h1>", "markdown": "# Sequence to Sequence Learning\nwith Neural Networks", "text": "Sequence to Sequence Learning\nwith Neural Networks"}, "coordinates": [{"x": 0.3056, "y": 0.1362}, {"x": 0.6948, "y": 0.1362}, {"x": 0.6948, "y": 0.1864}, {"x": 0.3056, "y": 0.1864}], "id": 0, "page": 1}, {"base64_encoding": "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", "category": "table", "content": {"html": "<table id='1' style='font-size:18px'><thead></thead><tbody><tr><td>Ilya Sutskever</td><td>Oriol Vinyals</td><td>Quoc V. Le</td></tr><tr><td>Google</td><td>Google</td><td>Google</td></tr><tr><td>ilyasu@google .com</td><td>vinyal s@google · com</td><td>qvl @google.com</td></tr></tbody></table>", "markdown": "| Ilya Sutskever | Oriol Vinyals | Quoc V. Le |\n| --- | --- | --- |\n| Google | Google | Google |\n| ilyasu@google .com | vinyal s@google · com | qvl @google.com |\n", "text": "Ilya Sutskever Oriol Vinyals Quoc V. Le\n Google Google Google\n ilyasu@google .com vinyal s@google · com qvl @google.com"}, "coordinates": [{"x": 0.2103, "y": 0.2339}, {"x": 0.7943, "y": 0.2339}, {"x": 0.7943, "y": 0.2798}, {"x": 0.2103, "y": 0.2798}], "id": 1, "page": 1}, {"category": "paragraph", "content": {"html": "<p id='2' data-category='paragraph' style='font-size:20px'>Abstract</p>", "markdown": "Abstract", "text": "Abstract"}, "coordinates": [{"x": 0.4607, "y": 0.3134}, {"x": 0.5404, "y": 0.3134}, {"x": 0.5404, "y": 0.3306}, {"x": 0.4607, "y": 0.3306}], "id": 2, "page": 1}, {"category": "paragraph", "content": {"html": "<p id='3' data-category='paragraph' style='font-size:16px'>Deep Neural Networks (DNNs) are powerful models that have achieved excel-<br>lent performance on difficult learning tasks. Although DNNs work well whenever<br>large labeled training sets are available, they cannot be used to map sequences to<br>sequences. In this paper, we present a general end-to-end approach to sequence<br>learning that makes minimal assumptions on the sequence structure. Our method<br>uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence<br>to a vector of a fixed dimensionality, and then another deep LSTM to decode the<br>target sequence from the vector. Our main result is that on an English to French<br>translation task from the WMT' 14 dataset, the translations produced by the LSTM<br>achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU<br>score was penalized on out-of-vocabulary words. Additionally, the LSTM did not<br>have difficulty on long sentences. For comparison, a phrase-based SMT system<br>achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM<br>to rerank the 1000 hypotheses produced by the aforementioned SMT system, its<br>BLEU score increases to 36.5, which is close to the previous best result on this<br>task. The LSTM also learned sensible phrase and sentence representations that<br>are sensitive to word order and are relatively invariant to the active and the pas-<br>sive voice. Finally, we found that reversing the order of the words in all source<br>sentences (but not target sentences) improved the LSTM's performance markedly,<br>because doing so introduced many short term dependencies between the source<br>and the target sentence which made the optimization problem easier.</p>", "markdown": "Deep Neural Networks (DNNs) are powerful models that have achieved excel-\nlent performance on difficult learning tasks. Although DNNs work well whenever\nlarge labeled training sets are available, they cannot be used to map sequences to\nsequences. In this paper, we present a general end-to-end approach to sequence\nlearning that makes minimal assumptions on the sequence structure. Our method\nuses a multilayered Long Short-Term Memory (LSTM) to map the input sequence\nto a vector of a fixed dimensionality, and then another deep LSTM to decode the\ntarget sequence from the vector. Our main result is that on an English to French\ntranslation task from the WMT' 14 dataset, the translations produced by the LSTM\nachieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU\nscore was penalized on out-of-vocabulary words. Additionally, the LSTM did not\nhave difficulty on long sentences. For comparison, a phrase-based SMT system\nachieves a BLEU score of 33.3 on the same dataset. When we used the LSTM\nto rerank the 1000 hypotheses produced by the aforementioned SMT system, its\nBLEU score increases to 36.5, which is close to the previous best result on this\ntask. The LSTM also learned sensible phrase and sentence representations that\nare sensitive to word order and are relatively invariant to the active and the pas-\nsive voice. Finally, we found that reversing the order of the words in all source\nsentences (but not target sentences) improved the LSTM's performance markedly,\nbecause doing so introduced many short term dependencies between the source\nand the target sentence which made the optimization problem easier.", "text": "Deep Neural Networks (DNNs) are powerful models that have achieved excel-\nlent performance on difficult learning tasks. Although DNNs work well whenever\nlarge labeled training sets are available, they cannot be used to map sequences to\nsequences. In this paper, we present a general end-to-end approach to sequence\nlearning that makes minimal assumptions on the sequence structure. Our method\nuses a multilayered Long Short-Term Memory (LSTM) to map the input sequence\nto a vector of a fixed dimensionality, and then another deep LSTM to decode the\ntarget sequence from the vector. Our main result is that on an English to French\ntranslation task from the WMT' 14 dataset, the translations produced by the LSTM\nachieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU\nscore was penalized on out-of-vocabulary words. Additionally, the LSTM did not\nhave difficulty on long sentences. For comparison, a phrase-based SMT system\nachieves a BLEU score of 33.3 on the same dataset. When we used the LSTM\nto rerank the 1000 hypotheses produced by the aforementioned SMT system, its\nBLEU score increases to 36.5, which is close to the previous best result on this\ntask. The LSTM also learned sensible phrase and sentence representations that\nare sensitive to word order and are relatively invariant to the active and the pas-\nsive voice. Finally, we found that reversing the order of the words in all source\nsentences (but not target sentences) improved the LSTM's performance markedly,\nbecause doing so introduced many short term dependencies between the source\nand the target sentence which made the optimization problem easier."}, "coordinates": [{"x": 0.2313, "y": 0.3479}, {"x": 0.7699, "y": 0.3479}, {"x": 0.7699, "y": 0.6417}, {"x": 0.2313, "y": 0.6417}], "id": 3, "page": 1}, {"category": "paragraph", "content": {"html": "<p id='4' data-category='paragraph' style='font-size:18px'>1 Introduction</p>", "markdown": "1 Introduction", "text": "1 Introduction"}, "coordinates": [{"x": 0.174, "y": 0.6734}, {"x": 0.3148, "y": 0.6734}, {"x": 0.3148, "y": 0.6912}, {"x": 0.174, "y": 0.6912}], "id": 4, "page": 1}, {"category": "paragraph", "content": {"html": "<p id='5' data-category='paragraph' style='font-size:16px'>Deep Neural Networks (DNNs) are extremely powerful machine learning models that achieve ex-<br>cellent performance on difficult problems such as speech recognition [13, 7] and visual object recog-<br>nition [19, 6, 21, 20]. DNNs are powerful because they can perform arbitrary parallel computation<br>for a modest number of steps. A surprising example of the power of DNNs is their ability to sort<br>N N-bit numbers using only 2 hidden layers of quadratic size [27]. So, while neural networks are<br>related to conventional statistical models, they learn an intricate computation. Furthermore, large<br>DNNs can be trained with supervised backpropagation whenever the labeled training set has enough<br>information to specify the network's parameters. Thus, if there exists a parameter setting of a large<br>DNN that achieves good results (for example, because humans can solve the task very rapidly),<br>supervised backpropagation will find these parameters and solve the problem.</p>", "markdown": "Deep Neural Networks (DNNs) are extremely powerful machine learning models that achieve ex-\ncellent performance on difficult problems such as speech recognition [13, 7] and visual object recog-\nnition [19, 6, 21, 20]. DNNs are powerful because they can perform arbitrary parallel computation\nfor a modest number of steps. A surprising example of the power of DNNs is their ability to sort\nN N-bit numbers using only 2 hidden layers of quadratic size [27]. So, while neural networks are\nrelated to conventional statistical models, they learn an intricate computation. Furthermore, large\nDNNs can be trained with supervised backpropagation whenever the labeled training set has enough\ninformation to specify the network's parameters. Thus, if there exists a parameter setting of a large\nDNN that achieves good results (for example, because humans can solve the task very rapidly),\nsupervised backpropagation will find these parameters and solve the problem.", "text": "Deep Neural Networks (DNNs) are extremely powerful machine learning models that achieve ex-\ncellent performance on difficult problems such as speech recognition [13, 7] and visual object recog-\nnition [19, 6, 21, 20]. DNNs are powerful because they can perform arbitrary parallel computation\nfor a modest number of steps. A surprising example of the power of DNNs is their ability to sort\nN N-bit numbers using only 2 hidden layers of quadratic size [27]. So, while neural networks are\nrelated to conventional statistical models, they learn an intricate computation. Furthermore, large\nDNNs can be trained with supervised backpropagation whenever the labeled training set has enough\ninformation to specify the network's parameters. Thus, if there exists a parameter setting of a large\nDNN that achieves good results (for example, because humans can solve the task very rapidly),\nsupervised backpropagation will find these parameters and solve the problem."}, "coordinates": [{"x": 0.173, "y": 0.7077}, {"x": 0.8278, "y": 0.7077}, {"x": 0.8278, "y": 0.8483}, {"x": 0.173, "y": 0.8483}], "id": 5, "page": 1}, {"category": "paragraph", "content": {"html": "<br><p id='6' data-category='paragraph' style='font-size:16px'>Despite their flexibility and power, DNNs can only be applied to problems whose inputs and targets<br>can be sensibly encoded with vectors of fixed dimensionality. It is a significant limitation, since<br>many important problems are best expressed with sequences whose lengths are not known a-priori.<br>For example, speech recognition and machine translation are sequential problems. Likewise, ques-<br>tion answering can also be seen as mapping a sequence of words representing the question to a</p>", "markdown": "Despite their flexibility and power, DNNs can only be applied to problems whose inputs and targets\ncan be sensibly encoded with vectors of fixed dimensionality. It is a significant limitation, since\nmany important problems are best expressed with sequences whose lengths are not known a-priori.\nFor example, speech recognition and machine translation are sequential problems. Likewise, ques-\ntion answering can also be seen as mapping a sequence of words representing the question to a", "text": "Despite their flexibility and power, DNNs can only be applied to problems whose inputs and targets\ncan be sensibly encoded with vectors of fixed dimensionality. It is a significant limitation, since\nmany important problems are best expressed with sequences whose lengths are not known a-priori.\nFor example, speech recognition and machine translation are sequential problems. Likewise, ques-\ntion answering can also be seen as mapping a sequence of words representing the question to a"}, "coordinates": [{"x": 0.1734, "y": 0.8546}, {"x": 0.829, "y": 0.8546}, {"x": 0.829, "y": 0.9264}, {"x": 0.1734, "y": 0.9264}], "id": 6, "page": 1}, {"category": "header", "content": {"html": "<br><header id='7' style='font-size:14px'>2014<br>Dec<br>14<br>[cs.CL]<br>arXiv:1409.3215v3</header>", "markdown": "2014\nDec\n14\n[cs.CL]\narXiv:1409.3215v3", "text": "2014\nDec\n14\n[cs.CL]\narXiv:1409.3215v3"}, "coordinates": [{"x": 0.0209, "y": 0.2457}, {"x": 0.0625, "y": 0.2457}, {"x": 0.0625, "y": 0.6804}, {"x": 0.0209, "y": 0.6804}], "id": 7, "page": 1}, {"category": "footer", "content": {"html": "<footer id='8' style='font-size:14px'>1</footer>", "markdown": "1", "text": "1"}, "coordinates": [{"x": 0.4936, "y": 0.9478}, {"x": 0.5069, "y": 0.9478}, {"x": 0.5069, "y": 0.963}, {"x": 0.4936, "y": 0.963}], "id": 8, "page": 1}, {"category": "paragraph", "content": {"html": "<p id='9' data-category='paragraph' style='font-size:16px'>sequence of words representing the answer. It is therefore clear that a domain-independent method<br>that learns to map sequences to sequences would be useful.</p>", "markdown": "sequence of words representing the answer. It is therefore clear that a domain-independent method\nthat learns to map sequences to sequences would be useful.", "text": "sequence of words representing the answer. It is therefore clear that a domain-independent method\nthat learns to map sequences to sequences would be useful."}, "coordinates": [{"x": 0.1728, "y": 0.1052}, {"x": 0.8279, "y": 0.1052}, {"x": 0.8279, "y": 0.1339}, {"x": 0.1728, "y": 0.1339}], "id": 9, "page": 2}, {"category": "paragraph", "content": {"html": "<br><p id='10' data-category='paragraph' style='font-size:20px'>Sequences pose a challenge for DNNs because they require that the dimensionality of the inputs and<br>outputs is known and fixed. In this paper, we show that a straightforward application of the Long<br>Short-Term Memory (LSTM) architecture [16] can solve general sequence to sequence problems.<br>The idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large fixed-<br>dimensional vector representation, and then to use another LSTM to extract the output sequence<br>from that vector (fig. 1). The second LSTM is essentially a recurrent neural network language model<br>[28, 23, 30] except that it is conditioned on the input sequence. The LSTM's ability to successfully<br>learn on data with long range temporal dependencies makes it a natural choice for this application<br>due to the considerable time lag between the inputs and their corresponding outputs (fig. 1).</p>", "markdown": "Sequences pose a challenge for DNNs because they require that the dimensionality of the inputs and\noutputs is known and fixed. In this paper, we show that a straightforward application of the Long\nShort-Term Memory (LSTM) architecture [16] can solve general sequence to sequence problems.\nThe idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large fixed-\ndimensional vector representation, and then to use another LSTM to extract the output sequence\nfrom that vector (fig. 1). The second LSTM is essentially a recurrent neural network language model\n[28, 23, 30] except that it is conditioned on the input sequence. The LSTM's ability to successfully\nlearn on data with long range temporal dependencies makes it a natural choice for this application\ndue to the considerable time lag between the inputs and their corresponding outputs (fig. 1).", "text": "Sequences pose a challenge for DNNs because they require that the dimensionality of the inputs and\noutputs is known and fixed. In this paper, we show that a straightforward application of the Long\nShort-Term Memory (LSTM) architecture [16] can solve general sequence to sequence problems.\nThe idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large fixed-\ndimensional vector representation, and then to use another LSTM to extract the output sequence\nfrom that vector (fig. 1). The second LSTM is essentially a recurrent neural network language model\n[28, 23, 30] except that it is conditioned on the input sequence. The LSTM's ability to successfully\nlearn on data with long range temporal dependencies makes it a natural choice for this application\ndue to the considerable time lag between the inputs and their corresponding outputs (fig. 1)."}, "coordinates": [{"x": 0.1734, "y": 0.1405}, {"x": 0.8277, "y": 0.1405}, {"x": 0.8277, "y": 0.2661}, {"x": 0.1734, "y": 0.2661}], "id": 10, "page": 2}, {"category": "paragraph", "content": {"html": "<br><p id='11' data-category='paragraph' style='font-size:20px'>There have been a number of related attempts to address the general sequence to sequence learning<br>problem with neural networks. Our approach is closely related to Kalchbrenner and Blunsom [18]<br>who were the first to map the entire input sentence to vector, and is related to Cho et al. [5] although<br>the latter was used only for rescoring hypotheses produced by a phrase-based system. Graves [10]<br>introduced a novel differentiable attention mechanism that allows neural networks to focus on dif-<br>ferent parts of their input, and an elegant variant of this idea was successfully applied to machine<br>translation by Bahdanau et al. [2]. The Connectionist Sequence Classification is another popular<br>technique for mapping sequences to sequences with neural networks, but it assumes a monotonic<br>alignment between the inputs and the outputs [1 1].</p>", "markdown": "There have been a number of related attempts to address the general sequence to sequence learning\nproblem with neural networks. Our approach is closely related to Kalchbrenner and Blunsom [18]\nwho were the first to map the entire input sentence to vector, and is related to Cho et al. [5] although\nthe latter was used only for rescoring hypotheses produced by a phrase-based system. Graves [10]\nintroduced a novel differentiable attention mechanism that allows neural networks to focus on dif-\nferent parts of their input, and an elegant variant of this idea was successfully applied to machine\ntranslation by Bahdanau et al. [2]. The Connectionist Sequence Classification is another popular\ntechnique for mapping sequences to sequences with neural networks, but it assumes a monotonic\nalignment between the inputs and the outputs [1 1].", "text": "There have been a number of related attempts to address the general sequence to sequence learning\nproblem with neural networks. Our approach is closely related to Kalchbrenner and Blunsom [18]\nwho were the first to map the entire input sentence to vector, and is related to Cho et al. [5] although\nthe latter was used only for rescoring hypotheses produced by a phrase-based system. Graves [10]\nintroduced a novel differentiable attention mechanism that allows neural networks to focus on dif-\nferent parts of their input, and an elegant variant of this idea was successfully applied to machine\ntranslation by Bahdanau et al. [2]. The Connectionist Sequence Classification is another popular\ntechnique for mapping sequences to sequences with neural networks, but it assumes a monotonic\nalignment between the inputs and the outputs [1 1]."}, "coordinates": [{"x": 0.1733, "y": 0.2718}, {"x": 0.8278, "y": 0.2718}, {"x": 0.8278, "y": 0.3981}, {"x": 0.1733, "y": 0.3981}], "id": 11, "page": 2}, {"base64_encoding": "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", "category": "figure", "content": {"html": "<figure id='12'><img style='font-size:16px' alt=\"W X Y Z <EOS>\nT T ↑\nA B C <EOS> w X Y Z\" data-coord=\"top-left:(257,674); bottom-right:(1019,837)\" /></figure>", "markdown": "![image](/image/placeholder)\nW X Y Z <EOS>\nT T ↑\nA B C <EOS> w X Y Z", "text": "W X Y Z <EOS>\nT T ↑\nA B C <EOS> w X Y Z"}, "coordinates": [{"x": 0.2018, "y": 0.4086}, {"x": 0.7993, "y": 0.4086}, {"x": 0.7993, "y": 0.5076}, {"x": 0.2018, "y": 0.5076}], "id": 12, "page": 2}, {"category": "paragraph", "content": {"html": "<p id='13' data-category='paragraph' style='font-size:14px'>Figure 1: Our model reads an input sentence \"ABC\" and produces \"WXYZ\" as the output sentence. The<br>model stops making predictions after outputting the end-of-sentence token. Note that the LSTM reads the<br>input sentence in reverse, because doing so introduces many short term dependencies in the data that make the<br>optimization problem much easier.</p>", "markdown": "Figure 1: Our model reads an input sentence \"ABC\" and produces \"WXYZ\" as the output sentence. The\nmodel stops making predictions after outputting the end-of-sentence token. Note that the LSTM reads the\ninput sentence in reverse, because doing so introduces many short term dependencies in the data that make the\noptimization problem much easier.", "text": "Figure 1: Our model reads an input sentence \"ABC\" and produces \"WXYZ\" as the output sentence. The\nmodel stops making predictions after outputting the end-of-sentence token. Note that the LSTM reads the\ninput sentence in reverse, because doing so introduces many short term dependencies in the data that make the\noptimization problem much easier."}, "coordinates": [{"x": 0.1737, "y": 0.5175}, {"x": 0.8284, "y": 0.5175}, {"x": 0.8284, "y": 0.5701}, {"x": 0.1737, "y": 0.5701}], "id": 13, "page": 2}, {"category": "paragraph", "content": {"html": "<p id='14' data-category='paragraph' style='font-size:20px'>The main result of this work is the following. On the WMT' 14 English to French translation task,<br>we obtained a BLEU score of 34.81 by directly extracting translations from an ensemble of 5 deep<br>LSTMs (with 384M parameters and 8,000 dimensional state each) using a simple left-to-right beam-<br>search decoder. This is by far the best result achieved by direct translation with large neural net-<br>works. For comparison, the BLEU score of an SMT baseline on this dataset is 33.30 [29]. The 34.81<br>BLEU score was achieved by an LSTM with a vocabulary of 80k words, so the score was penalized<br>whenever the reference translation contained a word not covered by these 80k. This result shows<br>that a relatively unoptimized small-vocabulary neural network architecture which has much room<br>for improvement outperforms a phrase-based SMT system.</p>", "markdown": "The main result of this work is the following. On the WMT' 14 English to French translation task,\nwe obtained a BLEU score of 34.81 by directly extracting translations from an ensemble of 5 deep\nLSTMs (with 384M parameters and 8,000 dimensional state each) using a simple left-to-right beam-\nsearch decoder. This is by far the best result achieved by direct translation with large neural net-\nworks. For comparison, the BLEU score of an SMT baseline on this dataset is 33.30 [29]. The 34.81\nBLEU score was achieved by an LSTM with a vocabulary of 80k words, so the score was penalized\nwhenever the reference translation contained a word not covered by these 80k. This result shows\nthat a relatively unoptimized small-vocabulary neural network architecture which has much room\nfor improvement outperforms a phrase-based SMT system.", "text": "The main result of this work is the following. On the WMT' 14 English to French translation task,\nwe obtained a BLEU score of 34.81 by directly extracting translations from an ensemble of 5 deep\nLSTMs (with 384M parameters and 8,000 dimensional state each) using a simple left-to-right beam-\nsearch decoder. This is by far the best result achieved by direct translation with large neural net-\nworks. For comparison, the BLEU score of an SMT baseline on this dataset is 33.30 [29]. The 34.81\nBLEU score was achieved by an LSTM with a vocabulary of 80k words, so the score was penalized\nwhenever the reference translation contained a word not covered by these 80k. This result shows\nthat a relatively unoptimized small-vocabulary neural network architecture which has much room\nfor improvement outperforms a phrase-based SMT system."}, "coordinates": [{"x": 0.1727, "y": 0.5828}, {"x": 0.8274, "y": 0.5828}, {"x": 0.8274, "y": 0.7097}, {"x": 0.1727, "y": 0.7097}], "id": 14, "page": 2}, {"category": "paragraph", "content": {"html": "<br><p id='15' data-category='paragraph' style='font-size:20px'>Finally, we used the LSTM to rescore the publicly available 1000-best lists of the SMT baseline on<br>the same task [29]. By doing so, we obtained a BLEU score of 36.5, which improves the baseline by<br>3.2 BLEU points and is close to the previous best published result on this task (which is 37.0 [9]).</p>", "markdown": "Finally, we used the LSTM to rescore the publicly available 1000-best lists of the SMT baseline on\nthe same task [29]. By doing so, we obtained a BLEU score of 36.5, which improves the baseline by\n3.2 BLEU points and is close to the previous best published result on this task (which is 37.0 [9]).", "text": "Finally, we used the LSTM to rescore the publicly available 1000-best lists of the SMT baseline on\nthe same task [29]. By doing so, we obtained a BLEU score of 36.5, which improves the baseline by\n3.2 BLEU points and is close to the previous best published result on this task (which is 37.0 [9])."}, "coordinates": [{"x": 0.173, "y": 0.7155}, {"x": 0.8273, "y": 0.7155}, {"x": 0.8273, "y": 0.7591}, {"x": 0.173, "y": 0.7591}], "id": 15, "page": 2}, {"category": "paragraph", "content": {"html": "<br><p id='16' data-category='paragraph' style='font-size:20px'>Surprisingly, the LSTM did not suffer on very long sentences, despite the recent experience of other<br>researchers with related architectures [26]. We were able to do well on long sentences because we<br>reversed the order of words in the source sentence but not the target sentences in the training and test<br>set. By doing so, we introduced many short term dependencies that made the optimization problem<br>much simpler (see sec. 2 and 3.3). As a result, SGD could learn LSTMs that had no trouble with<br>long sentences. The simple trick of reversing the words in the source sentence is one of the key<br>technical contributions of this work.</p>", "markdown": "Surprisingly, the LSTM did not suffer on very long sentences, despite the recent experience of other\nresearchers with related architectures [26]. We were able to do well on long sentences because we\nreversed the order of words in the source sentence but not the target sentences in the training and test\nset. By doing so, we introduced many short term dependencies that made the optimization problem\nmuch simpler (see sec. 2 and 3.3). As a result, SGD could learn LSTMs that had no trouble with\nlong sentences. The simple trick of reversing the words in the source sentence is one of the key\ntechnical contributions of this work.", "text": "Surprisingly, the LSTM did not suffer on very long sentences, despite the recent experience of other\nresearchers with related architectures [26]. We were able to do well on long sentences because we\nreversed the order of words in the source sentence but not the target sentences in the training and test\nset. By doing so, we introduced many short term dependencies that made the optimization problem\nmuch simpler (see sec. 2 and 3.3). As a result, SGD could learn LSTMs that had no trouble with\nlong sentences. The simple trick of reversing the words in the source sentence is one of the key\ntechnical contributions of this work."}, "coordinates": [{"x": 0.1732, "y": 0.7656}, {"x": 0.828, "y": 0.7656}, {"x": 0.828, "y": 0.8623}, {"x": 0.1732, "y": 0.8623}], "id": 16, "page": 2}, {"category": "paragraph", "content": {"html": "<p id='17' data-category='paragraph' style='font-size:16px'>A useful property of the LSTM is that it learns to map an input sentence of variable length into<br>a fixed-dimensional vector representation. Given that translations tend to be paraphrases of the<br>source sentences, the translation objective encourages the LSTM to find sentence representations<br>that capture their meaning, as sentences with similar meanings are close to each other while different</p>", "markdown": "A useful property of the LSTM is that it learns to map an input sentence of variable length into\na fixed-dimensional vector representation. Given that translations tend to be paraphrases of the\nsource sentences, the translation objective encourages the LSTM to find sentence representations\nthat capture their meaning, as sentences with similar meanings are close to each other while different", "text": "A useful property of the LSTM is that it learns to map an input sentence of variable length into\na fixed-dimensional vector representation. Given that translations tend to be paraphrases of the\nsource sentences, the translation objective encourages the LSTM to find sentence representations\nthat capture their meaning, as sentences with similar meanings are close to each other while different"}, "coordinates": [{"x": 0.1739, "y": 0.8694}, {"x": 0.8286, "y": 0.8694}, {"x": 0.8286, "y": 0.9261}, {"x": 0.1739, "y": 0.9261}], "id": 17, "page": 2}, {"category": "footer", "content": {"html": "<footer id='18' style='font-size:20px'>2</footer>", "markdown": "2", "text": "2"}, "coordinates": [{"x": 0.4927, "y": 0.9483}, {"x": 0.5076, "y": 0.9483}, {"x": 0.5076, "y": 0.9623}, {"x": 0.4927, "y": 0.9623}], "id": 18, "page": 2}, {"category": "paragraph", "content": {"html": "<p id='19' data-category='paragraph' style='font-size:14px'>sentences meanings will be far. A qualitative evaluation supports this claim, showing that our model<br>is aware of word order and is fairly invariant to the active and passive voice.</p>", "markdown": "sentences meanings will be far. A qualitative evaluation supports this claim, showing that our model\nis aware of word order and is fairly invariant to the active and passive voice.", "text": "sentences meanings will be far. A qualitative evaluation supports this claim, showing that our model\nis aware of word order and is fairly invariant to the active and passive voice."}, "coordinates": [{"x": 0.1728, "y": 0.105}, {"x": 0.8275, "y": 0.105}, {"x": 0.8275, "y": 0.1342}, {"x": 0.1728, "y": 0.1342}], "id": 19, "page": 3}, {"category": "paragraph", "content": {"html": "<p id='20' data-category='paragraph' style='font-size:18px'>2 The model</p>", "markdown": "2 The model", "text": "2 The model"}, "coordinates": [{"x": 0.1725, "y": 0.1527}, {"x": 0.2974, "y": 0.1527}, {"x": 0.2974, "y": 0.1695}, {"x": 0.1725, "y": 0.1695}], "id": 20, "page": 3}, {"category": "paragraph", "content": {"html": "<p id='21' data-category='paragraph' style='font-size:14px'>The Recurrent Neural Network (RNN) [31, 28] is a natural generalization of feedforward neural<br>networks to sequences. Given a sequence of inputs (x1, . · .,XT), a standard RNN computes a<br>sequence of outputs (y1,· · . , YT) by iterating the following equation:</p>", "markdown": "The Recurrent Neural Network (RNN) [31, 28] is a natural generalization of feedforward neural\nnetworks to sequences. Given a sequence of inputs (x1, . · .,XT), a standard RNN computes a\nsequence of outputs (y1,· · . , YT) by iterating the following equation:", "text": "The Recurrent Neural Network (RNN) [31, 28] is a natural generalization of feedforward neural\nnetworks to sequences. Given a sequence of inputs (x1, . · .,XT), a standard RNN computes a\nsequence of outputs (y1,· · . , YT) by iterating the following equation:"}, "coordinates": [{"x": 0.1734, "y": 0.184}, {"x": 0.8284, "y": 0.184}, {"x": 0.8284, "y": 0.229}, {"x": 0.1734, "y": 0.229}], "id": 21, "page": 3}, {"category": "equation", "content": {"html": "<p id='22' data-category='equation'>$$\\begin{array}{l l l}{{h_{t}}}&{{=}}&{{\\mathrm{sigm}\\left(W^{\\mathrm{hx}}x_{t}+W^{\\mathrm{hh}}h_{t-1}\\right)}}\\\\ {{y_{t}}}&{{=}}&{{W^{\\mathrm{yh}}h_{t}}}\\end{array}$$</p>", "markdown": "$$\\begin{array}{l l l}{{h_{t}}}&{{=}}&{{\\mathrm{sigm}\\left(W^{\\mathrm{hx}}x_{t}+W^{\\mathrm{hh}}h_{t-1}\\right)}}\\\\ {{y_{t}}}&{{=}}&{{W^{\\mathrm{yh}}h_{t}}}\\end{array}$$", "text": "ht = sigm (Whx Xt + Whh ht-1) \nyt = Wyhht"}, "coordinates": [{"x": 0.3727, "y": 0.2356}, {"x": 0.6218, "y": 0.2356}, {"x": 0.6218, "y": 0.2733}, {"x": 0.3727, "y": 0.2733}], "id": 22, "page": 3}, {"category": "paragraph", "content": {"html": "<br><p id='23' data-category='paragraph' style='font-size:14px'>The RNN can easily map sequences to sequences whenever the alignment between the inputs the<br>outputs is known ahead of time. However, it is not clear how to apply an RNN to problems whose<br>input and the output sequences have different lengths with complicated and non-monotonic relation-<br>ships.</p>", "markdown": "The RNN can easily map sequences to sequences whenever the alignment between the inputs the\noutputs is known ahead of time. However, it is not clear how to apply an RNN to problems whose\ninput and the output sequences have different lengths with complicated and non-monotonic relation-\nships.", "text": "The RNN can easily map sequences to sequences whenever the alignment between the inputs the\noutputs is known ahead of time. However, it is not clear how to apply an RNN to problems whose\ninput and the output sequences have different lengths with complicated and non-monotonic relation-\nships."}, "coordinates": [{"x": 0.1729, "y": 0.279}, {"x": 0.8276, "y": 0.279}, {"x": 0.8276, "y": 0.3363}, {"x": 0.1729, "y": 0.3363}], "id": 23, "page": 3}, {"category": "paragraph", "content": {"html": "<br><p id='24' data-category='paragraph' style='font-size:14px'>The simplest strategy for general sequence learning is to map the input sequence to a fixed-sized<br>vector using one RNN, and then to map the vector to the target sequence with another RNN (this<br>approach has also been taken by Cho et al. [5]). While it could work in principle since the RNN is<br>provided with all the relevant information, it would be difficult to train the RNNs due to the resulting<br>long term dependencies (figure 1) [14, 4, 16, 15]. However, the Long Short-Term Memory (LSTM)<br>[16] is known to learn problems with long range temporal dependencies, so an LSTM may succeed<br>in this setting.</p>", "markdown": "The simplest strategy for general sequence learning is to map the input sequence to a fixed-sized\nvector using one RNN, and then to map the vector to the target sequence with another RNN (this\napproach has also been taken by Cho et al. [5]). While it could work in principle since the RNN is\nprovided with all the relevant information, it would be difficult to train the RNNs due to the resulting\nlong term dependencies (figure 1) [14, 4, 16, 15]. However, the Long Short-Term Memory (LSTM)\n[16] is known to learn problems with long range temporal dependencies, so an LSTM may succeed\nin this setting.", "text": "The simplest strategy for general sequence learning is to map the input sequence to a fixed-sized\nvector using one RNN, and then to map the vector to the target sequence with another RNN (this\napproach has also been taken by Cho et al. [5]). While it could work in principle since the RNN is\nprovided with all the relevant information, it would be difficult to train the RNNs due to the resulting\nlong term dependencies (figure 1) [14, 4, 16, 15]. However, the Long Short-Term Memory (LSTM)\n[16] is known to learn problems with long range temporal dependencies, so an LSTM may succeed\nin this setting."}, "coordinates": [{"x": 0.1737, "y": 0.3423}, {"x": 0.828, "y": 0.3423}, {"x": 0.828, "y": 0.4397}, {"x": 0.1737, "y": 0.4397}], "id": 24, "page": 3}, {"category": "paragraph", "content": {"html": "<p id='25' data-category='paragraph' style='font-size:14px'>The goal of the LSTM is to estimate the conditional probability p(y1, . : , YT' x1,···, xT) where<br>(x1, · · · , xT) is an input sequence and Y1, · · · , YT' is its corresponding output sequence whose length<br>T' may differ from T. The LSTM computes this conditional probability by first obtaining the fixed-<br>dimensional representation v of the input sequence (x1,.. · , xT) given by the last hidden state of the<br>LSTM, and then computing the probability of y1, · · · , YT' with a standard LSTM-LM formulation<br>whose initial hidden state is set to the representation v of x1, · · · , XT:</p>", "markdown": "The goal of the LSTM is to estimate the conditional probability p(y1, . : , YT' x1,···, xT) where\n(x1, · · · , xT) is an input sequence and Y1, · · · , YT' is its corresponding output sequence whose length\nT' may differ from T. The LSTM computes this conditional probability by first obtaining the fixed-\ndimensional representation v of the input sequence (x1,.. · , xT) given by the last hidden state of the\nLSTM, and then computing the probability of y1, · · · , YT' with a standard LSTM-LM formulation\nwhose initial hidden state is set to the representation v of x1, · · · , XT:", "text": "The goal of the LSTM is to estimate the conditional probability p(y1, . : , YT' x1,···, xT) where\n(x1, · · · , xT) is an input sequence and Y1, · · · , YT' is its corresponding output sequence whose length\nT' may differ from T. The LSTM computes this conditional probability by first obtaining the fixed-\ndimensional representation v of the input sequence (x1,.. · , xT) given by the last hidden state of the\nLSTM, and then computing the probability of y1, · · · , YT' with a standard LSTM-LM formulation\nwhose initial hidden state is set to the representation v of x1, · · · , XT:"}, "coordinates": [{"x": 0.173, "y": 0.4463}, {"x": 0.8277, "y": 0.4463}, {"x": 0.8277, "y": 0.5325}, {"x": 0.173, "y": 0.5325}], "id": 25, "page": 3}, {"category": "equation", "content": {"html": "<p id='26' data-category='equation'>$$p(y_{1},\\cdot\\cdot\\cdot,y_{T^{\\prime}}|x_{1},\\cdot\\cdot\\cdot,x_{T})=\\prod_{t=1}^{T^{\\prime}}p(y_{t}|v,y_{1},\\cdot\\cdot\\cdot,y_{t-1})$$</p>", "markdown": "$$p(y_{1},\\cdot\\cdot\\cdot,y_{T^{\\prime}}|x_{1},\\cdot\\cdot\\cdot,x_{T})=\\prod_{t=1}^{T^{\\prime}}p(y_{t}|v,y_{1},\\cdot\\cdot\\cdot,y_{t-1})$$", "text": "T' \np(y1, . ··· YT' |X1,...,XT) = II p(yt|v, y1, · ··· yt-1) \nt=1"}, "coordinates": [{"x": 0.3168, "y": 0.5405}, {"x": 0.6843, "y": 0.5405}, {"x": 0.6843, "y": 0.5808}, {"x": 0.3168, "y": 0.5808}], "id": 26, "page": 3}, {"category": "caption", "content": {"html": "<br><caption id='27' style='font-size:20px'>(1)</caption>", "markdown": "(1)", "text": "(1)"}, "coordinates": [{"x": 0.803, "y": 0.5523}, {"x": 0.8261, "y": 0.5523}, {"x": 0.8261, "y": 0.5682}, {"x": 0.803, "y": 0.5682}], "id": 27, "page": 3}, {"category": "paragraph", "content": {"html": "<p id='28' data-category='paragraph' style='font-size:14px'>In this equation, each p(yt|v, y1, · · · , yt-1) distribution is represented with a softmax over all the<br>words in the vocabulary. We use the LSTM formulation from Graves [10]. Note that we require that<br>each sentence ends with a special end-of-sentence symbol \"<EOS>\", which enables the model to<br>define a distribution over sequences of all possible lengths. The overall scheme is outlined in figure<br>· <EOS>\" and then uses<br>1, where the shown LSTM computes the representation of \"A\", \"B\" , \"C\" ,<br>this representation to compute the probability of \"W\", \"X\", \"Y\" \"Z\" , <EOS>\"<br>\"<br>,</p>", "markdown": "In this equation, each p(yt|v, y1, · · · , yt-1) distribution is represented with a softmax over all the\nwords in the vocabulary. We use the LSTM formulation from Graves [10]. Note that we require that\neach sentence ends with a special end-of-sentence symbol \"<EOS>\", which enables the model to\ndefine a distribution over sequences of all possible lengths. The overall scheme is outlined in figure\n· <EOS>\" and then uses\n1, where the shown LSTM computes the representation of \"A\", \"B\" , \"C\" ,\nthis representation to compute the probability of \"W\", \"X\", \"Y\" \"Z\" , <EOS>\"\n\"\n,", "text": "In this equation, each p(yt|v, y1, · · · , yt-1) distribution is represented with a softmax over all the\nwords in the vocabulary. We use the LSTM formulation from Graves [10]. Note that we require that\neach sentence ends with a special end-of-sentence symbol \"<EOS>\", which enables the model to\ndefine a distribution over sequences of all possible lengths. The overall scheme is outlined in figure\n· <EOS>\" and then uses\n1, where the shown LSTM computes the representation of \"A\", \"B\" , \"C\" ,\nthis representation to compute the probability of \"W\", \"X\", \"Y\" \"Z\" , <EOS>\"\n\"\n,"}, "coordinates": [{"x": 0.1727, "y": 0.5881}, {"x": 0.8281, "y": 0.5881}, {"x": 0.8281, "y": 0.6724}, {"x": 0.1727, "y": 0.6724}], "id": 28, "page": 3}, {"category": "paragraph", "content": {"html": "<br><p id='29' data-category='paragraph' style='font-size:14px'>Our actual models differ from the above description in three important ways. First, we used two<br>different LSTMs: one for the input sequence and another for the output sequence, because doing<br>so increases the number model parameters at negligible computational cost and makes it natural to<br>train the LSTM on multiple language pairs simultaneously [18]. Second, we found that deep LSTMs<br>significantly outperformed shallow LSTMs, so we chose an LSTM with four layers. Third, we found<br>it extremely valuable to reverse the order of the words of the input sentence. So for example, instead<br>of mapping the sentence a, b, c to the sentence �, �, % the LSTM is asked to map c, b, a to �, �, %<br>where �, �, づ is the translation of a, b, c. This way, a is in close proximity to �, bis fairly close to �,<br>and so on, a fact that makes it easy for SGD to \"establish communication\" between the input and the<br>output. We found this simple data transformation to greatly improve the performance of the LSTM.</p>", "markdown": "Our actual models differ from the above description in three important ways. First, we used two\ndifferent LSTMs: one for the input sequence and another for the output sequence, because doing\nso increases the number model parameters at negligible computational cost and makes it natural to\ntrain the LSTM on multiple language pairs simultaneously [18]. Second, we found that deep LSTMs\nsignificantly outperformed shallow LSTMs, so we chose an LSTM with four layers. Third, we found\nit extremely valuable to reverse the order of the words of the input sentence. So for example, instead\nof mapping the sentence a, b, c to the sentence �, �, % the LSTM is asked to map c, b, a to �, �, %\nwhere �, �, づ is the translation of a, b, c. This way, a is in close proximity to �, bis fairly close to �,\nand so on, a fact that makes it easy for SGD to \"establish communication\" between the input and the\noutput. We found this simple data transformation to greatly improve the performance of the LSTM.", "text": "Our actual models differ from the above description in three important ways. First, we used two\ndifferent LSTMs: one for the input sequence and another for the output sequence, because doing\nso increases the number model parameters at negligible computational cost and makes it natural to\ntrain the LSTM on multiple language pairs simultaneously [18]. Second, we found that deep LSTMs\nsignificantly outperformed shallow LSTMs, so we chose an LSTM with four layers. Third, we found\nit extremely valuable to reverse the order of the words of the input sentence. So for example, instead\nof mapping the sentence a, b, c to the sentence �, �, % the LSTM is asked to map c, b, a to �, �, %\nwhere �, �, づ is the translation of a, b, c. This way, a is in close proximity to �, bis fairly close to �,\nand so on, a fact that makes it easy for SGD to \"establish communication\" between the input and the\noutput. We found this simple data transformation to greatly improve the performance of the LSTM."}, "coordinates": [{"x": 0.1734, "y": 0.6777}, {"x": 0.8279, "y": 0.6777}, {"x": 0.8279, "y": 0.819}, {"x": 0.1734, "y": 0.819}], "id": 29, "page": 3}, {"category": "paragraph", "content": {"html": "<p id='30' data-category='paragraph' style='font-size:22px'>3 Experiments</p>", "markdown": "3 Experiments", "text": "3 Experiments"}, "coordinates": [{"x": 0.1727, "y": 0.8368}, {"x": 0.3153, "y": 0.8368}, {"x": 0.3153, "y": 0.8545}, {"x": 0.1727, "y": 0.8545}], "id": 30, "page": 3}, {"category": "paragraph", "content": {"html": "<p id='31' data-category='paragraph' style='font-size:14px'>We applied our method to the WMT' 14 English to French MT task in two ways. We used it to<br>directly translate the input sentence without using a reference SMT system and we it to rescore the<br>n-best lists of an SMT baseline. We report the accuracy of these translation methods, present sample<br>translations, and visualize the resulting sentence representation.</p>", "markdown": "We applied our method to the WMT' 14 English to French MT task in two ways. We used it to\ndirectly translate the input sentence without using a reference SMT system and we it to rescore the\nn-best lists of an SMT baseline. We report the accuracy of these translation methods, present sample\ntranslations, and visualize the resulting sentence representation.", "text": "We applied our method to the WMT' 14 English to French MT task in two ways. We used it to\ndirectly translate the input sentence without using a reference SMT system and we it to rescore the\nn-best lists of an SMT baseline. We report the accuracy of these translation methods, present sample\ntranslations, and visualize the resulting sentence representation."}, "coordinates": [{"x": 0.173, "y": 0.8682}, {"x": 0.8279, "y": 0.8682}, {"x": 0.8279, "y": 0.9255}, {"x": 0.173, "y": 0.9255}], "id": 31, "page": 3}, {"category": "footer", "content": {"html": "<footer id='32' style='font-size:14px'>3</footer>", "markdown": "3", "text": "3"}, "coordinates": [{"x": 0.4936, "y": 0.9484}, {"x": 0.507, "y": 0.9484}, {"x": 0.507, "y": 0.9623}, {"x": 0.4936, "y": 0.9623}], "id": 32, "page": 3}, {"category": "paragraph", "content": {"html": "<p id='33' data-category='paragraph' style='font-size:14px'>3.1 Dataset details</p>", "markdown": "3.1 Dataset details", "text": "3.1 Dataset details"}, "coordinates": [{"x": 0.1728, "y": 0.1042}, {"x": 0.3182, "y": 0.1042}, {"x": 0.3182, "y": 0.1192}, {"x": 0.1728, "y": 0.1192}], "id": 33, "page": 4}, {"category": "paragraph", "content": {"html": "<p id='34' data-category='paragraph' style='font-size:14px'>We used the WMT' 14 English to French dataset. We trained our models on a subset of 12M sen-<br>tences consisting of 348M French words and 304M English words, which is a clean \"selected\"<br>subset from [29]. We chose this translation task and this specific training set subset because of the<br>public availability of a tokenized training and test set together with 1000-best lists from the baseline<br>SMT [29].</p>", "markdown": "We used the WMT' 14 English to French dataset. We trained our models on a subset of 12M sen-\ntences consisting of 348M French words and 304M English words, which is a clean \"selected\"\nsubset from [29]. We chose this translation task and this specific training set subset because of the\npublic availability of a tokenized training and test set together with 1000-best lists from the baseline\nSMT [29].", "text": "We used the WMT' 14 English to French dataset. We trained our models on a subset of 12M sen-\ntences consisting of 348M French words and 304M English words, which is a clean \"selected\"\nsubset from [29]. We chose this translation task and this specific training set subset because of the\npublic availability of a tokenized training and test set together with 1000-best lists from the baseline\nSMT [29]."}, "coordinates": [{"x": 0.1733, "y": 0.1313}, {"x": 0.828, "y": 0.1313}, {"x": 0.828, "y": 0.2009}, {"x": 0.1733, "y": 0.2009}], "id": 34, "page": 4}, {"category": "paragraph", "content": {"html": "<p id='35' data-category='paragraph' style='font-size:14px'>As typical neural language models rely on a vector representation for each word, we used a fixed<br>vocabulary for both languages. We used 160,000 of the most frequent words for the source language<br>and 80,000 of the most frequent words for the target language. Every out-of-vocabulary word was<br>replaced with a special \"UNK\" token.</p>", "markdown": "As typical neural language models rely on a vector representation for each word, we used a fixed\nvocabulary for both languages. We used 160,000 of the most frequent words for the source language\nand 80,000 of the most frequent words for the target language. Every out-of-vocabulary word was\nreplaced with a special \"UNK\" token.", "text": "As typical neural language models rely on a vector representation for each word, we used a fixed\nvocabulary for both languages. We used 160,000 of the most frequent words for the source language\nand 80,000 of the most frequent words for the target language. Every out-of-vocabulary word was\nreplaced with a special \"UNK\" token."}, "coordinates": [{"x": 0.1734, "y": 0.2079}, {"x": 0.8276, "y": 0.2079}, {"x": 0.8276, "y": 0.265}, {"x": 0.1734, "y": 0.265}], "id": 35, "page": 4}, {"category": "paragraph", "content": {"html": "<p id='36' data-category='paragraph' style='font-size:18px'>3.2 Decoding and Rescoring</p>", "markdown": "3.2 Decoding and Rescoring", "text": "3.2 Decoding and Rescoring"}, "coordinates": [{"x": 0.1716, "y": 0.2812}, {"x": 0.3863, "y": 0.2812}, {"x": 0.3863, "y": 0.2973}, {"x": 0.1716, "y": 0.2973}], "id": 36, "page": 4}, {"category": "paragraph", "content": {"html": "<p id='37' data-category='paragraph' style='font-size:14px'>The core of our experiments involved training a large deep LSTM on many sentence pairs. We<br>trained it by maximizing the log probability of a correct translation T given the source sentence S,<br>so the training objective is</p>", "markdown": "The core of our experiments involved training a large deep LSTM on many sentence pairs. We\ntrained it by maximizing the log probability of a correct translation T given the source sentence S,\nso the training objective is", "text": "The core of our experiments involved training a large deep LSTM on many sentence pairs. We\ntrained it by maximizing the log probability of a correct translation T given the source sentence S,\nso the training objective is"}, "coordinates": [{"x": 0.1724, "y": 0.3072}, {"x": 0.8274, "y": 0.3072}, {"x": 0.8274, "y": 0.3496}, {"x": 0.1724, "y": 0.3496}], "id": 37, "page": 4}, {"category": "equation", "content": {"html": "<br><p id='38' data-category='equation'>$$\\textstyle{1/|{\\cal G}|}_{(T,S)\\in{\\cal S}}\\log p(T|S)$$</p>", "markdown": "$$\\textstyle{1/|{\\cal G}|}_{(T,S)\\in{\\cal S}}\\log p(T|S)$$", "text": "1/S � log p(T|S) \n(T,S)ES"}, "coordinates": [{"x": 0.4154, "y": 0.3504}, {"x": 0.5852, "y": 0.3504}, {"x": 0.5852, "y": 0.3828}, {"x": 0.4154, "y": 0.3828}], "id": 38, "page": 4}, {"category": "paragraph", "content": {"html": "<br><p id='39' data-category='paragraph' style='font-size:16px'>where s is the training set. Once training is complete, we produce translations by finding the most<br>likely translation according to the LSTM:</p>", "markdown": "where s is the training set. Once training is complete, we produce translations by finding the most\nlikely translation according to the LSTM:", "text": "where s is the training set. Once training is complete, we produce translations by finding the most\nlikely translation according to the LSTM:"}, "coordinates": [{"x": 0.1728, "y": 0.388}, {"x": 0.829, "y": 0.388}, {"x": 0.829, "y": 0.4174}, {"x": 0.1728, "y": 0.4174}], "id": 39, "page": 4}, {"category": "equation", "content": {"html": "<p id='40' data-category='equation'>$${\\hat{T}}=\\arg\\operatorname*{max}_{T}p(T|S)$$</p>", "markdown": "$${\\hat{T}}=\\arg\\operatorname*{max}_{T}p(T|S)$$", "text": "T = arg max p(T|S) \nT"}, "coordinates": [{"x": 0.4267, "y": 0.4246}, {"x": 0.5727, "y": 0.4246}, {"x": 0.5727, "y": 0.4463}, {"x": 0.4267, "y": 0.4463}], "id": 40, "page": 4}, {"category": "caption", "content": {"html": "<br><caption id='41' style='font-size:20px'>(2)</caption>", "markdown": "(2)", "text": "(2)"}, "coordinates": [{"x": 0.803, "y": 0.4268}, {"x": 0.8259, "y": 0.4268}, {"x": 0.8259, "y": 0.4423}, {"x": 0.803, "y": 0.4423}], "id": 41, "page": 4}, {"category": "paragraph", "content": {"html": "<p id='42' data-category='paragraph' style='font-size:14px'>We search for the most likely translation using a simple left-to-right beam search decoder which<br>maintains a small number B of partial hypotheses, where a partial hypothesis is a prefix of some<br>translation. At each timestep we extend each partial hypothesis in the beam with every possible<br>word in the vocabulary. This greatly increases the number of the hypotheses so we discard all but<br>the B most likely hypotheses according to the model's log probability. As soon as the \"<EOS>\"<br>symbol is appended to a hypothesis, it is removed from the beam and is added to the set of complete<br>hypotheses. While this decoder is approximate, it is simple to implement. Interestingly, our system<br>performs well even with a beam size of 1, and a beam of size 2 provides most of the benefits of beam<br>search (Table 1).</p>", "markdown": "We search for the most likely translation using a simple left-to-right beam search decoder which\nmaintains a small number B of partial hypotheses, where a partial hypothesis is a prefix of some\ntranslation. At each timestep we extend each partial hypothesis in the beam with every possible\nword in the vocabulary. This greatly increases the number of the hypotheses so we discard all but\nthe B most likely hypotheses according to the model's log probability. As soon as the \"<EOS>\"\nsymbol is appended to a hypothesis, it is removed from the beam and is added to the set of complete\nhypotheses. While this decoder is approximate, it is simple to implement. Interestingly, our system\nperforms well even with a beam size of 1, and a beam of size 2 provides most of the benefits of beam\nsearch (Table 1).", "text": "We search for the most likely translation using a simple left-to-right beam search decoder which\nmaintains a small number B of partial hypotheses, where a partial hypothesis is a prefix of some\ntranslation. At each timestep we extend each partial hypothesis in the beam with every possible\nword in the vocabulary. This greatly increases the number of the hypotheses so we discard all but\nthe B most likely hypotheses according to the model's log probability. As soon as the \"<EOS>\"\nsymbol is appended to a hypothesis, it is removed from the beam and is added to the set of complete\nhypotheses. While this decoder is approximate, it is simple to implement. Interestingly, our system\nperforms well even with a beam size of 1, and a beam of size 2 provides most of the benefits of beam\nsearch (Table 1)."}, "coordinates": [{"x": 0.1725, "y": 0.4552}, {"x": 0.8277, "y": 0.4552}, {"x": 0.8277, "y": 0.5808}, {"x": 0.1725, "y": 0.5808}], "id": 42, "page": 4}, {"category": "paragraph", "content": {"html": "<br><p id='43' data-category='paragraph' style='font-size:14px'>We also used the LSTM to rescore the 1000-best lists produced by the baseline system [29]. To<br>rescore an n-best list, we computed the log probability of every hypothesis with our LSTM and took<br>an even average with their score and the LSTM's score.</p>", "markdown": "We also used the LSTM to rescore the 1000-best lists produced by the baseline system [29]. To\nrescore an n-best list, we computed the log probability of every hypothesis with our LSTM and took\nan even average with their score and the LSTM's score.", "text": "We also used the LSTM to rescore the 1000-best lists produced by the baseline system [29]. To\nrescore an n-best list, we computed the log probability of every hypothesis with our LSTM and took\nan even average with their score and the LSTM's score."}, "coordinates": [{"x": 0.1738, "y": 0.5873}, {"x": 0.8274, "y": 0.5873}, {"x": 0.8274, "y": 0.6313}, {"x": 0.1738, "y": 0.6313}], "id": 43, "page": 4}, {"category": "heading1", "content": {"html": "<h1 id='44' style='font-size:16px'>3.3 Reversing the Source Sentences</h1>", "markdown": "# 3.3 Reversing the Source Sentences", "text": "3.3 Reversing the Source Sentences"}, "coordinates": [{"x": 0.1722, "y": 0.6472}, {"x": 0.4348, "y": 0.6472}, {"x": 0.4348, "y": 0.6631}, {"x": 0.1722, "y": 0.6631}], "id": 44, "page": 4}, {"category": "paragraph", "content": {"html": "<p id='45' data-category='paragraph' style='font-size:14px'>While the LSTM is capable of solving problems with long term dependencies, we discovered that<br>the LSTM learns much better when the source sentences are reversed (the target sentences are not<br>reversed). By doing so, the LSTM's test perplexity dropped from 5.8 to 4.7, and the test BLEU<br>scores of its decoded translations increased from 25.9 to 30.6.</p>", "markdown": "While the LSTM is capable of solving problems with long term dependencies, we discovered that\nthe LSTM learns much better when the source sentences are reversed (the target sentences are not\nreversed). By doing so, the LSTM's test perplexity dropped from 5.8 to 4.7, and the test BLEU\nscores of its decoded translations increased from 25.9 to 30.6.", "text": "While the LSTM is capable of solving problems with long term dependencies, we discovered that\nthe LSTM learns much better when the source sentences are reversed (the target sentences are not\nreversed). By doing so, the LSTM's test perplexity dropped from 5.8 to 4.7, and the test BLEU\nscores of its decoded translations increased from 25.9 to 30.6."}, "coordinates": [{"x": 0.1734, "y": 0.6734}, {"x": 0.827, "y": 0.6734}, {"x": 0.827, "y": 0.7299}, {"x": 0.1734, "y": 0.7299}], "id": 45, "page": 4}, {"category": "paragraph", "content": {"html": "<p id='46' data-category='paragraph' style='font-size:14px'>While we do not have a complete explanation to this phenomenon, we believe that it is caused by<br>the introduction of many short term dependencies to the dataset. Normally, when we concatenate a<br>source sentence with a target sentence, each word in the source sentence is far from its corresponding<br>word in the target sentence. As a result, the problem has a large \"minimal time lag\" [17]. By<br>reversing the words in the source sentence, the average distance between corresponding words in<br>the source and target language is unchanged. However, the first few words in the source language<br>are now very close to the first few words in the target language, so the problem's minimal time lag is<br>greatly reduced. Thus, backpropagation has an easier time \"establishing communication\" between<br>the source sentence and the target sentence, which in turn results in substantially improved overall<br>performance.</p>", "markdown": "While we do not have a complete explanation to this phenomenon, we believe that it is caused by\nthe introduction of many short term dependencies to the dataset. Normally, when we concatenate a\nsource sentence with a target sentence, each word in the source sentence is far from its corresponding\nword in the target sentence. As a result, the problem has a large \"minimal time lag\" [17]. By\nreversing the words in the source sentence, the average distance between corresponding words in\nthe source and target language is unchanged. However, the first few words in the source language\nare now very close to the first few words in the target language, so the problem's minimal time lag is\ngreatly reduced. Thus, backpropagation has an easier time \"establishing communication\" between\nthe source sentence and the target sentence, which in turn results in substantially improved overall\nperformance.", "text": "While we do not have a complete explanation to this phenomenon, we believe that it is caused by\nthe introduction of many short term dependencies to the dataset. Normally, when we concatenate a\nsource sentence with a target sentence, each word in the source sentence is far from its corresponding\nword in the target sentence. As a result, the problem has a large \"minimal time lag\" [17]. By\nreversing the words in the source sentence, the average distance between corresponding words in\nthe source and target language is unchanged. However, the first few words in the source language\nare now very close to the first few words in the target language, so the problem's minimal time lag is\ngreatly reduced. Thus, backpropagation has an easier time \"establishing communication\" between\nthe source sentence and the target sentence, which in turn results in substantially improved overall\nperformance."}, "coordinates": [{"x": 0.1739, "y": 0.7373}, {"x": 0.8274, "y": 0.7373}, {"x": 0.8274, "y": 0.8744}, {"x": 0.1739, "y": 0.8744}], "id": 46, "page": 4}, {"category": "paragraph", "content": {"html": "<p id='47' data-category='paragraph' style='font-size:14px'>Initially, we believed that reversing the input sentences would only lead to more confident predic-<br>tions in the early parts of the target sentence and to less confident predictions in the later parts. How-<br>ever, LSTMs trained on reversed source sentences did much better on long sentences than LSTMs</p>", "markdown": "Initially, we believed that reversing the input sentences would only lead to more confident predic-\ntions in the early parts of the target sentence and to less confident predictions in the later parts. How-\never, LSTMs trained on reversed source sentences did much better on long sentences than LSTMs", "text": "Initially, we believed that reversing the input sentences would only lead to more confident predic-\ntions in the early parts of the target sentence and to less confident predictions in the later parts. How-\never, LSTMs trained on reversed source sentences did much better on long sentences than LSTMs"}, "coordinates": [{"x": 0.1735, "y": 0.8833}, {"x": 0.8281, "y": 0.8833}, {"x": 0.8281, "y": 0.9264}, {"x": 0.1735, "y": 0.9264}], "id": 47, "page": 4}, {"category": "footer", "content": {"html": "<footer id='48' style='font-size:14px'>4</footer>", "markdown": "4", "text": "4"}, "coordinates": [{"x": 0.493, "y": 0.9489}, {"x": 0.5079, "y": 0.9489}, {"x": 0.5079, "y": 0.9614}, {"x": 0.493, "y": 0.9614}], "id": 48, "page": 4}, {"category": "paragraph", "content": {"html": "<p id='49' data-category='paragraph' style='font-size:16px'>trained on the raw source sentences (see sec. 3.7), which suggests that reversing the input sentences<br>results in LSTMs with better memory utilization.</p>", "markdown": "trained on the raw source sentences (see sec. 3.7), which suggests that reversing the input sentences\nresults in LSTMs with better memory utilization.", "text": "trained on the raw source sentences (see sec. 3.7), which suggests that reversing the input sentences\nresults in LSTMs with better memory utilization."}, "coordinates": [{"x": 0.172, "y": 0.1051}, {"x": 0.828, "y": 0.1051}, {"x": 0.828, "y": 0.1341}, {"x": 0.172, "y": 0.1341}], "id": 49, "page": 5}, {"category": "paragraph", "content": {"html": "<p id='50' data-category='paragraph' style='font-size:22px'>3.4 Training details</p>", "markdown": "3.4 Training details", "text": "3.4 Training details"}, "coordinates": [{"x": 0.1732, "y": 0.1498}, {"x": 0.3253, "y": 0.1498}, {"x": 0.3253, "y": 0.1652}, {"x": 0.1732, "y": 0.1652}], "id": 50, "page": 5}, {"category": "paragraph", "content": {"html": "<p id='51' data-category='paragraph' style='font-size:18px'>We found that the LSTM models are fairly easy to train. We used deep LSTMs with 4 layers,<br>with 1000 cells at each layer and 1000 dimensional word embeddings, with an input vocabulary<br>of 160,000 and an output vocabulary of 80,000. Thus the deep LSTM uses 8000 real numbers to<br>represent a sentence. We found deep LSTMs to significantly outperform shallow LSTMs, where<br>each additional layer reduced perplexity by nearly 10%, possibly due to their much larger hidden<br>state. We used a naive softmax over 80,000 words at each output. The resulting LSTM has 384M<br>parameters of which 64M are pure recurrent connections (32M for the \"encoder\" LSTM and 32M<br>for the \"decoder\" LSTM). The complete training details are given below:</p>", "markdown": "We found that the LSTM models are fairly easy to train. We used deep LSTMs with 4 layers,\nwith 1000 cells at each layer and 1000 dimensional word embeddings, with an input vocabulary\nof 160,000 and an output vocabulary of 80,000. Thus the deep LSTM uses 8000 real numbers to\nrepresent a sentence. We found deep LSTMs to significantly outperform shallow LSTMs, where\neach additional layer reduced perplexity by nearly 10%, possibly due to their much larger hidden\nstate. We used a naive softmax over 80,000 words at each output. The resulting LSTM has 384M\nparameters of which 64M are pure recurrent connections (32M for the \"encoder\" LSTM and 32M\nfor the \"decoder\" LSTM). The complete training details are given below:", "text": "We found that the LSTM models are fairly easy to train. We used deep LSTMs with 4 layers,\nwith 1000 cells at each layer and 1000 dimensional word embeddings, with an input vocabulary\nof 160,000 and an output vocabulary of 80,000. Thus the deep LSTM uses 8000 real numbers to\nrepresent a sentence. We found deep LSTMs to significantly outperform shallow LSTMs, where\neach additional layer reduced perplexity by nearly 10%, possibly due to their much larger hidden\nstate. We used a naive softmax over 80,000 words at each output. The resulting LSTM has 384M\nparameters of which 64M are pure recurrent connections (32M for the \"encoder\" LSTM and 32M\nfor the \"decoder\" LSTM). The complete training details are given below:"}, "coordinates": [{"x": 0.173, "y": 0.1757}, {"x": 0.8275, "y": 0.1757}, {"x": 0.8275, "y": 0.288}, {"x": 0.173, "y": 0.288}], "id": 51, "page": 5}, {"category": "list", "content": {"html": "<p id='52' data-category='list' style='font-size:18px'>· We initialized all of the LSTM's parameters with the uniform distribution between -0.08<br>and 0.08<br>· We used stochastic gradient descent without momentum, with a fixed learning rate of 0.7.<br>After 5 epochs, we begun halving the learning rate every half epoch. We trained our models<br>for a total of 7.5 epochs.<br>· We used batches of 128 sequences for the gradient and divided it the size of the batch<br>(namely, 128).<br>· Although LSTMs tend to not suffer from the vanishing gradient problem, they can have<br>exploding gradients. Thus we enforced a hard constraint on the norm of the gradient [10,<br>25] by scaling it when its norm exceeded a threshold. For each training batch, we compute<br>s = 1191|2, where g is the gradient divided by 128. If s > 5, we set g = 5g.<br>s<br>· Different sentences have different lengths. Most sentences are short (e.g., length 20-30)<br>but some sentences are long (e.g., length > 100), so a minibatch of 128 randomly chosen<br>training sentences will have many short sentences and few long sentences, and as a result,<br>much of the computation in the minibatch is wasted. To address this problem, we made sure<br>that all sentences in a minibatch are roughly of the same length, yielding a 2x speedup.</p>", "markdown": "- · We initialized all of the LSTM's parameters with the uniform distribution between -0.08\n- and 0.08\n- · We used stochastic gradient descent without momentum, with a fixed learning rate of 0.7.\n- After 5 epochs, we begun halving the learning rate every half epoch. We trained our models\n- for a total of 7.5 epochs.\n- · We used batches of 128 sequences for the gradient and divided it the size of the batch\n- (namely, 128).\n- · Although LSTMs tend to not suffer from the vanishing gradient problem, they can have\n- exploding gradients. Thus we enforced a hard constraint on the norm of the gradient [10,\n- 25] by scaling it when its norm exceeded a threshold. For each training batch, we compute\n- s = 1191|2, where g is the gradient divided by 128. If s > 5, we set g = 5g.\n- s\n- · Different sentences have different lengths. Most sentences are short (e.g., length 20-30)\n- but some sentences are long (e.g., length > 100), so a minibatch of 128 randomly chosen\n- training sentences will have many short sentences and few long sentences, and as a result,\n- much of the computation in the minibatch is wasted. To address this problem, we made sure\n- that all sentences in a minibatch are roughly of the same length, yielding a 2x speedup.\n", "text": "· We initialized all of the LSTM's parameters with the uniform distribution between -0.08\nand 0.08\n· We used stochastic gradient descent without momentum, with a fixed learning rate of 0.7.\nAfter 5 epochs, we begun halving the learning rate every half epoch. We trained our models\nfor a total of 7.5 epochs.\n· We used batches of 128 sequences for the gradient and divided it the size of the batch\n(namely, 128).\n· Although LSTMs tend to not suffer from the vanishing gradient problem, they can have\nexploding gradients. Thus we enforced a hard constraint on the norm of the gradient [10,\n25] by scaling it when its norm exceeded a threshold. For each training batch, we compute\ns = 1191|2, where g is the gradient divided by 128. If s > 5, we set g = 5g.\ns\n· Different sentences have different lengths. Most sentences are short (e.g., length 20-30)\nbut some sentences are long (e.g., length > 100), so a minibatch of 128 randomly chosen\ntraining sentences will have many short sentences and few long sentences, and as a result,\nmuch of the computation in the minibatch is wasted. To address this problem, we made sure\nthat all sentences in a minibatch are roughly of the same length, yielding a 2x speedup."}, "coordinates": [{"x": 0.2139, "y": 0.2991}, {"x": 0.8294, "y": 0.2991}, {"x": 0.8294, "y": 0.5425}, {"x": 0.2139, "y": 0.5425}], "id": 52, "page": 5}, {"category": "heading1", "content": {"html": "<h1 id='53' style='font-size:20px'>3.5 Parallelization</h1>", "markdown": "# 3.5 Parallelization", "text": "3.5 Parallelization"}, "coordinates": [{"x": 0.1732, "y": 0.5599}, {"x": 0.3169, "y": 0.5599}, {"x": 0.3169, "y": 0.575}, {"x": 0.1732, "y": 0.575}], "id": 53, "page": 5}, {"category": "paragraph", "content": {"html": "<p id='54' data-category='paragraph' style='font-size:16px'>A C++ implementation of deep LSTM with the configuration from the previous section on a sin-<br>gle GPU processes a speed of approximately 1,700 words per second. This was too slow for our<br>purposes, so we parallelized our model using an 8-GPU machine. Each layer of the LSTM was<br>executed on a different GPU and communicated its activations to the next GPU / layer as soon as<br>they were computed. Our models have 4 layers of LSTMs, each of which resides on a separate<br>GPU. The remaining 4 GPUs were used to parallelize the softmax, so each GPU was responsible<br>for multiplying by a 1000 x 20000 matrix. The resulting implementation achieved a speed of 6,300<br>(both English and French) words per second with a minibatch size of 128. Training took about a ten<br>days with this implementation.</p>", "markdown": "A C++ implementation of deep LSTM with the configuration from the previous section on a sin-\ngle GPU processes a speed of approximately 1,700 words per second. This was too slow for our\npurposes, so we parallelized our model using an 8-GPU machine. Each layer of the LSTM was\nexecuted on a different GPU and communicated its activations to the next GPU / layer as soon as\nthey were computed. Our models have 4 layers of LSTMs, each of which resides on a separate\nGPU. The remaining 4 GPUs were used to parallelize the softmax, so each GPU was responsible\nfor multiplying by a 1000 x 20000 matrix. The resulting implementation achieved a speed of 6,300\n(both English and French) words per second with a minibatch size of 128. Training took about a ten\ndays with this implementation.", "text": "A C++ implementation of deep LSTM with the configuration from the previous section on a sin-\ngle GPU processes a speed of approximately 1,700 words per second. This was too slow for our\npurposes, so we parallelized our model using an 8-GPU machine. Each layer of the LSTM was\nexecuted on a different GPU and communicated its activations to the next GPU / layer as soon as\nthey were computed. Our models have 4 layers of LSTMs, each of which resides on a separate\nGPU. The remaining 4 GPUs were used to parallelize the softmax, so each GPU was responsible\nfor multiplying by a 1000 x 20000 matrix. The resulting implementation achieved a speed of 6,300\n(both English and French) words per second with a minibatch size of 128. Training took about a ten\ndays with this implementation."}, "coordinates": [{"x": 0.1731, "y": 0.5857}, {"x": 0.8274, "y": 0.5857}, {"x": 0.8274, "y": 0.7124}, {"x": 0.1731, "y": 0.7124}], "id": 54, "page": 5}, {"category": "paragraph", "content": {"html": "<p id='55' data-category='paragraph' style='font-size:22px'>3.6 Experimental Results</p>", "markdown": "3.6 Experimental Results", "text": "3.6 Experimental Results"}, "coordinates": [{"x": 0.1725, "y": 0.7279}, {"x": 0.3659, "y": 0.7279}, {"x": 0.3659, "y": 0.7432}, {"x": 0.1725, "y": 0.7432}], "id": 55, "page": 5}, {"category": "paragraph", "content": {"html": "<p id='56' data-category='paragraph' style='font-size:16px'>We used the cased BLEU score [24] to evaluate the quality of our translations. We computed our<br>BLEU scores using multi-bleu · pl 1 the tokenized predictions and ground truth. This way<br>on<br>of evaluating the BELU score is consistent with [5] and [2], and reproduces the 33.3 score of [29].<br>However, if we evaluate the best WMT' 14 system [9] (whose predictions can be downloaded from<br>statmt · org \\matrix) in this manner, we get 37.0, which is greater than the 35.8 reported by<br>statmt · org \\matrix.</p>", "markdown": "We used the cased BLEU score [24] to evaluate the quality of our translations. We computed our\nBLEU scores using multi-bleu · pl 1 the tokenized predictions and ground truth. This way\non\nof evaluating the BELU score is consistent with [5] and [2], and reproduces the 33.3 score of [29].\nHowever, if we evaluate the best WMT' 14 system [9] (whose predictions can be downloaded from\nstatmt · org \\matrix) in this manner, we get 37.0, which is greater than the 35.8 reported by\nstatmt · org \\matrix.", "text": "We used the cased BLEU score [24] to evaluate the quality of our translations. We computed our\nBLEU scores using multi-bleu · pl 1 the tokenized predictions and ground truth. This way\non\nof evaluating the BELU score is consistent with [5] and [2], and reproduces the 33.3 score of [29].\nHowever, if we evaluate the best WMT' 14 system [9] (whose predictions can be downloaded from\nstatmt · org \\matrix) in this manner, we get 37.0, which is greater than the 35.8 reported by\nstatmt · org \\matrix."}, "coordinates": [{"x": 0.1732, "y": 0.7542}, {"x": 0.8279, "y": 0.7542}, {"x": 0.8279, "y": 0.8383}, {"x": 0.1732, "y": 0.8383}], "id": 56, "page": 5}, {"category": "paragraph", "content": {"html": "<br><p id='57' data-category='paragraph' style='font-size:16px'>The results are presented in tables 1 and 2. Our best results are obtained with an ensemble of LSTMs<br>that differ in their random initializations and in the random order of minibatches. While the decoded<br>translations of the LSTM ensemble do not outperform the best WMT' 14 system, it is the first time<br>that a pure neural translation system outperforms a phrase-based SMT baseline on a large scale MT</p>", "markdown": "The results are presented in tables 1 and 2. Our best results are obtained with an ensemble of LSTMs\nthat differ in their random initializations and in the random order of minibatches. While the decoded\ntranslations of the LSTM ensemble do not outperform the best WMT' 14 system, it is the first time\nthat a pure neural translation system outperforms a phrase-based SMT baseline on a large scale MT", "text": "The results are presented in tables 1 and 2. Our best results are obtained with an ensemble of LSTMs\nthat differ in their random initializations and in the random order of minibatches. While the decoded\ntranslations of the LSTM ensemble do not outperform the best WMT' 14 system, it is the first time\nthat a pure neural translation system outperforms a phrase-based SMT baseline on a large scale MT"}, "coordinates": [{"x": 0.1736, "y": 0.8449}, {"x": 0.8279, "y": 0.8449}, {"x": 0.8279, "y": 0.9024}, {"x": 0.1736, "y": 0.9024}], "id": 57, "page": 5}, {"category": "footnote", "content": {"html": "<p id='58' data-category='footnote' style='font-size:14px'>1There several variants of the BLEU score, and each variant is defined with a perl script.</p>", "markdown": "1There several variants of the BLEU score, and each variant is defined with a perl script.", "text": "1There several variants of the BLEU score, and each variant is defined with a perl script."}, "coordinates": [{"x": 0.1976, "y": 0.9107}, {"x": 0.7159, "y": 0.9107}, {"x": 0.7159, "y": 0.9259}, {"x": 0.1976, "y": 0.9259}], "id": 58, "page": 5}, {"category": "footer", "content": {"html": "<footer id='59' style='font-size:18px'>5</footer>", "markdown": "5", "text": "5"}, "coordinates": [{"x": 0.4929, "y": 0.9481}, {"x": 0.5075, "y": 0.9481}, {"x": 0.5075, "y": 0.9625}, {"x": 0.4929, "y": 0.9625}], "id": 59, "page": 5}], "model": "document-parse-250618", "ocr": true, "usage": {"pages": 5}}