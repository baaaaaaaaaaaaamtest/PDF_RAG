{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "127c7dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.utils import *\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b338293",
   "metadata": {},
   "source": [
    "### 파일 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e1c1619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 페이지 수: 9\n",
      "./test/seq2seq\n",
      "분할 PDF 생성: ./test/seq2seq_0000_0004.pdf\n",
      "./test/seq2seq\n",
      "분할 PDF 생성: ./test/seq2seq_0005_0008.pdf\n"
     ]
    }
   ],
   "source": [
    "filepath = \"./test/seq2seq.pdf\"\n",
    "\n",
    "file_paths = split_pdf(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033da952",
   "metadata": {},
   "source": [
    "### upstage에서 데이터 파싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03bc73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_file_path = upstage_layout_analysis(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bf0b558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./test/seq2seq_0000_0004.json', './test/seq2seq_0005_0008.json']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analysis_file_path = ['./test/hana_0000_0004.json', './test/hana_0005_0008.json']\n",
    "analysis_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ac87a3",
   "metadata": {},
   "source": [
    "### 각 파일 분리하여 저장\n",
    "- page 번호 조정\n",
    "- id 조정\n",
    "- html id tag 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be9c6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test/seq2seq\n",
      "0\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "element_content = []\n",
    "output_file_basename = os.path.splitext(filepath)[0]\n",
    "print(output_file_basename)\n",
    "page_range = 5\n",
    "last_number = 0\n",
    "for i, path in enumerate(sorted(analysis_file_path)):\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        # 파일 내용을 파이썬 객체로 변환\n",
    "        data = json.load(file)  # 보통 JSON 배열이면 list 타입이 됨\n",
    "        html_str = data[\"content\"][\"html\"]\n",
    "\n",
    "        change_page = page_range * i\n",
    "        print(last_number)\n",
    "        for j, element in enumerate(data[\"elements\"]):\n",
    "            element[\"id\"] = last_number + j\n",
    "            html_content = element[\"content\"][\"html\"]\n",
    "            soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "            tag = soup.find(attrs={\"id\": True})\n",
    "            tag[\"id\"] = last_number + j\n",
    "            # for tag in soup.find_all(attrs={\"id\": True}):\n",
    "            # # 여기서 id 속성을 원하는 값으로 변경\n",
    "            #     tag['id'] = last_number+j\n",
    "\n",
    "            element[\"content\"][\"html\"] = str(tag)\n",
    "            element[\"page\"] = change_page + element[\"page\"]\n",
    "        last_number = len(data[\"elements\"])\n",
    "        element_content.extend(data[\"elements\"])\n",
    "\n",
    "\n",
    "with open(f\"{output_file_basename}_merged.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(\n",
    "        element_content,\n",
    "        json_file,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36777216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./test/seq2seq'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file_basename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5ef34d",
   "metadata": {},
   "source": [
    "### 분리한 파일에서 이미지 가져와서 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3558f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test/seq2seq_merged.json\n",
      "base64_encoding params 없습니다.\n",
      "이미지 태그가 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "Saved image: ./test/seq2seq/image_12.png\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "이미지 태그가 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "이미지 태그가 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "Saved image: ./test/seq2seq/image_68.png\n",
      "Saved image: ./test/seq2seq/image_69.png\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "이미지 태그가 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "Saved image: ./test/seq2seq/image_75.png\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import base64\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "input_json_path = f\"{output_file_basename}_merged.json\"  # JSON 파일 경로\n",
    "output_dir = output_file_basename  # 이미지 저장 폴더\n",
    "print(input_json_path)\n",
    "# 저장 폴더가 없으면 생성\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "with open(input_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)  # JSON 배열 데이터 로드\n",
    "\n",
    "arr = []\n",
    "for idx, item in enumerate(data):\n",
    "    _dict = dict()\n",
    "    if \"base64_encoding\" in item:\n",
    "        _dict = {\"base64_encoding\": item[\"base64_encoding\"]}\n",
    "        html_str = item[\"content\"][\"html\"]\n",
    "        soup = BeautifulSoup(html_str, \"html.parser\")\n",
    "        img_tags = soup.find_all(\"img\")\n",
    "        if img_tags:\n",
    "            base64_str = item[\"base64_encoding\"]\n",
    "            # base64 문자열 디코딩\n",
    "            image_data = base64.b64decode(base64_str)\n",
    "            image_path = os.path.join(f\"{output_dir}/image_{idx}.png\")\n",
    "            for img_tag in soup.find_all(\"img\"):\n",
    "                # 기존 속성 모두 제거\n",
    "                img_tag.attrs.clear()\n",
    "                # 원하는 텍스트를 src 속성에 넣기\n",
    "                img_tag[\"src\"] = image_path\n",
    "            # 이미지 파일 저장\n",
    "            _dict[\"content_text\"] = str(img_tag)\n",
    "            with open(f\"{image_path}\", \"wb\") as img_file:\n",
    "                img_file.write(image_data)\n",
    "            print(f\"Saved image: {image_path}\")\n",
    "        else:\n",
    "            _dict[\"content_text\"] = item[\"content\"][\"html\"]\n",
    "            print(\"이미지 태그가 없습니다.\")\n",
    "    else:\n",
    "        _dict[\"content_text\"] = item[\"content\"][\"html\"]\n",
    "        print(\"base64_encoding params 없습니다.\")\n",
    "\n",
    "    _dict[\"metadata\"] = {\"id\": item[\"id\"], \"page\": item[\"page\"]}\n",
    "    arr.append(_dict)\n",
    "\n",
    "with open(f\"{output_file_basename}_merged_1.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(\n",
    "        arr,\n",
    "        json_file,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8384eb21",
   "metadata": {},
   "source": [
    "### content_text 에서 html 태그 가져와 tag에 img 존재하는 경우 모아서 summary 진행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f334ce70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test/seq2seq_merged_1.json\n",
      "\n",
      "        Please consider the following text context—both the preceding paragraph and the following paragraph—along with the included image. \n",
      "        Based on this context, provide a simple and clear explanation of the image, suitable for someone with no prior background knowledge. \n",
      "        Use easy-to-understand language and focus on the main ideas that the image conveys in relation to the text.\n",
      "\n",
      "        [Preceding paragraph text]\n",
      "        <p data-category=\"paragraph\" id=\"11\" style=\"font-size:20px\">There have been a number of related attempts to address the general sequence to sequence learning<br/>problem with neural networks. Our approach is closely related to Kalchbrenner and Blunsom [18]<br/>who were the first to map the entire input sentence to vector, and is related to Cho et al. [5] although<br/>the latter was used only for rescoring hypotheses produced by a phrase-based system. Graves [10]<br/>introduced a novel differentiable attention mechanism that allows neural networks to focus on dif-<br/>ferent parts of their input, and an elegant variant of this idea was successfully applied to machine<br/>translation by Bahdanau et al. [2]. The Connectionist Sequence Classification is another popular<br/>technique for mapping sequences to sequences with neural networks, but it assumes a monotonic<br/>alignment between the inputs and the outputs [1 1].</p>\n",
      "        [Following paragraph text]\n",
      "        <p data-category=\"paragraph\" id=\"13\" style=\"font-size:14px\">Figure 1: Our model reads an input sentence \"ABC\" and produces \"WXYZ\" as the output sentence. The<br/>model stops making predictions after outputting the end-of-sentence token. Note that the LSTM reads the<br/>input sentence in reverse, because doing so introduces many short term dependencies in the data that make the<br/>optimization problem much easier.</p>\n",
      "\n",
      "\n",
      "        ###\n",
      "        Output Format:\n",
      "        <image>\n",
      "        <title>\n",
      "        <summary>\n",
      "        <entities> \n",
      "        <path> <img src=\"./test/seq2seq/image_12.png\"/> </path>\n",
      "        <id> 12 </id>\n",
      "        </image>\n",
      "\n",
      "\n",
      "\n",
      "        Please consider the following text context—both the preceding paragraph and the following paragraph—along with the included image. \n",
      "        Based on this context, provide a simple and clear explanation of the image, suitable for someone with no prior background knowledge. \n",
      "        Use easy-to-understand language and focus on the main ideas that the image conveys in relation to the text.\n",
      "\n",
      "        [Preceding paragraph text]\n",
      "        <p data-category=\"paragraph\" id=\"67\" style=\"font-size:20px\">3.8 Model Analysis</p>\n",
      "        [Following paragraph text]\n",
      "        <img src=\"./test/seq2seq/image_69.png\"/>\n",
      "\n",
      "\n",
      "        ###\n",
      "        Output Format:\n",
      "        <image>\n",
      "        <title>\n",
      "        <summary>\n",
      "        <entities> \n",
      "        <path> <img src=\"./test/seq2seq/image_68.png\"/> </path>\n",
      "        <id> 68 </id>\n",
      "        </image>\n",
      "\n",
      "\n",
      "\n",
      "        Please consider the following text context—both the preceding paragraph and the following paragraph—along with the included image. \n",
      "        Based on this context, provide a simple and clear explanation of the image, suitable for someone with no prior background knowledge. \n",
      "        Use easy-to-understand language and focus on the main ideas that the image conveys in relation to the text.\n",
      "\n",
      "        [Preceding paragraph text]\n",
      "        <img src=\"./test/seq2seq/image_68.png\"/>\n",
      "        [Following paragraph text]\n",
      "        <caption id=\"70\" style=\"font-size:16px\">Figure 2: The ﬁgure shows a 2-dimensional PCA projection of the LSTM hidden states that are obtained<br/>after processing the phrases in the ﬁgures. The phrases are clustered by meaning, which in these examples is<br/>primarily a function of word order, which would be difﬁcult to capture with a bag-of-words model. Notice that<br/>both clusters have similar internal structure.</caption>\n",
      "\n",
      "\n",
      "        ###\n",
      "        Output Format:\n",
      "        <image>\n",
      "        <title>\n",
      "        <summary>\n",
      "        <entities> \n",
      "        <path> <img src=\"./test/seq2seq/image_69.png\"/> </path>\n",
      "        <id> 69 </id>\n",
      "        </image>\n",
      "\n",
      "\n",
      "\n",
      "        Please consider the following text context—both the preceding paragraph and the following paragraph—along with the included image. \n",
      "        Based on this context, provide a simple and clear explanation of the image, suitable for someone with no prior background knowledge. \n",
      "        Use easy-to-understand language and focus on the main ideas that the image conveys in relation to the text.\n",
      "\n",
      "        [Preceding paragraph text]\n",
      "        <caption id=\"74\" style=\"font-size:18px\">Table 3: A few examples of long translations produced by the LSTM alongside the ground truth<br/>translations. The reader can verify that the translations are sensible using Google translate.</caption>\n",
      "        [Following paragraph text]\n",
      "        <caption id=\"76\" style=\"font-size:16px\">Figure 3: The left plot shows the performance of our system as a function of sentence length, where the<br/>x-axis corresponds to the test sentences sorted by their length and is marked by the actual sequence lengths.<br/>There is no degradation on sentences with less than 35 words, there is only a minor degradation on the longest<br/>sentences. The right plot shows the LSTM’s performance on sentences with progressively more rare words,<br/>where the x-axis corresponds to the test sentences sorted by their “average word frequency rank”.</caption>\n",
      "\n",
      "\n",
      "        ###\n",
      "        Output Format:\n",
      "        <image>\n",
      "        <title>\n",
      "        <summary>\n",
      "        <entities> \n",
      "        <path> <img src=\"./test/seq2seq/image_75.png\"/> </path>\n",
      "        <id> 75 </id>\n",
      "        </image>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import base64\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_system_prompt():\n",
    "    return \"\"\"You are an expert in extracting useful information from IMAGE.\n",
    "    In particular, you specialize in analyzing papers.\n",
    "    With a given image, your task is to extract key entities, summarize them, and write useful information that can be used later for retrieval.\"\"\"\n",
    "\n",
    "\n",
    "def get_user_image_template(previous_context, next_context, image_paths, id):\n",
    "    return f\"\"\"\n",
    "        Please consider the following text context—both the preceding paragraph and the following paragraph—along with the included image. \n",
    "        Based on this context, provide a simple and clear explanation of the image, suitable for someone with no prior background knowledge. \n",
    "        Use easy-to-understand language and focus on the main ideas that the image conveys in relation to the text.\n",
    "\n",
    "        [Preceding paragraph text]\n",
    "        {previous_context}\n",
    "        [Following paragraph text]\n",
    "        {next_context}\n",
    "\n",
    "\n",
    "        ###\n",
    "        Output Format:\n",
    "        <image>\n",
    "        <title>\n",
    "        <summary>\n",
    "        <entities> \n",
    "        <path> {image_paths} </path>\n",
    "        <id> {id} </id>\n",
    "        </image>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "image_urls: list[str] = []\n",
    "system_prompts: list[str] = []\n",
    "user_prompts: list[str] = []\n",
    "# 저장 폴더가 없으면 생성\n",
    "input_json_path = f\"{output_file_basename}_merged_1.json\"  # JSON 파일 경로\n",
    "print(input_json_path)\n",
    "with open(input_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)  # JSON 배열 데이터 로드\n",
    "\n",
    "for idx, item in enumerate(data):\n",
    "    previous_context = data[idx - 1][\"content_text\"] if idx - 1 >= 0 else None\n",
    "    next_context = data[idx + 1][\"content_text\"] if idx + 1 < len(data) else None\n",
    "    id_str = item[\"metadata\"][\"id\"]\n",
    "    html_str = item[\"content_text\"]\n",
    "    soup = BeautifulSoup(html_str, \"html.parser\")\n",
    "    img_tag = soup.find(\"img\")\n",
    "    # equation_tag = soup.find(attrs={'data-category': 'equation'})\n",
    "    if img_tag:\n",
    "        src_path = img_tag.get(\"src\")\n",
    "        image_urls.append(src_path)\n",
    "        system_prompts.append(get_system_prompt())\n",
    "        user_prompts.append(\n",
    "            get_user_image_template(previous_context, next_context, html_str, id_str)\n",
    "        )\n",
    "\n",
    "\n",
    "# print(image_urls)\n",
    "for u in user_prompts:\n",
    "    print(u + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3420378f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<image>\\n<title>Sequence-to-Sequence Model Processing an Input Sentence</title>\\n<summary>\\nThis image shows how a special type of neural network called an LSTM processes an input sentence and generates an output sentence. The input sentence consists of the letters \"A\", \"B\", and \"C\". The model reads these letters one by one and then produces an output sequence \"W\", \"X\", \"Y\", \"Z\", followed by a special end-of-sentence token (\"<EOS>\"). The arrows indicate the flow of information through the network. The model stops generating output once it produces the \"<EOS>\" token. Importantly, the model reads the input sentence in reverse order to make learning easier.\\n</summary>\\n<entities>\\n- LSTM (Long Short-Term Memory) neural network\\n- Input sentence: \"A\", \"B\", \"C\"\\n- Output sentence: \"W\", \"X\", \"Y\", \"Z\", \"<EOS>\"\\n- End-of-sentence token (\"<EOS>\")\\n- Sequence-to-sequence learning\\n</entities>\\n<path> <img src=\"./test/seq2seq/image_12.png\"/> </path>\\n<id> 12 </id>\\n</image>', '<image>\\n<title>Visualization of Relationship Sentences in a Model Analysis</title>\\n<summary>\\nThis image shows a simple graph that helps us understand how a model views different sentences about relationships between two people, Mary and John. Each point on the graph represents a sentence describing feelings or respect between Mary and John. The sentences are grouped based on their meaning and who the subject is (Mary or John). For example, sentences about Mary admiring or loving John are close together, while sentences about John admiring or loving Mary are grouped separately. This helps show how the model organizes and understands different but related sentences.\\n</summary>\\n<entities>\\n- Mary\\n- John\\n- Admire\\n- Love\\n- Respect\\n- Sentence grouping\\n- Model analysis\\n</entities>\\n<path> <img src=\"./test/seq2seq/image_68.png\"/> </path>\\n<id> 68 </id>\\n</image>', '<image>\\n<title>Visualization of Phrase Clusters Based on Meaning Using LSTM Hidden States</title>\\n<summary>\\nThis image shows how different sentences with similar meanings are grouped together based on their word order. Each point represents a sentence, and sentences that mean the same thing are placed close to each other in the graph. The graph uses a method called PCA to simplify complex data from an LSTM model, which understands the order of words. This helps show that the model can recognize meaning beyond just the words used, by paying attention to how the words are arranged.\\n</summary>\\n<entities>\\n- LSTM (Long Short-Term Memory): a type of neural network that processes sequences of words.\\n- PCA (Principal Component Analysis): a technique to reduce complex data into simpler visual forms.\\n- Phrase clusters: groups of sentences with similar meanings.\\n- Word order: the sequence in which words appear in a sentence.\\n</entities>\\n<path> <img src=\"./test/seq2seq/image_69.png\"/> </path>\\n<id> 69 </id>\\n</image>', '<image>\\n<title>Performance of LSTM Translation System by Sentence Length and Word Rarity</title>\\n<summary>\\nThis image shows two graphs comparing how well an LSTM-based translation system performs against a baseline system. The left graph measures translation quality based on sentence length, showing that the LSTM does well on short to medium sentences and only slightly worse on very long sentences. The right graph measures performance based on how rare the words in the sentences are, showing that the LSTM handles common words better but its performance drops as sentences contain rarer words. Overall, the LSTM system generally outperforms the baseline in both cases.\\n</summary>\\n<entities>\\n- LSTM (Long Short-Term Memory) translation system\\n- Baseline translation system\\n- BLEU score (a measure of translation quality)\\n- Sentence length\\n- Word frequency (rarity)\\n</entities>\\n<path> <img src=\"./test/seq2seq/image_75.png\"/> </path>\\n<id> 75 </id>\\n</image>']\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.models import MultiModal\n",
    "\n",
    "llm = get_gpt()\n",
    "multimodal_llm = MultiModal(llm)\n",
    "answer = multimodal_llm.batch(\n",
    "    image_urls, system_prompts, user_prompts, display_image=False\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687d49fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id 값: 12\n",
      "id 값: 68\n",
      "id 값: 69\n",
      "id 값: 75\n"
     ]
    }
   ],
   "source": [
    "# arr = ['<image>\\n<title>도표 8. DDR4 가격 추이</title>\\n<summary>이 그래프는 2021년 1월부터 2025년 8월까지 DDR4 메모리 가격의 변동 추이를 보여준다. 그래프에는 현물 가격과 고정 가격 두 가지가 표시되어 있으며, 초기에는 가격이 약 5달러에서 시작해 점차 하락하여 2023년 중반에는 1.5달러 근처까지 떨어졌다. 이후 가격은 다시 상승세로 돌아서 2025년 8월에는 약 6달러에 도달하는 모습을 나타낸다.</summary>\\n<entities>\\n- DDR4 메모리\\n- 현물 가격\\n- 고정 가격\\n- 가격 변동 추이\\n- 기간: 2021년 1월 ~ 2025년 8월\\n- 단위: 달러 ($)\\n</entities>\\n<path> <img src=\"./test/hana/image_43.png\"/> </path>\\n<id> 43 </id>\\n</image>', '<image>\\n<title>도표 9. DDR5 가격 추이</title>\\n<summary>이 그래프는 2023년 1월부터 7월까지 DDR5 메모리 가격의 변동 추이를 보여준다. 두 가지 가격이 표시되어 있는데, 하나는 현물 가격(진한 녹색 선)이고 다른 하나는 고정 가격(회색 선)이다. 초기에는 가격이 약 5.5달러에서 시작해 3.8달러까지 하락했다가 이후 점차 상승하여 7달러 이상으로 증가하는 추세를 보인다. 고정 가격은 현물 가격보다 낮거나 비슷한 수준에서 움직이다가 후반부에 상승하는 경향을 나타낸다.</summary>\\n<entities>DDR5, 가격 추이, 현물 가격, 고정 가격, 2023년 1월~7월, 달러($)</entities>\\n<path> <img src=\"./test/hana/image_46.png\"/> </path>\\n<id>46</id>\\n</image>', '<image>\\n<title>도표 11. SLC 8Gb 가격 추이 (SLC 8Gb Price Trend)</title>\\n<summary>\\nThis line graph shows the price trend of SLC 8Gb memory chips over time, from January 2021 (21.1) to August 2025 (25.8). The vertical axis represents the price in dollars ($), ranging from 0 to 12. Two price lines are depicted: the 현물 가격 (spot price) in blue-green and the 고정 가격 (fixed price) in gray. The spot price starts around $5.5 in early 2021, rises to above $8 by early 2022, then declines and stabilizes around $6.5 until early 2024. After that, it sharply increases to nearly $10 by mid-2025 before slightly dropping. The fixed price remains relatively stable around $4.5 until late 2024, then drops sharply to below $2.5 before gradually rising again to about $3.5 by mid-2025.\\n</summary>\\n<entities>\\n- SLC 8Gb memory chip\\n- Price in USD ($)\\n- 현물 가격 (Spot Price)\\n- 고정 가격 (Fixed Price)\\n- Time period: January 2021 to August 2025\\n- Data sources: DRAMExchange, 하나증권 (Hana Securities)\\n</entities>\\n<path> <img src=\"./test/hana/image_50.png\"/> </path>\\n<id> 50 </id>\\n</image>', '<image>\\n<title>현물 가격과 고정 가격 추이 (2021년 1월 ~ 2025년 8월 예상)</title>\\n<summary>이 그래프는 2021년 1월부터 2025년 8월까지의 현물 가격과 고정 가격 변동 추이를 보여준다. 현물 가격은 2022년 초에 약 4달러까지 상승했다가 이후 점차 하락하여 2024년 초까지 2.7달러 근처에서 유지되다가 2025년부터 다시 상승하는 경향을 보인다. 고정 가격은 현물 가격보다 전반적으로 낮거나 비슷한 수준을 유지하며, 2024년 중반 이후부터는 현물 가격보다 낮은 수준에서 완만하게 상승하는 모습을 나타낸다.</summary>\\n<entities>\\n- 현물 가격 (Spot Price)\\n- 고정 가격 (Fixed Price)\\n- 기간: 2021년 1월 ~ 2025년 8월 (예상 포함)\\n- 단위: 달러($)\\n- 출처: DRAMExchange, 하나증권\\n</entities>\\n<path> <img src=\"./test/hana/image_52.png\"/> </path>\\n<id> 52 </id>\\n</image>', '<image>\\n<title>도표 12. 주요 DRAM 업체들의 연초 이후 주가 추이</title>\\n<summary>이 그래프는 2024년 12월 31일을 기준으로 주요 DRAM 업체들인 삼성전자, SK하이닉스, Micron, Nanya의 주가 변동 추이를 나타내고 있다. 각 업체의 주가는 2024년 1월부터 9월까지의 기간 동안 변동을 보였으며, 특히 Nanya의 주가가 가장 큰 폭으로 상승하여 210 이상을 기록한 반면, Micron은 상대적으로 안정적인 흐름을 보이며 100 근처에서 움직였다. 삼성전자와 SK하이닉스는 중간 수준의 상승세를 유지하였다.</summary>\\n<entities>\\n- 삼성전자 (Samsung Electronics)\\n- SK하이닉스 (SK Hynix)\\n- Micron\\n- Nanya\\n- 주가 추이 (Stock Price Trend)\\n- 기간: 2024년 1월 ~ 9월\\n</entities>\\n<path> <img src=\"./test/hana/image_57.png\"/> </path>\\n<id> 57 </id>\\n</image>']\n",
    "\n",
    "# html_str = data[0]  # 배열에서 첫번째 문자열 추출\n",
    "\n",
    "input_json_path = f\"{output_file_basename}_merged_1.json\"  # JSON 파일 경로\n",
    "with open(input_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)  # JSON 배열 데이터 로드\n",
    "\n",
    "for html_str in answer:\n",
    "    soup = BeautifulSoup(html_str, \"html.parser\")\n",
    "    id_tag = soup.find(\"id\")  # id 태그 찾기\n",
    "    if id_tag:\n",
    "        id_value = id_tag.text.strip()  # 텍스트 추출 후 공백 제거\n",
    "        # data[id_value]['content_text'] = html_str\n",
    "        data[int(id_value)][\"content_text\"] = html_str\n",
    "        print(\"id 값:\", id_value)\n",
    "    else:\n",
    "        print(\"id 태그를 찾을 수 없습니다.\")\n",
    "\n",
    "with open(f\"{output_file_basename}_merged_2.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(\n",
    "        data,\n",
    "        json_file,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afab9d05",
   "metadata": {},
   "source": [
    "### 방정식 설명 추가 로직"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e65bb4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        You are an AI assistant who analyzes formulas written in LaTeX format in the paper.\n",
      "        Explain the given equation easily with an appropriate example so that even a 5-year-old can understand it.\n",
      "        Explain the equation so that it can be understood without the equation by sufficiently solving it.\n",
      "\n",
      "        [Preceding paragraph text]\n",
      "        <p data-category=\"paragraph\" id=\"21\" style=\"font-size:14px\">The Recurrent Neural Network (RNN) [31, 28] is a natural generalization of feedforward neural<br/>networks to sequences. Given a sequence of inputs (x1, . · .,XT), a standard RNN computes a<br/>sequence of outputs (y1,· · . , YT) by iterating the following equation:</p>\n",
      "\n",
      "        [Equation]\n",
      "        $$\\begin{array}{l l l}{{h_{t}}}&{{=}}&{{\\mathrm{sigm}\\left(W^{\\mathrm{hx}}x_{t}+W^{\\mathrm{hh}}h_{t-1}\\right)}}\\\\ {{y_{t}}}&{{=}}&{{W^{\\mathrm{yh}}h_{t}}}\\end{array}$$\n",
      "\n",
      "        [Following paragraph text]\n",
      "        <p data-category=\"paragraph\" id=\"23\" style=\"font-size:14px\">The RNN can easily map sequences to sequences whenever the alignment between the inputs the<br/>outputs is known ahead of time. However, it is not clear how to apply an RNN to problems whose<br/>input and the output sequences have different lengths with complicated and non-monotonic relation-<br/>ships.</p>\n",
      "\n",
      "\n",
      "        # Output Format:\n",
      "        <title>\n",
      "        <explain>\n",
      "        <examples> \n",
      "        <id>22</id>\n",
      "\n",
      "\n",
      "\n",
      "        You are an AI assistant who analyzes formulas written in LaTeX format in the paper.\n",
      "        Explain the given equation easily with an appropriate example so that even a 5-year-old can understand it.\n",
      "        Explain the equation so that it can be understood without the equation by sufficiently solving it.\n",
      "\n",
      "        [Preceding paragraph text]\n",
      "        <p data-category=\"paragraph\" id=\"25\" style=\"font-size:14px\">The goal of the LSTM is to estimate the conditional probability p(y1, . : , YT' x1,···, xT) where<br/>(x1, · · · , xT) is an input sequence and Y1, · · · , YT' is its corresponding output sequence whose length<br/>T' may differ from T. The LSTM computes this conditional probability by first obtaining the fixed-<br/>dimensional representation v of the input sequence (x1,.. · , xT) given by the last hidden state of the<br/>LSTM, and then computing the probability of y1, · · · , YT' with a standard LSTM-LM formulation<br/>whose initial hidden state is set to the representation v of x1, · · · , XT:</p>\n",
      "\n",
      "        [Equation]\n",
      "        $$p(y_{1},\\cdot\\cdot\\cdot,y_{T^{\\prime}}|x_{1},\\cdot\\cdot\\cdot,x_{T})=\\prod_{t=1}^{T^{\\prime}}p(y_{t}|v,y_{1},\\cdot\\cdot\\cdot,y_{t-1})$$\n",
      "\n",
      "        [Following paragraph text]\n",
      "        <caption id=\"27\" style=\"font-size:20px\">(1)</caption>\n",
      "\n",
      "\n",
      "        # Output Format:\n",
      "        <title>\n",
      "        <explain>\n",
      "        <examples> \n",
      "        <id>26</id>\n",
      "\n",
      "\n",
      "\n",
      "        You are an AI assistant who analyzes formulas written in LaTeX format in the paper.\n",
      "        Explain the given equation easily with an appropriate example so that even a 5-year-old can understand it.\n",
      "        Explain the equation so that it can be understood without the equation by sufficiently solving it.\n",
      "\n",
      "        [Preceding paragraph text]\n",
      "        <p data-category=\"paragraph\" id=\"37\" style=\"font-size:14px\">The core of our experiments involved training a large deep LSTM on many sentence pairs. We<br/>trained it by maximizing the log probability of a correct translation T given the source sentence S,<br/>so the training objective is</p>\n",
      "\n",
      "        [Equation]\n",
      "        $$\\textstyle{1/|{\\cal G}|}_{(T,S)\\in{\\cal S}}\\log p(T|S)$$\n",
      "\n",
      "        [Following paragraph text]\n",
      "        <p data-category=\"paragraph\" id=\"39\" style=\"font-size:16px\">where s is the training set. Once training is complete, we produce translations by finding the most<br/>likely translation according to the LSTM:</p>\n",
      "\n",
      "\n",
      "        # Output Format:\n",
      "        <title>\n",
      "        <explain>\n",
      "        <examples> \n",
      "        <id>38</id>\n",
      "\n",
      "\n",
      "\n",
      "        You are an AI assistant who analyzes formulas written in LaTeX format in the paper.\n",
      "        Explain the given equation easily with an appropriate example so that even a 5-year-old can understand it.\n",
      "        Explain the equation so that it can be understood without the equation by sufficiently solving it.\n",
      "\n",
      "        [Preceding paragraph text]\n",
      "        <p data-category=\"paragraph\" id=\"39\" style=\"font-size:16px\">where s is the training set. Once training is complete, we produce translations by finding the most<br/>likely translation according to the LSTM:</p>\n",
      "\n",
      "        [Equation]\n",
      "        $${\\hat{T}}=\\arg\\operatorname*{max}_{T}p(T|S)$$\n",
      "\n",
      "        [Following paragraph text]\n",
      "        <caption id=\"41\" style=\"font-size:20px\">(2)</caption>\n",
      "\n",
      "\n",
      "        # Output Format:\n",
      "        <title>\n",
      "        <explain>\n",
      "        <examples> \n",
      "        <id>40</id>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import base64\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_system_prompt():\n",
    "    return \"\"\"You are an expert in extracting useful information from IMAGE.\n",
    "    In particular, you specialize in analyzing papers.\n",
    "    With a given image, your task is to extract key entities, summarize them, and write useful information that can be used later for retrieval.\"\"\"\n",
    "\n",
    "\n",
    "def get_user_equation_template(previous_context, next_context, equation, id):\n",
    "    return f\"\"\"\n",
    "        You are an AI assistant who analyzes formulas written in LaTeX format in the paper.\n",
    "        Explain the given equation easily with an appropriate example so that even a 5-year-old can understand it.\n",
    "        Explain the equation so that it can be understood without the equation by sufficiently solving it.\n",
    "\n",
    "        [Preceding paragraph text]\n",
    "        {previous_context}\n",
    "        \n",
    "        [Equation]\n",
    "        {equation}\n",
    "\n",
    "        [Following paragraph text]\n",
    "        {next_context}\n",
    "\n",
    "        \n",
    "        # Output Format:\n",
    "        <title>\n",
    "        <explain>\n",
    "        <examples> \n",
    "        <id>{id}</id>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "system_prompts: list[str] = []\n",
    "user_prompts: list[str] = []\n",
    "# 저장 폴더가 없으면 생성\n",
    "input_json_path = f\"{output_file_basename}_merged_2.json\"  # JSON 파일 경로\n",
    "# print(input_json_path)\n",
    "with open(input_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)  # JSON 배열 데이터 로드\n",
    "\n",
    "for idx, item in enumerate(data):\n",
    "    previous_context = data[idx - 1][\"content_text\"] if idx - 1 >= 0 else None\n",
    "    next_context = data[idx + 1][\"content_text\"] if idx + 1 < len(data) else None\n",
    "    id_str = item[\"metadata\"][\"id\"]\n",
    "    html_str = item[\"content_text\"]\n",
    "    soup = BeautifulSoup(html_str, \"html.parser\")\n",
    "    equation_tag = soup.find(attrs={\"data-category\": \"equation\"})\n",
    "    if equation_tag:\n",
    "        system_prompts.append(get_system_prompt())\n",
    "        user_prompts.append(\n",
    "            get_user_equation_template(\n",
    "                previous_context, next_context, equation_tag.get_text(), id_str\n",
    "            )\n",
    "        )\n",
    "\n",
    "for u in user_prompts:\n",
    "    print(u + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8896212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_messages(system_prompt, user_prompt):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": user_prompt,\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc19dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "for system_prompt, user_prompt in zip(system_prompts, user_prompts):\n",
    "    message = create_messages(system_prompt, user_prompt)\n",
    "    messages.append(message)\n",
    "messages\n",
    "llm = get_gpt()\n",
    "response = llm.batch(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "940f3965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<title>Understanding the Basic Recurrent Neural Network (RNN) Equation</title>\\n<explain>\\nThis equation describes how a Recurrent Neural Network (RNN) processes a sequence of inputs step-by-step to produce outputs. Imagine you have a list of things coming in one after another, like words in a sentence or notes in a song. At each step (time t), the RNN looks at the current input (xt) and also remembers what it learned from the previous step (ht-1). It combines these two pieces of information using some weights (W) and a special function called \"sigmoid\" (sigm), which helps decide how much to remember or forget. This combination creates a new \"hidden state\" (ht), which is like the RNN\\'s memory at that moment. Then, the RNN uses this hidden state to produce an output (yt) for that step. This way, the RNN can understand sequences by keeping track of what happened before while looking at the new input.\\n</explain>\\n<examples>\\nImagine you are listening to a story one word at a time. At each word, you remember what happened before and use that memory to understand the next word better. For example, if the story says \"The cat sat on the,\" you remember those words so when you hear the next word \"mat,\" you understand the full sentence. Here, the hidden state (ht) is like your memory of the story so far, and the output (yt) is your understanding or reaction to the current word.\\n</examples>\\n<id>22</id>' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 324, 'prompt_tokens': 433, 'total_tokens': 757, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_c064fdde7c', 'id': 'chatcmpl-CT1JSXXMBqLvZN8OmQILLe0bTfZOV', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--8c2ee0ee-698a-4011-a0c2-46b36776ea04-0' usage_metadata={'input_tokens': 433, 'output_tokens': 324, 'total_tokens': 757, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "content=\"<title>Understanding the Conditional Probability Equation in LSTM Sequence Modeling</title>\\n\\n<explain>\\nThis equation is about how an LSTM (a type of smart computer program) predicts a sequence of outputs (like words or actions) based on a sequence of inputs. Imagine you have a story made of pictures (inputs), and you want to tell a new story (outputs) that matches those pictures.\\n\\nThe equation says: the chance of getting the whole output story (y1 to yT') given the input story (x1 to xT) can be found by looking at each output step one by one. For each output step t, the LSTM guesses the next output y_t by considering:\\n1. The summary of the entire input story (called v),\\n2. All the outputs it has already said before (y1 to y_{t-1}).\\n\\nBy multiplying all these step-by-step guesses together, we get the chance of the full output story.\\n\\nIn simpler terms, the LSTM first reads and remembers the whole input story as a summary. Then, it tells the output story one piece at a time, always remembering what it has said before and the input summary, to make the best guess for the next piece.\\n\\n</explain>\\n\\n<examples>\\nImagine you have a box of colored blocks (inputs) arranged in a certain order: red, blue, green. You want to build a tower (outputs) that matches the colors but maybe in a different order or length.\\n\\n1. First, the LSTM looks at all the blocks and makes a mental note (v) of the colors and order.\\n2. Then, it starts building the tower one block at a time.\\n3. When choosing the first block of the tower, it uses the mental note v and no previous blocks (since it's the first).\\n4. For the second block, it uses the mental note v and the first block it already placed.\\n5. It keeps doing this until the tower is complete.\\n\\nThe equation tells us that the chance of building the entire tower is the product of the chances of placing each block correctly, given what it remembers and what it has already placed.\\n\\nThis way, the LSTM can handle situations where the number of blocks in the tower (output length) is different from the number of blocks in the box (input length).\\n</examples>\\n\\n<id>26</id>\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 482, 'prompt_tokens': 455, 'total_tokens': 937, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_c064fdde7c', 'id': 'chatcmpl-CT1JSd0hFvxUIN6wH9VUJVRe0zkl9', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--b77620d2-dbb7-4641-bc31-1c558daa19da-0' usage_metadata={'input_tokens': 455, 'output_tokens': 482, 'total_tokens': 937, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "content='<title>Training Objective for LSTM-based Translation Model</title>\\n<explain>\\nThis equation describes how the model learns to translate sentences from one language to another. Imagine you have many pairs of sentences: one in the original language (source sentence S) and one in the translated language (target sentence T). The model tries to guess the correct translation T when given the source sentence S.\\n\\nTo teach the model, we look at all these sentence pairs in our training set (denoted as 𝒮). For each pair, the model calculates the probability that it would produce the correct translation T given S, written as p(T|S). We then take the logarithm (log) of this probability to make the numbers easier to work with and sum these values for all sentence pairs.\\n\\nFinally, we average this sum by dividing by the total number of sentence pairs (|𝒢|). The goal during training is to adjust the model so that this average log probability is as large as possible, meaning the model becomes better at predicting correct translations.\\n\\nIn simple terms, the model learns by trying to be as confident as possible that its translations are right, across many examples.\\n</explain>\\n<examples>\\nImagine you have a box of toy blocks, each block has a picture on it in one language, and you want to find the matching block with the same picture but in another language. The model is like a smart friend who guesses which block matches. At first, your friend might guess wrong, but every time you tell them the right answer, they get better at guessing. The equation is like a score that tells your friend how good their guesses are on average, and your friend tries to get the highest score by learning from many examples.\\n</examples>\\n<id>38</id>' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 362, 'prompt_tokens': 316, 'total_tokens': 678, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_c064fdde7c', 'id': 'chatcmpl-CT1JSt3NJ3WgiPCEQs92DstjDqK6X', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--624d57a2-fe6c-4e7c-9c75-d7d38ed0e5a8-0' usage_metadata={'input_tokens': 316, 'output_tokens': 362, 'total_tokens': 678, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "content='<title>Finding the Best Translation Using Probability</title>\\n<explain>\\nThis equation is about choosing the best translation for a sentence. Imagine you have a sentence in one language (called S), and you want to find the best way to say it in another language (called T). The equation says: \"Look at all possible translations (T) and pick the one that is most likely to be correct given the original sentence (S).\" \\n\\nIn simpler words, it’s like having many guesses for how to translate a sentence, and you pick the guess that the computer thinks is the best or most probable. The computer uses a special method (called LSTM) to figure out which translation makes the most sense.\\n\\nSo, instead of just picking any translation, it picks the one with the highest chance of being right.\\n</explain>\\n<examples>\\nImagine you have a box of different colored balls, and you want to pick the ball that is most likely to be your favorite color. You look at all the balls and choose the color that appears the most or seems the best choice. \\n\\nSimilarly, if you want to translate \"Hello\" into another language, the computer looks at all possible translations like \"Hola,\" \"Bonjour,\" or \"Ciao,\" and picks the one it thinks is the best fit based on what it learned before.\\n\\nSo, the computer is like a smart friend who listens to the original sentence and then picks the best way to say it in another language.\\n</examples>\\n<id>40</id>' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 307, 'prompt_tokens': 262, 'total_tokens': 569, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_c064fdde7c', 'id': 'chatcmpl-CT1JSRPTyWDhiYqHiyzd6mDkKldzJ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--1569c9b1-9872-4ae7-81f5-ecdbf938de40-0' usage_metadata={'input_tokens': 262, 'output_tokens': 307, 'total_tokens': 569, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "for r in response:\n",
    "    print(r)\n",
    "    # print(r.content +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea63976f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<title>Understanding the Basic Recurrent Neural Network (RNN) Equation</title>\\n<explain>\\nThis equation describes how a Recurrent Neural Network (RNN) processes a sequence of inputs step-by-step to produce outputs. Imagine you have a list of things coming in one after another, like words in a sentence or notes in a song. At each step (time t), the RNN looks at the current input (xt) and also remembers what it learned from the previous step (ht-1). It combines these two pieces of information using some weights (W) and a special function called \"sigmoid\" (sigm), which helps decide how much to remember or forget. This combination creates a new \"hidden state\" (ht), which is like the RNN\\'s memory at that moment. Then, the RNN uses this hidden state to produce an output (yt) for that step. This way, the RNN can understand sequences by keeping track of what happened before while looking at the new input.\\n</explain>\\n<examples>\\nImagine you are listening to a story one word at a time. At each word, you remember what happened before and use that memory to understand the next word better. For example, if the story says \"The cat sat on the,\" you remember those words so when you hear the next word \"mat,\" you understand the full sentence. Here, the hidden state (ht) is like your memory of the story so far, and the output (yt) is your understanding or reaction to the current word.\\n</examples>\\n<id>22</id>' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 324, 'prompt_tokens': 433, 'total_tokens': 757, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_c064fdde7c', 'id': 'chatcmpl-CT1JSXXMBqLvZN8OmQILLe0bTfZOV', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--8c2ee0ee-698a-4011-a0c2-46b36776ea04-0' usage_metadata={'input_tokens': 433, 'output_tokens': 324, 'total_tokens': 757, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "id 값: 22\n",
      "content=\"<title>Understanding the Conditional Probability Equation in LSTM Sequence Modeling</title>\\n\\n<explain>\\nThis equation is about how an LSTM (a type of smart computer program) predicts a sequence of outputs (like words or actions) based on a sequence of inputs. Imagine you have a story made of pictures (inputs), and you want to tell a new story (outputs) that matches those pictures.\\n\\nThe equation says: the chance of getting the whole output story (y1 to yT') given the input story (x1 to xT) can be found by looking at each output step one by one. For each output step t, the LSTM guesses the next output y_t by considering:\\n1. The summary of the entire input story (called v),\\n2. All the outputs it has already said before (y1 to y_{t-1}).\\n\\nBy multiplying all these step-by-step guesses together, we get the chance of the full output story.\\n\\nIn simpler terms, the LSTM first reads and remembers the whole input story as a summary. Then, it tells the output story one piece at a time, always remembering what it has said before and the input summary, to make the best guess for the next piece.\\n\\n</explain>\\n\\n<examples>\\nImagine you have a box of colored blocks (inputs) arranged in a certain order: red, blue, green. You want to build a tower (outputs) that matches the colors but maybe in a different order or length.\\n\\n1. First, the LSTM looks at all the blocks and makes a mental note (v) of the colors and order.\\n2. Then, it starts building the tower one block at a time.\\n3. When choosing the first block of the tower, it uses the mental note v and no previous blocks (since it's the first).\\n4. For the second block, it uses the mental note v and the first block it already placed.\\n5. It keeps doing this until the tower is complete.\\n\\nThe equation tells us that the chance of building the entire tower is the product of the chances of placing each block correctly, given what it remembers and what it has already placed.\\n\\nThis way, the LSTM can handle situations where the number of blocks in the tower (output length) is different from the number of blocks in the box (input length).\\n</examples>\\n\\n<id>26</id>\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 482, 'prompt_tokens': 455, 'total_tokens': 937, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_c064fdde7c', 'id': 'chatcmpl-CT1JSd0hFvxUIN6wH9VUJVRe0zkl9', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--b77620d2-dbb7-4641-bc31-1c558daa19da-0' usage_metadata={'input_tokens': 455, 'output_tokens': 482, 'total_tokens': 937, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "id 값: 26\n",
      "content='<title>Training Objective for LSTM-based Translation Model</title>\\n<explain>\\nThis equation describes how the model learns to translate sentences from one language to another. Imagine you have many pairs of sentences: one in the original language (source sentence S) and one in the translated language (target sentence T). The model tries to guess the correct translation T when given the source sentence S.\\n\\nTo teach the model, we look at all these sentence pairs in our training set (denoted as 𝒮). For each pair, the model calculates the probability that it would produce the correct translation T given S, written as p(T|S). We then take the logarithm (log) of this probability to make the numbers easier to work with and sum these values for all sentence pairs.\\n\\nFinally, we average this sum by dividing by the total number of sentence pairs (|𝒢|). The goal during training is to adjust the model so that this average log probability is as large as possible, meaning the model becomes better at predicting correct translations.\\n\\nIn simple terms, the model learns by trying to be as confident as possible that its translations are right, across many examples.\\n</explain>\\n<examples>\\nImagine you have a box of toy blocks, each block has a picture on it in one language, and you want to find the matching block with the same picture but in another language. The model is like a smart friend who guesses which block matches. At first, your friend might guess wrong, but every time you tell them the right answer, they get better at guessing. The equation is like a score that tells your friend how good their guesses are on average, and your friend tries to get the highest score by learning from many examples.\\n</examples>\\n<id>38</id>' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 362, 'prompt_tokens': 316, 'total_tokens': 678, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_c064fdde7c', 'id': 'chatcmpl-CT1JSt3NJ3WgiPCEQs92DstjDqK6X', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--624d57a2-fe6c-4e7c-9c75-d7d38ed0e5a8-0' usage_metadata={'input_tokens': 316, 'output_tokens': 362, 'total_tokens': 678, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "id 값: 38\n",
      "content='<title>Finding the Best Translation Using Probability</title>\\n<explain>\\nThis equation is about choosing the best translation for a sentence. Imagine you have a sentence in one language (called S), and you want to find the best way to say it in another language (called T). The equation says: \"Look at all possible translations (T) and pick the one that is most likely to be correct given the original sentence (S).\" \\n\\nIn simpler words, it’s like having many guesses for how to translate a sentence, and you pick the guess that the computer thinks is the best or most probable. The computer uses a special method (called LSTM) to figure out which translation makes the most sense.\\n\\nSo, instead of just picking any translation, it picks the one with the highest chance of being right.\\n</explain>\\n<examples>\\nImagine you have a box of different colored balls, and you want to pick the ball that is most likely to be your favorite color. You look at all the balls and choose the color that appears the most or seems the best choice. \\n\\nSimilarly, if you want to translate \"Hello\" into another language, the computer looks at all possible translations like \"Hola,\" \"Bonjour,\" or \"Ciao,\" and picks the one it thinks is the best fit based on what it learned before.\\n\\nSo, the computer is like a smart friend who listens to the original sentence and then picks the best way to say it in another language.\\n</examples>\\n<id>40</id>' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 307, 'prompt_tokens': 262, 'total_tokens': 569, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_c064fdde7c', 'id': 'chatcmpl-CT1JSRPTyWDhiYqHiyzd6mDkKldzJ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--1569c9b1-9872-4ae7-81f5-ecdbf938de40-0' usage_metadata={'input_tokens': 262, 'output_tokens': 307, 'total_tokens': 569, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "id 값: 40\n"
     ]
    }
   ],
   "source": [
    "input_json_path = f\"{output_file_basename}_merged_2.json\"  # JSON 파일 경로\n",
    "with open(input_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)  # JSON 배열 데이터 로드\n",
    "\n",
    "for r in response:\n",
    "    html_str = r.content\n",
    "    print(r)\n",
    "    soup = BeautifulSoup(html_str, \"html.parser\")\n",
    "    id_tag = soup.find(\"id\")  # id 태그 찾기\n",
    "    if id_tag:\n",
    "        id_value = id_tag.text.strip()  # 텍스트 추출 후 공백 제거\n",
    "        new_p = soup.new_tag(\"p\")\n",
    "        new_p.string = html_str\n",
    "        data[int(id_value)][\"content_text\"] = (\n",
    "            data[int(id_value)][\"content_text\"] + html_str\n",
    "        )\n",
    "        print(\"id 값:\", id_value)\n",
    "    else:\n",
    "        print(\"id 태그를 찾을 수 없습니다.\")\n",
    "\n",
    "with open(\n",
    "    f\"{output_file_basename}_merged_final.json\", \"w\", encoding=\"utf-8\"\n",
    ") as json_file:\n",
    "    json.dump(\n",
    "        data,\n",
    "        json_file,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "96746e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{output_file_basename}_merged_final.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)  # JSON 배열 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c588b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_user_request() -> ChatPromptTemplate:\n",
    "    prompt = \"\"\"\n",
    "    You are an expert academic explainer tasked with simplifying difficult and complex research papers. Your responses must be:\n",
    "\n",
    "    - **Professional and detailed**: Use precise technical terms but always explain them thoroughly.\n",
    "    - **Clear and accessible**: Include sufficient explanations and relevant examples so that even a young child can understand.\n",
    "    - **Formatted in Markdown**: Use headings, lists, and emphasis where appropriate for readability.\n",
    "    - **Source-aware**: If the original document references specific pages or sections, always include the source page number or reference.\n",
    "    - **Image-inclusive**: If the input contains image or figure tags, display the image path or URL alongside your explanation.\n",
    "\n",
    "\n",
    "    When given a text input from a paper, first break down complex concepts step-by-step, \n",
    "    illustrate with examples or analogies, and clearly indicate the source pages if applicable. \n",
    "    Once text is entered in the paper, we first break down complex concepts step by step,\n",
    "    Explain with examples or parables and, if applicable, clearly display the source page.\n",
    "    Provides a final cohesive summary in Markdown format.\n",
    "    To represent the equation (x_1, \\\\cdots, x_T), switch to $(x_1, \\\\cdots, x_T)$ sentence and output it\n",
    "    If <img src='example'/> exists in the document you refer to, output the src path as it is\n",
    "    \n",
    "\n",
    "    ---\n",
    "\n",
    "    **Example Usage:**\n",
    "\n",
    "    Input: \"Explain the LSTM model from page 5 that includes 4 layers and GPU parallelization. Include any figures if present.\"\n",
    "\n",
    "    Output: *(Detailed, clear Markdown explanation with examples, source page 5 cited, and image paths if any).* \n",
    "    ---\n",
    "\n",
    "    Input: \"Please explain the key mechanism equation\n",
    "\"\n",
    "    Output: $(x_1, \\\\cdots, x_T)$\n",
    "    ---\n",
    "\n",
    "    **Important:**\n",
    "    Make sure to answer in Korean except for the image path and equation\n",
    " \n",
    "    \n",
    "    ** User Request:**\n",
    "    {question}\n",
    "\n",
    "    ** context :**\n",
    "    {context}    \n",
    "\"\"\"\n",
    "    return ChatPromptTemplate.from_template(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "64cce8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 이 논문이 해결하고자 하는 문제\n",
      "\n",
      "이 논문은 **LSTM(Long Short-Term Memory)** 모델을 사용하여 **입력 시퀀스** $(x_1, \\cdots, x_T)$에 대해 길이가 다를 수 있는 **출력 시퀀스** $(y_1, \\cdots, y_{T'})$의 조건부 확률 $p(y_1, \\cdots, y_{T'} | x_1, \\cdots, x_T)$를 추정하는 문제를 다룹니다. \n",
      "\n",
      "---\n",
      "\n",
      "## 문제의 핵심\n",
      "\n",
      "- **입력 시퀀스와 출력 시퀀스의 길이가 다를 수 있음**  \n",
      "  예를 들어, 영어 문장(입력)이 10단어이고, 그에 대응하는 프랑스어 문장(출력)이 12단어일 수 있습니다.  \n",
      "- **입력 시퀀스를 고정된 차원의 벡터로 요약하는 것**  \n",
      "  LSTM은 입력 시퀀스 전체를 읽고, 마지막 숨겨진 상태(hidden state)를 통해 이 시퀀스를 하나의 벡터 $v$로 요약합니다.  \n",
      "- **요약된 벡터 $v$를 바탕으로 출력 시퀀스를 생성**  \n",
      "  이 벡터 $v$를 초기 상태로 하여, LSTM 언어 모델(LM)이 출력 시퀀스의 각 단어를 순차적으로 예측합니다.\n",
      "\n",
      "---\n",
      "\n",
      "## 문제를 쉽게 이해하기 위한 비유\n",
      "\n",
      "- **입력 시퀀스**: 여러 장의 그림이 순서대로 놓여 있는 책  \n",
      "- **출력 시퀀스**: 그 그림들을 보고 새롭게 이야기를 만드는 것  \n",
      "- LSTM은 먼저 책 전체를 쭉 읽고(입력 시퀀스 요약), 그 내용을 머릿속에 저장합니다(벡터 $v$).  \n",
      "- 그 다음, 머릿속에 저장한 내용을 바탕으로 한 문장씩 차례대로 이야기를 만들어 나갑니다(출력 시퀀스 생성).\n",
      "\n",
      "---\n",
      "\n",
      "## 수식으로 표현된 문제 (출처: 3페이지)\n",
      "\n",
      "$$\n",
      "p(y_{1}, \\cdots, y_{T'} | x_{1}, \\cdots, x_{T}) = \\prod_{t=1}^{T'} p(y_t | v, y_1, \\cdots, y_{t-1})\n",
      "$$\n",
      "\n",
      "- 이 식은 출력 시퀀스의 각 단어 $y_t$가,  \n",
      "  - 입력 시퀀스 전체를 요약한 벡터 $v$와  \n",
      "  - 지금까지 생성된 출력 단어들 $y_1, \\cdots, y_{t-1}$에 조건부로 의존한다는 의미입니다.  \n",
      "- 즉, LSTM은 이전에 생성한 단어들과 입력 시퀀스 요약 정보를 모두 고려하여 다음 단어를 예측합니다.\n",
      "\n",
      "---\n",
      "\n",
      "## 요약\n",
      "\n",
      "| 문제점 | 설명 |\n",
      "|--------|-------|\n",
      "| 입력과 출력 시퀀스 길이 불일치 | 입력과 출력의 길이가 다를 수 있어, 단순한 매핑이 어려움 |\n",
      "| 시퀀스 전체를 고정된 크기로 요약 | LSTM의 마지막 숨겨진 상태를 사용해 입력 시퀀스를 하나의 벡터로 요약 |\n",
      "| 조건부 확률 추정 | 요약 벡터와 이전 출력 단어들을 바탕으로 다음 단어를 예측하는 확률 모델 구축 |\n",
      "\n",
      "이 논문은 위 문제를 해결하기 위해 LSTM 기반의 시퀀스-투-시퀀스(sequence-to-sequence) 모델을 제안하고, 이를 통해 자연어 번역 등 다양한 시퀀스 변환 문제에 적용하고자 합니다.\n",
      "\n",
      "---\n",
      "\n",
      "# 참고  \n",
      "- 문제 정의 및 수식: **3페이지**  \n",
      "- 수식 설명 및 예시: 3페이지 (id: 26)  \n",
      "\n",
      "---\n",
      "\n",
      "필요하면 추가 설명이나 다른 부분도 알려주세요!\n"
     ]
    }
   ],
   "source": [
    "user_input = \"이논문이 해결하고자하는 문제가머야 ?\"\n",
    "docs = []\n",
    "for d in data:\n",
    "    doc = Document(page_content=d[\"content_text\"], metadata=d[\"metadata\"])\n",
    "    docs.append(doc)\n",
    "\n",
    "faiss_retriever = get_retriever(docs)\n",
    "bm25_retriever = get_bm25_retriever(docs)\n",
    "esenmble_retriever = get_esenmble_retriever(faiss_retriever, bm25_retriever)\n",
    "rerank = get_reranker(esenmble_retriever, user_input)\n",
    "reorder_context = reorder_documents(rerank)\n",
    "prompt = get_prompt_user_request()\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"question\": user_input, \"context\": reorder_context})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9abe3b",
   "metadata": {},
   "source": [
    "# 이 논문이 해결하고자 하는 문제\n",
    "\n",
    "이 논문은 **LSTM(Long Short-Term Memory)** 모델을 사용하여 **입력 시퀀스** $(x_1, \\cdots, x_T)$에 대해 길이가 다를 수 있는 **출력 시퀀스** $(y_1, \\cdots, y_{T'})$의 조건부 확률 $p(y_1, \\cdots, y_{T'} | x_1, \\cdots, x_T)$를 추정하는 문제를 다룹니다. \n",
    "\n",
    "---\n",
    "\n",
    "## 문제의 핵심\n",
    "\n",
    "- **입력 시퀀스와 출력 시퀀스의 길이가 다를 수 있음**  \n",
    "  예를 들어, 영어 문장(입력)이 10단어이고, 그에 대응하는 프랑스어 문장(출력)이 12단어일 수 있습니다.  \n",
    "- **입력 시퀀스를 고정된 차원의 벡터로 요약하는 것**  \n",
    "  LSTM은 입력 시퀀스 전체를 읽고, 마지막 숨겨진 상태(hidden state)를 통해 이 시퀀스를 하나의 벡터 $v$로 요약합니다.  \n",
    "- **요약된 벡터 $v$를 바탕으로 출력 시퀀스를 생성**  \n",
    "  이 벡터 $v$를 초기 상태로 하여, LSTM 언어 모델(LM)이 출력 시퀀스의 각 단어를 순차적으로 예측합니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 문제를 쉽게 이해하기 위한 비유\n",
    "\n",
    "- **입력 시퀀스**: 여러 장의 그림이 순서대로 놓여 있는 책  \n",
    "- **출력 시퀀스**: 그 그림들을 보고 새롭게 이야기를 만드는 것  \n",
    "- LSTM은 먼저 책 전체를 쭉 읽고(입력 시퀀스 요약), 그 내용을 머릿속에 저장합니다(벡터 $v$).  \n",
    "- 그 다음, 머릿속에 저장한 내용을 바탕으로 한 문장씩 차례대로 이야기를 만들어 나갑니다(출력 시퀀스 생성).\n",
    "\n",
    "---\n",
    "\n",
    "## 수식으로 표현된 문제 (출처: 3페이지)\n",
    "\n",
    "$$\n",
    "p(y_{1}, \\cdots, y_{T'} | x_{1}, \\cdots, x_{T}) = \\prod_{t=1}^{T'} p(y_t | v, y_1, \\cdots, y_{t-1})\n",
    "$$\n",
    "\n",
    "- 이 식은 출력 시퀀스의 각 단어 $y_t$가,  \n",
    "  - 입력 시퀀스 전체를 요약한 벡터 $v$와  \n",
    "  - 지금까지 생성된 출력 단어들 $y_1, \\cdots, y_{t-1}$에 조건부로 의존한다는 의미입니다.  \n",
    "- 즉, LSTM은 이전에 생성한 단어들과 입력 시퀀스 요약 정보를 모두 고려하여 다음 단어를 예측합니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 요약\n",
    "\n",
    "| 문제점 | 설명 |\n",
    "|--------|-------|\n",
    "| 입력과 출력 시퀀스 길이 불일치 | 입력과 출력의 길이가 다를 수 있어, 단순한 매핑이 어려움 |\n",
    "| 시퀀스 전체를 고정된 크기로 요약 | LSTM의 마지막 숨겨진 상태를 사용해 입력 시퀀스를 하나의 벡터로 요약 |\n",
    "| 조건부 확률 추정 | 요약 벡터와 이전 출력 단어들을 바탕으로 다음 단어를 예측하는 확률 모델 구축 |\n",
    "\n",
    "이 논문은 위 문제를 해결하기 위해 LSTM 기반의 시퀀스-투-시퀀스(sequence-to-sequence) 모델을 제안하고, 이를 통해 자연어 번역 등 다양한 시퀀스 변환 문제에 적용하고자 합니다.\n",
    "\n",
    "---\n",
    "\n",
    "# 참고  \n",
    "- 문제 정의 및 수식: **3페이지**  \n",
    "- 수식 설명 및 예시: 3페이지 (id: 26)  \n",
    "\n",
    "---\n",
    "\n",
    "필요하면 추가 설명이나 다른 부분도 알려주세요!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-3tRZr1Gy-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
