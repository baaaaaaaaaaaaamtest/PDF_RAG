{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "127c7dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.utils import *\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b338293",
   "metadata": {},
   "source": [
    "### 파일 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dfd121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_pdf(filepath, batch_size=5):\n",
    "    \"\"\"\n",
    "    입력 PDF를 분할 PDF 파일로 분할\n",
    "    \"\"\"\n",
    "    # PDF 파일 열기\n",
    "    basename = os.path.splitext(filepath)[0]\n",
    "    filename = os.path.splitext(os.path.basename(basename))[0]\n",
    "    os.makedirs(basename, exist_ok=True)\n",
    "    input_pdf = pymupdf.open(filepath)\n",
    "    num_pages = len(input_pdf)\n",
    "\n",
    "    ret = []\n",
    "    # PDF 분할\n",
    "    for start_page in range(0, num_pages, batch_size):\n",
    "        end_page = min(start_page + batch_size, num_pages) - 1\n",
    "        # 분할된 PDF 저장\n",
    "        # /folder/example.pdf = > ['/folder/example','.pdf']\n",
    "        output_file = f\"{basename}\\\\{filename}_{start_page:04d}_{end_page:04d}.pdf\"\n",
    "        print(f\"분할 PDF 생성: {output_file}\")\n",
    "        with pymupdf.open() as output_pdf:\n",
    "            output_pdf.insert_pdf(input_pdf, from_page=start_page, to_page=end_page)\n",
    "            output_pdf.save(output_file)\n",
    "            ret.append(output_file)\n",
    "\n",
    "    # 입력 PDF 파일 닫기\n",
    "    input_pdf.close()\n",
    "\n",
    "    return ret, basename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e1c1619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분할 PDF 생성: .cache\\files\\seq2seq\\seq2seq_0000_0004.pdf\n",
      "분할 PDF 생성: .cache\\files\\seq2seq\\seq2seq_0005_0008.pdf\n",
      "['.cache\\\\files\\\\seq2seq\\\\seq2seq_0000_0004.pdf', '.cache\\\\files\\\\seq2seq\\\\seq2seq_0005_0008.pdf'] .cache\\files\\seq2seq\n"
     ]
    }
   ],
   "source": [
    "filepath = \".cache\\\\files\\\\seq2seq.pdf\"\n",
    "file_paths,basename = split_pdf(filepath)\n",
    "\n",
    "print(file_paths,basename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033da952",
   "metadata": {},
   "source": [
    "### upstage에서 데이터 파싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03bc73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_file_path = upstage_layout_analysis(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf0b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis_file_path = ['.cache\\\\files\\\\seq2seq\\\\seq2seq_0000_0004.json', '.cache\\\\files\\\\seq2seq\\\\seq2seq_0005_0008.json']\n",
    "analysis_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ac87a3",
   "metadata": {},
   "source": [
    "### 각 파일 분리하여 저장\n",
    "- page 번호 조정\n",
    "- id 조정\n",
    "- html id tag 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "befb08d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reOrder_id(analysis_file_path):\n",
    "    element_content = []\n",
    "    page_range = 5\n",
    "    last_number = 0\n",
    "    for i, path in enumerate(sorted(analysis_file_path)):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "            # 파일 내용을 파이썬 객체로 변환\n",
    "            data = json.load(file)  # 보통 JSON 배열이면 list 타입이 됨\n",
    "            change_page = page_range * i\n",
    "            for j, element in enumerate(data[\"elements\"]):\n",
    "                element[\"id\"] = last_number + j\n",
    "                html_content = element[\"content\"][\"html\"]\n",
    "                soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "                tag = soup.find(attrs={\"id\": True})\n",
    "                tag[\"id\"] = last_number + j\n",
    "                element[\"content\"][\"html\"] = str(tag)\n",
    "                element[\"page\"] = change_page + element[\"page\"]\n",
    "            last_number = len(data[\"elements\"])\n",
    "            element_content.extend(data[\"elements\"])\n",
    "    return element_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5be9c6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "element_content = reOrder_id(analysis_file_path)\n",
    "# print(element_content)\n",
    "# with open(f\"{output_file_basename}_merged.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "#     json.dump(\n",
    "#         element_content,\n",
    "#         json_file,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5ef34d",
   "metadata": {},
   "source": [
    "### 분리한 파일에서 이미지 가져와서 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3558f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# input_json_path = f\"{output_file_basename}_merged.json\"  # JSON 파일 경로\n",
    "# output_dir = output_file_basename  # 이미지 저장 폴더\n",
    "# print(input_json_path)\n",
    "# # 저장 폴더가 없으면 생성\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "# with open(input_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     data = json.load(f)  # JSON 배열 데이터 로드\n",
    "\n",
    "def extract_image(element_content):\n",
    "    arr = []\n",
    "    for idx, item in enumerate(element_content):\n",
    "        _dict = dict()\n",
    "        if \"base64_encoding\" in item:\n",
    "            _dict = {\"base64_encoding\": item[\"base64_encoding\"]}\n",
    "            html_str = item[\"content\"][\"html\"]\n",
    "            soup = BeautifulSoup(html_str, \"html.parser\")\n",
    "            img_tags = soup.find_all(\"img\")\n",
    "            if img_tags:\n",
    "                base64_str = item[\"base64_encoding\"]\n",
    "                # base64 문자열 디코딩\n",
    "                image_data = base64.b64decode(base64_str)\n",
    "                image_path = os.path.join(f\"{basename}\\\\image_{idx}.png\")\n",
    "                for img_tag in soup.find_all(\"img\"):\n",
    "                    # 기존 속성 모두 제거\n",
    "                    img_tag.attrs.clear()\n",
    "                    # 원하는 텍스트를 src 속성에 넣기\n",
    "                    img_tag[\"src\"] = image_path\n",
    "                # 이미지 파일 저장\n",
    "                _dict[\"content_text\"] = str(img_tag)\n",
    "                with open(f\"{image_path}\", \"wb\") as img_file:\n",
    "                    img_file.write(image_data)\n",
    "                print(f\"Saved image: {image_path}\")\n",
    "            else:\n",
    "                _dict[\"content_text\"] = item[\"content\"][\"html\"]\n",
    "                print(\"이미지 태그가 없습니다.\")\n",
    "        else:\n",
    "            _dict[\"content_text\"] = item[\"content\"][\"html\"]\n",
    "            print(\"base64_encoding params 없습니다.\")\n",
    "\n",
    "        _dict[\"metadata\"] = {\"id\": item[\"id\"], \"page\": item[\"page\"]}\n",
    "        arr.append(_dict)\n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de17deca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base64_encoding params 없습니다.\n",
      "이미지 태그가 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "Saved image: .cache\\files\\seq2seq\\image_12.png\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "이미지 태그가 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "이미지 태그가 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "Saved image: .cache\\files\\seq2seq\\image_68.png\n",
      "Saved image: .cache\\files\\seq2seq\\image_69.png\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "이미지 태그가 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "Saved image: .cache\\files\\seq2seq\\image_75.png\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n",
      "base64_encoding params 없습니다.\n"
     ]
    }
   ],
   "source": [
    "_element_content = extract_image(element_content)\n",
    "# print(_element_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8384eb21",
   "metadata": {},
   "source": [
    "### content_text 에서 html 태그 가져와 tag에 img 존재하는 경우 모아서 summary 진행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f334ce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_system_prompt():\n",
    "    return \"\"\"You are an expert in extracting useful information from IMAGE.\n",
    "    In particular, you specialize in analyzing papers.\n",
    "    With a given image, your task is to extract key entities, summarize them, and write useful information that can be used later for retrieval.\"\"\"\n",
    "\n",
    "\n",
    "def get_user_image_template(previous_context, next_context, image_paths, id):\n",
    "    return f\"\"\"\n",
    "        Please consider the following text context—both the preceding paragraph and the following paragraph—along with the included image. \n",
    "        Based on this context, provide a simple and clear explanation of the image, suitable for someone with no prior background knowledge. \n",
    "        Use easy-to-understand language and focus on the main ideas that the image conveys in relation to the text.\n",
    "\n",
    "        [Preceding paragraph text]\n",
    "        {previous_context}\n",
    "        [Following paragraph text]\n",
    "        {next_context}\n",
    "\n",
    "\n",
    "        ###\n",
    "        Output Format:\n",
    "        <image>\n",
    "        <title>\n",
    "        <summary>\n",
    "        <entities> \n",
    "        <path> {image_paths} </path>\n",
    "        <id> {id} </id>\n",
    "        </image>\n",
    "\"\"\"\n",
    "\n",
    "def llm_summary_image(_element_content):\n",
    "    image_urls: list[str] = []\n",
    "    system_prompts: list[str] = []\n",
    "    user_prompts: list[str] = []\n",
    "    for idx, item in enumerate(_element_content):\n",
    "        previous_context = _element_content[idx - 1][\"content_text\"] if idx - 1 >= 0 else None\n",
    "        next_context = _element_content[idx + 1][\"content_text\"] if idx + 1 < len(_element_content) else None\n",
    "        id_str = item[\"metadata\"][\"id\"]\n",
    "        html_str = item[\"content_text\"]\n",
    "        soup = BeautifulSoup(html_str, \"html.parser\")\n",
    "        img_tag = soup.find(\"img\")\n",
    "        # equation_tag = soup.find(attrs={'data-category': 'equation'})\n",
    "        if img_tag:\n",
    "            src_path = img_tag.get(\"src\")\n",
    "            image_urls.append(src_path)\n",
    "            system_prompts.append(get_system_prompt())\n",
    "            user_prompts.append(\n",
    "                get_user_image_template(previous_context, next_context, html_str, id_str)\n",
    "            )\n",
    "    return image_urls,system_prompts,user_prompts\n",
    "\n",
    "# print(image_urls)\n",
    "# for u in user_prompts:\n",
    "#     print(u + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d085062",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_urls,system_prompts,user_prompts = llm_summary_image(_element_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3420378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.models import MultiModal\n",
    "\n",
    "llm = get_gpt()\n",
    "multimodal_llm = MultiModal(llm)\n",
    "answer = multimodal_llm.batch(\n",
    "    image_urls, system_prompts, user_prompts, display_image=False\n",
    ")\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52c491bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<image>\\n<title>Sequence-to-Sequence Model with LSTM Reading Input in Reverse</title>\\n<summary>\\nThis image shows how a special type of neural network called an LSTM processes an input sentence and generates an output sentence. The input sentence is \"ABC,\" and the model reads it in reverse order (starting from C, then B, then A). After reading the input, the model produces the output sentence \"WXYZ\" step by step. It stops generating words when it outputs a special end-of-sentence token labeled \"<EOS>.\" Reading the input backward helps the model learn better by making the connections between words easier to understand.\\n</summary>\\n<entities>\\n- LSTM (Long Short-Term Memory) neural network\\n- Input sentence: \"ABC\"\\n- Output sentence: \"WXYZ\"\\n- End-of-sentence token: \"<EOS>\"\\n- Reverse reading of input sequence\\n</entities>\\n<path> <img src=\".cache\\\\files\\\\seq2seq\\\\image_12.png\"/> </path>\\n<id> 12 </id>\\n</image>',\n",
       " '<image>\\n<title>Visualization of Relationship Sentences in a Two-Dimensional Space</title>\\n<summary>\\nThis image shows how different sentences about relationships between two people, Mary and John, are placed on a simple map with two axes. Each point represents a sentence describing feelings like admiration, love, or respect between Mary and John. The sentences are grouped based on their meaning and who the subject is (Mary or John). For example, sentences about Mary’s feelings toward John are close to each other, and sentences about John’s feelings toward Mary are grouped separately. This helps to understand how similar or different these relationship statements are from each other.\\n</summary>\\n<entities>\\n- Mary\\n- John\\n- Admiration\\n- Love\\n- Respect\\n- Sentence similarity\\n- Relationship analysis\\n</entities>\\n<path> <img src=\".cache\\\\files\\\\seq2seq\\\\image_68.png\"/> </path>\\n<id> 68 </id>\\n</image>',\n",
       " '<image>\\n<title>Visualization of Phrase Meaning Clusters Using LSTM Hidden States</title>\\n<summary>\\nThis image shows how different sentences with similar meanings are grouped together based on their word order. Each point represents a sentence, and sentences that mean similar things are placed close to each other. The colors and positions help us see that the model understands the order of words, which is important for meaning, unlike simpler methods that just look at which words are present without considering their order.\\n</summary>\\n<entities>\\n- LSTM hidden states: internal data from a type of neural network that processes sentences.\\n- PCA projection: a way to simplify complex data into two dimensions for easy visualization.\\n- Phrase clusters: groups of sentences with similar meanings.\\n- Word order: the sequence of words in a sentence, which affects meaning.\\n</entities>\\n<path> <img src=\".cache\\\\files\\\\seq2seq\\\\image_69.png\"/> </path>\\n<id> 69 </id>\\n</image>',\n",
       " '<image>\\n<title>Performance of LSTM vs Baseline in Translation Quality</title>\\n<summary>\\nThis image shows two graphs comparing how well a translation system called LSTM performs against a baseline system. The left graph measures translation quality based on sentence length, showing that LSTM does better or about the same as the baseline for sentences up to 35 words, with only a small drop in quality for very long sentences. The right graph measures performance based on how rare the words in the sentences are, showing that LSTM handles sentences with rare words better than the baseline, although performance decreases as words become less common. Overall, the LSTM system generally produces better translations, especially for longer sentences and those with rare words.\\n</summary>\\n<entities>\\n- LSTM (Long Short-Term Memory) translation system\\n- Baseline translation system\\n- BLEU score (a measure of translation quality)\\n- Sentence length\\n- Word frequency (common vs rare words)\\n</entities>\\n<path> <img src=\".cache\\\\files\\\\seq2seq\\\\image_75.png\"/> </path>\\n<id> 75 </id>\\n</image>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "687d49fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr = ['<image>\\n<title>도표 8. DDR4 가격 추이</title>\\n<summary>이 그래프는 2021년 1월부터 2025년 8월까지 DDR4 메모리 가격의 변동 추이를 보여준다. 그래프에는 현물 가격과 고정 가격 두 가지가 표시되어 있으며, 초기에는 가격이 약 5달러에서 시작해 점차 하락하여 2023년 중반에는 1.5달러 근처까지 떨어졌다. 이후 가격은 다시 상승세로 돌아서 2025년 8월에는 약 6달러에 도달하는 모습을 나타낸다.</summary>\\n<entities>\\n- DDR4 메모리\\n- 현물 가격\\n- 고정 가격\\n- 가격 변동 추이\\n- 기간: 2021년 1월 ~ 2025년 8월\\n- 단위: 달러 ($)\\n</entities>\\n<path> <img src=\"./test/hana/image_43.png\"/> </path>\\n<id> 43 </id>\\n</image>', '<image>\\n<title>도표 9. DDR5 가격 추이</title>\\n<summary>이 그래프는 2023년 1월부터 7월까지 DDR5 메모리 가격의 변동 추이를 보여준다. 두 가지 가격이 표시되어 있는데, 하나는 현물 가격(진한 녹색 선)이고 다른 하나는 고정 가격(회색 선)이다. 초기에는 가격이 약 5.5달러에서 시작해 3.8달러까지 하락했다가 이후 점차 상승하여 7달러 이상으로 증가하는 추세를 보인다. 고정 가격은 현물 가격보다 낮거나 비슷한 수준에서 움직이다가 후반부에 상승하는 경향을 나타낸다.</summary>\\n<entities>DDR5, 가격 추이, 현물 가격, 고정 가격, 2023년 1월~7월, 달러($)</entities>\\n<path> <img src=\"./test/hana/image_46.png\"/> </path>\\n<id>46</id>\\n</image>', '<image>\\n<title>도표 11. SLC 8Gb 가격 추이 (SLC 8Gb Price Trend)</title>\\n<summary>\\nThis line graph shows the price trend of SLC 8Gb memory chips over time, from January 2021 (21.1) to August 2025 (25.8). The vertical axis represents the price in dollars ($), ranging from 0 to 12. Two price lines are depicted: the 현물 가격 (spot price) in blue-green and the 고정 가격 (fixed price) in gray. The spot price starts around $5.5 in early 2021, rises to above $8 by early 2022, then declines and stabilizes around $6.5 until early 2024. After that, it sharply increases to nearly $10 by mid-2025 before slightly dropping. The fixed price remains relatively stable around $4.5 until late 2024, then drops sharply to below $2.5 before gradually rising again to about $3.5 by mid-2025.\\n</summary>\\n<entities>\\n- SLC 8Gb memory chip\\n- Price in USD ($)\\n- 현물 가격 (Spot Price)\\n- 고정 가격 (Fixed Price)\\n- Time period: January 2021 to August 2025\\n- Data sources: DRAMExchange, 하나증권 (Hana Securities)\\n</entities>\\n<path> <img src=\"./test/hana/image_50.png\"/> </path>\\n<id> 50 </id>\\n</image>', '<image>\\n<title>현물 가격과 고정 가격 추이 (2021년 1월 ~ 2025년 8월 예상)</title>\\n<summary>이 그래프는 2021년 1월부터 2025년 8월까지의 현물 가격과 고정 가격 변동 추이를 보여준다. 현물 가격은 2022년 초에 약 4달러까지 상승했다가 이후 점차 하락하여 2024년 초까지 2.7달러 근처에서 유지되다가 2025년부터 다시 상승하는 경향을 보인다. 고정 가격은 현물 가격보다 전반적으로 낮거나 비슷한 수준을 유지하며, 2024년 중반 이후부터는 현물 가격보다 낮은 수준에서 완만하게 상승하는 모습을 나타낸다.</summary>\\n<entities>\\n- 현물 가격 (Spot Price)\\n- 고정 가격 (Fixed Price)\\n- 기간: 2021년 1월 ~ 2025년 8월 (예상 포함)\\n- 단위: 달러($)\\n- 출처: DRAMExchange, 하나증권\\n</entities>\\n<path> <img src=\"./test/hana/image_52.png\"/> </path>\\n<id> 52 </id>\\n</image>', '<image>\\n<title>도표 12. 주요 DRAM 업체들의 연초 이후 주가 추이</title>\\n<summary>이 그래프는 2024년 12월 31일을 기준으로 주요 DRAM 업체들인 삼성전자, SK하이닉스, Micron, Nanya의 주가 변동 추이를 나타내고 있다. 각 업체의 주가는 2024년 1월부터 9월까지의 기간 동안 변동을 보였으며, 특히 Nanya의 주가가 가장 큰 폭으로 상승하여 210 이상을 기록한 반면, Micron은 상대적으로 안정적인 흐름을 보이며 100 근처에서 움직였다. 삼성전자와 SK하이닉스는 중간 수준의 상승세를 유지하였다.</summary>\\n<entities>\\n- 삼성전자 (Samsung Electronics)\\n- SK하이닉스 (SK Hynix)\\n- Micron\\n- Nanya\\n- 주가 추이 (Stock Price Trend)\\n- 기간: 2024년 1월 ~ 9월\\n</entities>\\n<path> <img src=\"./test/hana/image_57.png\"/> </path>\\n<id> 57 </id>\\n</image>']\n",
    "\n",
    "# html_str = data[0]  # 배열에서 첫번째 문자열 추출\n",
    "\n",
    "# input_json_path = f\"{output_file_basename}_merged_1.json\"  # JSON 파일 경로\n",
    "# with open(input_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     data = json.load(f)  # JSON 배열 데이터 로드\n",
    "\n",
    "def change_image_text_to_summary(answer,_element_content):\n",
    "    for html_str in answer:\n",
    "        soup = BeautifulSoup(html_str, \"html.parser\")\n",
    "        id_tag = soup.find(\"id\")  # id 태그 찾기\n",
    "        if id_tag:\n",
    "            id_value = id_tag.text.strip()  # 텍스트 추출 후 공백 제거\n",
    "            # data[id_value]['content_text'] = html_str\n",
    "            _element_content[int(id_value)][\"content_text\"] = html_str\n",
    "            print(\"id 값:\", id_value)\n",
    "        else:\n",
    "            print(\"id 태그를 찾을 수 없습니다.\")\n",
    "    return _element_content\n",
    "\n",
    "# with open(f\"{output_file_basename}_merged_2.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "#     json.dump(\n",
    "#         data,\n",
    "#         json_file,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "531e3293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id 값: 12\n",
      "id 값: 68\n",
      "id 값: 69\n",
      "id 값: 75\n"
     ]
    }
   ],
   "source": [
    "change_element_content = change_image_text_to_summary(answer,_element_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afab9d05",
   "metadata": {},
   "source": [
    "### 방정식 설명 추가 로직"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e65bb4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_system_prompt():\n",
    "    return \"\"\"You are an expert in extracting useful information from IMAGE.\n",
    "    In particular, you specialize in analyzing papers.\n",
    "    With a given image, your task is to extract key entities, summarize them, and write useful information that can be used later for retrieval.\"\"\"\n",
    "\n",
    "\n",
    "def get_user_equation_template(previous_context, next_context, equation, id):\n",
    "    return f\"\"\"\n",
    "        You are an AI assistant who analyzes formulas written in LaTeX format in the paper.\n",
    "        Explain the given equation easily with an appropriate example so that even a 5-year-old can understand it.\n",
    "        Explain the equation so that it can be understood without the equation by sufficiently solving it.\n",
    "\n",
    "        [Preceding paragraph text]\n",
    "        {previous_context}\n",
    "        \n",
    "        [Equation]\n",
    "        {equation}\n",
    "\n",
    "        [Following paragraph text]\n",
    "        {next_context}\n",
    "\n",
    "        \n",
    "        # Output Format:\n",
    "        <title>\n",
    "        <explain>\n",
    "        <examples> \n",
    "        <id>{id}</id>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# # 저장 폴더가 없으면 생성\n",
    "# input_json_path = f\"{output_file_basename}_merged_2.json\"  # JSON 파일 경로\n",
    "# # print(input_json_path)\n",
    "# with open(input_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     data = json.load(f)  # JSON 배열 데이터 로드\n",
    "\n",
    "def change_equation(change_element_content):\n",
    "    system_prompts: list[str] = []\n",
    "    user_prompts: list[str] = []\n",
    "\n",
    "    for idx, item in enumerate(change_element_content):\n",
    "        previous_context = change_element_content[idx - 1][\"content_text\"] if idx - 1 >= 0 else None\n",
    "        next_context = change_element_content[idx + 1][\"content_text\"] if idx + 1 < len(change_element_content) else None\n",
    "        id_str = item[\"metadata\"][\"id\"]\n",
    "        html_str = item[\"content_text\"]\n",
    "        soup = BeautifulSoup(html_str, \"html.parser\")\n",
    "        equation_tag = soup.find(attrs={\"data-category\": \"equation\"})\n",
    "        if equation_tag:\n",
    "            system_prompts.append(get_system_prompt())\n",
    "            user_prompts.append(\n",
    "                get_user_equation_template(\n",
    "                    previous_context, next_context, equation_tag.get_text(), id_str\n",
    "                )\n",
    "            )\n",
    "    return system_prompts,user_prompts\n",
    "# for u in user_prompts:\n",
    "#     print(u + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43955f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompts,user_prompts = change_equation(change_element_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8896212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_messages(system_prompt, user_prompt):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": user_prompt,\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fc19dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_llm_equation(system_prompts, user_prompts):\n",
    "    messages = []\n",
    "    for system_prompt, user_prompt in zip(system_prompts, user_prompts):\n",
    "        message = create_messages(system_prompt, user_prompt)\n",
    "        messages.append(message)\n",
    "    llm = get_gpt()\n",
    "    return llm.batch(messages)\n",
    "\n",
    "answer = request_llm_equation(system_prompts, user_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae2a4018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='<title>Understanding the Basic Recurrent Neural Network (RNN) Equation</title>\\n<explain>\\nThis equation describes how a Recurrent Neural Network (RNN) processes a sequence of inputs step-by-step to produce outputs. Imagine you have a list of things coming in one after another, like words in a sentence or notes in a song. At each step (time t), the RNN looks at the current input (xt) and also remembers what it learned from the previous step (ht-1). It combines these two pieces of information using some weights (which are like knobs that control how much each part matters) and then applies a special function called \"sigmoid\" that squashes the result into a number between 0 and 1. This new number (ht) is the RNN\\'s updated memory or understanding at step t. Finally, it uses this memory to produce an output (yt) by multiplying it with another set of weights. This way, the RNN can keep track of information over time and make predictions or decisions based on the whole sequence it has seen so far.\\n</explain>\\n<examples>\\nImagine you are listening to a story one word at a time. At each word, you remember what happened before and use that memory to understand the story better. For example, if the story says \"The cat sat on the...\", when you hear \"mat,\" you remember the earlier words to know the sentence makes sense. The RNN works similarly: it takes the current word (input), remembers the previous words (hidden state), and then guesses what comes next (output). The \"sigmoid\" function helps decide how much of the old memory to keep and how much new information to add, like deciding how much of the story you remember at each step.\\n</examples>\\n<id>22</id>', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 371, 'prompt_tokens': 433, 'total_tokens': 804, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'id': 'chatcmpl-CUuAHhwk6GiE7M6QSL27CFyK0NtJQ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--bdeb3a97-c4f7-47dc-b8f5-c1e1796a67f1-0', usage_metadata={'input_tokens': 433, 'output_tokens': 371, 'total_tokens': 804, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " AIMessage(content='<title>Understanding the Conditional Probability Equation in LSTM Sequence Modeling</title>\\n\\n<explain>\\nThis equation is about how an LSTM (a type of smart computer program) predicts a sequence of outputs (like words or actions) based on a sequence of inputs. Imagine you have a story made of several sentences (inputs), and you want the computer to guess the next story (outputs) based on what it learned from the first one.\\n\\nThe equation says: to find the chance (probability) of the whole output sequence happening given the input sequence, we look at each output step one by one. For each step, the chance of that output depends on two things: a summary of the entire input sequence (called \"v\") and all the outputs that came before it.\\n\\nIn simpler terms, the computer first reads the whole input and remembers it as a summary. Then, it predicts the first output using that summary. Next, it predicts the second output using the summary plus the first output it just predicted, and so on, until it predicts the entire output sequence.\\n\\nThis step-by-step prediction is like telling a story where each new sentence depends on what was said before and the main idea of the original story.\\n\\n</explain>\\n\\n<examples>\\nImagine you have a box of colored beads (inputs) arranged in a certain order. You want to make a necklace (outputs) that matches the pattern of the beads in the box.\\n\\n1. First, you look at all the beads in the box and remember the overall color pattern (this is the summary \"v\").\\n2. Then, you pick the first bead for your necklace based on the pattern you remembered.\\n3. Next, you pick the second bead based on the pattern and the first bead you already chose.\\n4. You keep doing this, choosing each bead by looking at the pattern and the beads you\\'ve already put on the necklace.\\n\\nBy doing this, you create a necklace that matches the original bead pattern, one bead at a time.\\n\\n</examples>\\n\\n<id>26</id>', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 411, 'prompt_tokens': 455, 'total_tokens': 866, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'id': 'chatcmpl-CUuAG5VxnaIX2yrM7T0qPpjMjXc2G', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--f8230c3e-2492-4270-b4e7-967251829c6f-0', usage_metadata={'input_tokens': 455, 'output_tokens': 411, 'total_tokens': 866, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " AIMessage(content='<title>Understanding the Training Objective for Translation in LSTM Models</title>\\n<explain>\\nThis equation describes how the model learns to translate sentences. Imagine you have many pairs of sentences: one in the original language (source sentence S) and one in the translated language (correct translation T). The model tries to get better at predicting the correct translation by looking at all these pairs.\\n\\nThe equation says: \"Take all the sentence pairs in the training set, and for each pair, calculate how likely the model thinks the correct translation is, then take the logarithm of that likelihood.\" After doing this for all pairs, average these values by dividing by the total number of pairs. The goal during training is to make this average as big as possible, which means the model is getting better at predicting correct translations.\\n\\nIn simple terms, the model is practicing on many examples, and it tries to be as confident as possible that its translations are right.\\n\\n</explain>\\n<examples>\\nImagine you have a box of 100 flashcards, each with a word in English on one side and the correct word in Spanish on the other. You want to teach a robot to say the Spanish word when you show it the English word.\\n\\n- For each flashcard, the robot guesses a Spanish word.\\n- If the guess is correct, the robot gets a high score; if not, a low score.\\n- The robot tries to improve by looking at all 100 flashcards and adjusting itself to get better guesses overall.\\n- The equation is like measuring how good the robot is on average across all flashcards.\\n- The robot keeps practicing until it guesses the Spanish words correctly most of the time.\\n\\nThis is exactly what the model does with sentences instead of single words.\\n</examples>\\n<id>38</id>', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 361, 'prompt_tokens': 316, 'total_tokens': 677, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'id': 'chatcmpl-CUuAGEEIEiqM5GoSCEvyuWtYsCkkV', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--16884b05-6553-4cd9-b450-33f757dc2dd8-0', usage_metadata={'input_tokens': 316, 'output_tokens': 361, 'total_tokens': 677, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " AIMessage(content='<title>Understanding How We Choose the Best Translation</title>\\n<explain>\\nImagine you have a sentence in one language, and you want to find the best way to say it in another language. The equation is about picking the translation that is most likely to be correct. \\n\\nHere, we have a set of possible translations, and we want to choose the one that has the highest chance of being the right translation given the original sentence. The symbol \"arg max\" means \"find the thing that makes this probability the biggest.\" So, we look at all possible translations and pick the one that the model thinks is most likely.\\n\\nIn simple words, it\\'s like having many guesses for how to say something, and you pick the guess that seems the best based on what you\\'ve learned.\\n</explain>\\n<examples>\\nImagine you have a box of different colored balls, and you want to pick the ball that is most likely to be your favorite color. You look at all the balls and choose the one that you think is the favorite color with the highest chance. Similarly, the model looks at all possible translations and picks the one it thinks is the best.\\n\\nFor example, if the original sentence is \"Hello,\" the model might consider \"Hola,\" \"Bonjour,\" and \"Ciao\" as possible translations. It will pick the one it thinks is most likely correct based on what it learned during training.\\n</examples>\\n<id>40</id>', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 290, 'prompt_tokens': 262, 'total_tokens': 552, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'id': 'chatcmpl-CUuAHFpr0BP2E5Yji2HIxtV2BY2uc', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--12610f70-d533-4528-b4b8-fcd908d059af-0', usage_metadata={'input_tokens': 262, 'output_tokens': 290, 'total_tokens': 552, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940f3965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for r in response:\n",
    "#     print(r)\n",
    "    # print(r.content +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea63976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_json_path = f\"{output_file_basename}_merged_2.json\"  # JSON 파일 경로\n",
    "# with open(input_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     data = json.load(f)  # JSON 배열 데이터 로드\n",
    "\n",
    "def add_eqaution_description(answer,change_element_content):\n",
    "    for r in answer:\n",
    "        html_str = r.content\n",
    "        print(r)\n",
    "        soup = BeautifulSoup(html_str, \"html.parser\")\n",
    "        id_tag = soup.find(\"id\")  # id 태그 찾기\n",
    "        if id_tag:\n",
    "            id_value = id_tag.text.strip()  # 텍스트 추출 후 공백 제거\n",
    "            new_p = soup.new_tag(\"p\")\n",
    "            new_p.string = html_str\n",
    "            change_element_content[int(id_value)][\"content_text\"] = (\n",
    "                change_element_content[int(id_value)][\"content_text\"] + html_str\n",
    "            )\n",
    "            print(\"id 값:\", id_value)\n",
    "        else:\n",
    "            print(\"id 태그를 찾을 수 없습니다.\")\n",
    "    return change_element_content\n",
    "\n",
    "\n",
    "# with open(\n",
    "#     f\"{output_file_basename}_merged_final.json\", \"w\", encoding=\"utf-8\"\n",
    "# ) as json_file:\n",
    "#     json.dump(\n",
    "#         data,\n",
    "#         json_file,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62a5e22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<title>Understanding the Basic Recurrent Neural Network (RNN) Equation</title>\\n<explain>\\nThis equation describes how a Recurrent Neural Network (RNN) processes a sequence of inputs step-by-step to produce outputs. Imagine you have a list of things coming in one after another, like words in a sentence or notes in a song. At each step (time t), the RNN looks at the current input (xt) and also remembers what it learned from the previous step (ht-1). It combines these two pieces of information using some weights (which are like knobs that control how much each part matters) and then applies a special function called \"sigmoid\" that squashes the result into a number between 0 and 1. This new number (ht) is the RNN\\'s updated memory or understanding at step t. Finally, it uses this memory to produce an output (yt) by multiplying it with another set of weights. This way, the RNN can keep track of information over time and make predictions or decisions based on the whole sequence it has seen so far.\\n</explain>\\n<examples>\\nImagine you are listening to a story one word at a time. At each word, you remember what happened before and use that memory to understand the story better. For example, if the story says \"The cat sat on the...\", when you hear \"mat,\" you remember the earlier words to know the sentence makes sense. The RNN works similarly: it takes the current word (input), remembers the previous words (hidden state), and then guesses what comes next (output). The \"sigmoid\" function helps decide how much of the old memory to keep and how much new information to add, like deciding how much of the story you remember at each step.\\n</examples>\\n<id>22</id>' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 371, 'prompt_tokens': 433, 'total_tokens': 804, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'id': 'chatcmpl-CUuAHhwk6GiE7M6QSL27CFyK0NtJQ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--bdeb3a97-c4f7-47dc-b8f5-c1e1796a67f1-0' usage_metadata={'input_tokens': 433, 'output_tokens': 371, 'total_tokens': 804, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "id 값: 22\n",
      "content='<title>Understanding the Conditional Probability Equation in LSTM Sequence Modeling</title>\\n\\n<explain>\\nThis equation is about how an LSTM (a type of smart computer program) predicts a sequence of outputs (like words or actions) based on a sequence of inputs. Imagine you have a story made of several sentences (inputs), and you want the computer to guess the next story (outputs) based on what it learned from the first one.\\n\\nThe equation says: to find the chance (probability) of the whole output sequence happening given the input sequence, we look at each output step one by one. For each step, the chance of that output depends on two things: a summary of the entire input sequence (called \"v\") and all the outputs that came before it.\\n\\nIn simpler terms, the computer first reads the whole input and remembers it as a summary. Then, it predicts the first output using that summary. Next, it predicts the second output using the summary plus the first output it just predicted, and so on, until it predicts the entire output sequence.\\n\\nThis step-by-step prediction is like telling a story where each new sentence depends on what was said before and the main idea of the original story.\\n\\n</explain>\\n\\n<examples>\\nImagine you have a box of colored beads (inputs) arranged in a certain order. You want to make a necklace (outputs) that matches the pattern of the beads in the box.\\n\\n1. First, you look at all the beads in the box and remember the overall color pattern (this is the summary \"v\").\\n2. Then, you pick the first bead for your necklace based on the pattern you remembered.\\n3. Next, you pick the second bead based on the pattern and the first bead you already chose.\\n4. You keep doing this, choosing each bead by looking at the pattern and the beads you\\'ve already put on the necklace.\\n\\nBy doing this, you create a necklace that matches the original bead pattern, one bead at a time.\\n\\n</examples>\\n\\n<id>26</id>' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 411, 'prompt_tokens': 455, 'total_tokens': 866, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'id': 'chatcmpl-CUuAG5VxnaIX2yrM7T0qPpjMjXc2G', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--f8230c3e-2492-4270-b4e7-967251829c6f-0' usage_metadata={'input_tokens': 455, 'output_tokens': 411, 'total_tokens': 866, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "id 값: 26\n",
      "content='<title>Understanding the Training Objective for Translation in LSTM Models</title>\\n<explain>\\nThis equation describes how the model learns to translate sentences. Imagine you have many pairs of sentences: one in the original language (source sentence S) and one in the translated language (correct translation T). The model tries to get better at predicting the correct translation by looking at all these pairs.\\n\\nThe equation says: \"Take all the sentence pairs in the training set, and for each pair, calculate how likely the model thinks the correct translation is, then take the logarithm of that likelihood.\" After doing this for all pairs, average these values by dividing by the total number of pairs. The goal during training is to make this average as big as possible, which means the model is getting better at predicting correct translations.\\n\\nIn simple terms, the model is practicing on many examples, and it tries to be as confident as possible that its translations are right.\\n\\n</explain>\\n<examples>\\nImagine you have a box of 100 flashcards, each with a word in English on one side and the correct word in Spanish on the other. You want to teach a robot to say the Spanish word when you show it the English word.\\n\\n- For each flashcard, the robot guesses a Spanish word.\\n- If the guess is correct, the robot gets a high score; if not, a low score.\\n- The robot tries to improve by looking at all 100 flashcards and adjusting itself to get better guesses overall.\\n- The equation is like measuring how good the robot is on average across all flashcards.\\n- The robot keeps practicing until it guesses the Spanish words correctly most of the time.\\n\\nThis is exactly what the model does with sentences instead of single words.\\n</examples>\\n<id>38</id>' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 361, 'prompt_tokens': 316, 'total_tokens': 677, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'id': 'chatcmpl-CUuAGEEIEiqM5GoSCEvyuWtYsCkkV', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--16884b05-6553-4cd9-b450-33f757dc2dd8-0' usage_metadata={'input_tokens': 316, 'output_tokens': 361, 'total_tokens': 677, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "id 값: 38\n",
      "content='<title>Understanding How We Choose the Best Translation</title>\\n<explain>\\nImagine you have a sentence in one language, and you want to find the best way to say it in another language. The equation is about picking the translation that is most likely to be correct. \\n\\nHere, we have a set of possible translations, and we want to choose the one that has the highest chance of being the right translation given the original sentence. The symbol \"arg max\" means \"find the thing that makes this probability the biggest.\" So, we look at all possible translations and pick the one that the model thinks is most likely.\\n\\nIn simple words, it\\'s like having many guesses for how to say something, and you pick the guess that seems the best based on what you\\'ve learned.\\n</explain>\\n<examples>\\nImagine you have a box of different colored balls, and you want to pick the ball that is most likely to be your favorite color. You look at all the balls and choose the one that you think is the favorite color with the highest chance. Similarly, the model looks at all possible translations and picks the one it thinks is the best.\\n\\nFor example, if the original sentence is \"Hello,\" the model might consider \"Hola,\" \"Bonjour,\" and \"Ciao\" as possible translations. It will pick the one it thinks is most likely correct based on what it learned during training.\\n</examples>\\n<id>40</id>' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 290, 'prompt_tokens': 262, 'total_tokens': 552, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'id': 'chatcmpl-CUuAHFpr0BP2E5Yji2HIxtV2BY2uc', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--12610f70-d533-4528-b4b8-fcd908d059af-0' usage_metadata={'input_tokens': 262, 'output_tokens': 290, 'total_tokens': 552, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "id 값: 40\n"
     ]
    }
   ],
   "source": [
    "final_content = add_eqaution_description(answer,change_element_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96746e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f\"{output_file_basename}_merged_final.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     data = json.load(f)  # JSON 배열 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c588b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_user_request() -> ChatPromptTemplate:\n",
    "    prompt = \"\"\"\n",
    "    You are an expert academic explainer tasked with simplifying difficult and complex research papers. Your responses must be:\n",
    "\n",
    "    - **Professional and detailed**: Use precise technical terms but always explain them thoroughly.\n",
    "    - **Clear and accessible**: Include sufficient explanations and relevant examples so that even a young child can understand.\n",
    "    - **Formatted in Markdown**: Use headings, lists, and emphasis where appropriate for readability.\n",
    "    - **Source-aware**: If the original document references specific pages or sections, always include the source page number or reference.\n",
    "    - **Image-inclusive**: If the input contains image or figure tags, display the image path or URL alongside your explanation.\n",
    "\n",
    "\n",
    "    When given a text input from a paper, first break down complex concepts step-by-step, \n",
    "    illustrate with examples or analogies, and clearly indicate the source pages if applicable. \n",
    "    Once text is entered in the paper, we first break down complex concepts step by step,\n",
    "    Explain with examples or parables and, if applicable, clearly display the source page.\n",
    "    Provides a final cohesive summary in Markdown format.\n",
    "    To represent the equation (x_1, \\\\cdots, x_T), switch to $(x_1, \\\\cdots, x_T)$ sentence and output it\n",
    "    If <img src='example'/> exists in the document you refer to, output the src path as it is\n",
    "    \n",
    "\n",
    "    ---\n",
    "\n",
    "    **Example Usage:**\n",
    "\n",
    "    Input: \"Explain the LSTM model from page 5 that includes 4 layers and GPU parallelization. Include any figures if present.\"\n",
    "\n",
    "    Output: *(Detailed, clear Markdown explanation with examples, source page 5 cited, and image paths if any).* \n",
    "    ---\n",
    "\n",
    "    Input: \"Please explain the key mechanism equation\n",
    "\"\n",
    "    Output: $(x_1, \\\\cdots, x_T)$\n",
    "    ---\n",
    "\n",
    "    **Important:**\n",
    "    Make sure to answer in Korean except for the image path and equation\n",
    " \n",
    "    \n",
    "    ** User Request:**\n",
    "    {question}\n",
    "\n",
    "    ** context :**\n",
    "    {context}    \n",
    "\"\"\"\n",
    "    return ChatPromptTemplate.from_template(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64cce8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 이 논문이 해결하고자 하는 문제\n",
      "\n",
      "이 논문은 **LSTM(Long Short-Term Memory)** 모델을 사용하여 **입력 시퀀스**에 대응하는 **출력 시퀀스의 조건부 확률**을 추정하는 문제를 다루고 있습니다. \n",
      "\n",
      "---\n",
      "\n",
      "## 문제 설명\n",
      "\n",
      "- 입력 시퀀스 $(x_1, \\cdots, x_T)$가 주어졌을 때, 이에 대응하는 출력 시퀀스 $(y_1, \\cdots, y_{T'})$의 확률 $p(y_1, \\cdots, y_{T'} | x_1, \\cdots, x_T)$를 계산하는 것이 목표입니다.  \n",
      "- 여기서 입력 시퀀스와 출력 시퀀스의 길이 $T$와 $T'$는 서로 다를 수 있습니다.  \n",
      "- LSTM은 입력 시퀀스를 고정된 차원의 벡터 표현 $v$로 변환한 뒤, 이 벡터를 초기 상태로 하여 출력 시퀀스의 확률을 계산합니다.  \n",
      "\n",
      "---\n",
      "\n",
      "## 왜 이 문제가 중요한가?\n",
      "\n",
      "- 시퀀스 데이터(예: 문장, 음성, 동작 등)는 길이가 다양하고, 각 시퀀스의 각 요소가 서로 복잡하게 연결되어 있습니다.  \n",
      "- 전통적인 모델들은 긴 시퀀스에서 앞부분과 뒷부분 사이의 관계를 잘 포착하지 못하는 문제가 있습니다.  \n",
      "- LSTM은 이런 긴 시퀀스 내의 **장기 의존성(long-term dependencies)** 문제를 해결할 수 있는 구조로, 입력과 출력 시퀀스 간의 복잡한 관계를 모델링하는 데 적합합니다.  \n",
      "\n",
      "---\n",
      "\n",
      "## 쉽게 이해하기 위한 비유\n",
      "\n",
      "- **입력 시퀀스**를 여러 문장으로 이루어진 이야기라고 생각해보세요.  \n",
      "- LSTM은 이 이야기를 읽고, 이야기의 핵심 내용을 요약한 메모(벡터 $v$)를 만듭니다.  \n",
      "- 그리고 이 메모를 바탕으로 새로운 이야기를 한 문장씩 차례대로 만들어 나갑니다.  \n",
      "- 각 문장은 이전에 만든 문장들과 이야기의 핵심 메모를 참고해서 만들어집니다.  \n",
      "\n",
      "---\n",
      "\n",
      "## 핵심 수식\n",
      "\n",
      "논문에서 제시한 조건부 확률은 다음과 같습니다 (3페이지):\n",
      "\n",
      "$$\n",
      "p(y_{1}, \\cdots, y_{T^{\\prime}} | x_{1}, \\cdots, x_{T}) = \\prod_{t=1}^{T^{\\prime}} p(y_t | v, y_1, \\cdots, y_{t-1})\n",
      "$$\n",
      "\n",
      "- 즉, 출력 시퀀스의 각 단어 $y_t$는 입력 시퀀스의 요약 $v$와 이전에 생성된 단어들 $y_1, \\cdots, y_{t-1}$에 조건부로 의존합니다.  \n",
      "- 이 수식은 LSTM이 어떻게 순차적으로 출력을 생성하는지 보여줍니다.  \n",
      "\n",
      "---\n",
      "\n",
      "## 요약\n",
      "\n",
      "| 항목 | 설명 |\n",
      "|---|---|\n",
      "| **해결하고자 하는 문제** | 입력 시퀀스에 대해 대응하는 출력 시퀀스의 조건부 확률을 정확히 추정하는 것 |\n",
      "| **주요 도전 과제** | 입력과 출력 시퀀스 길이가 다르고, 긴 시퀀스 내 장기 의존성을 학습하는 어려움 |\n",
      "| **해결 방법** | LSTM을 사용해 입력 시퀀스를 고정 차원 벡터로 요약하고, 이를 초기 상태로 출력 시퀀스를 생성 |\n",
      "| **적용 분야 예시** | 기계 번역, 음성 인식, 텍스트 생성 등 시퀀스-투-시퀀스 문제 |\n",
      "\n",
      "---\n",
      "\n",
      "## 참고 문헌 및 페이지\n",
      "\n",
      "- 문제 정의 및 수식: 3페이지 (id: 25, 26)  \n",
      "- 관련 설명: 3페이지  \n",
      "- 추가 설명 및 현상 분석: 4페이지 (id: 46)  \n",
      "\n",
      "---\n",
      "\n",
      "필요하면 더 구체적인 부분이나 다른 섹션도 설명해 드릴 수 있습니다!\n"
     ]
    }
   ],
   "source": [
    "user_input = \"이논문이 해결하고자하는 문제가머야 ?\"\n",
    "docs = []\n",
    "for d in final_content:\n",
    "    doc = Document(page_content=d[\"content_text\"], metadata=d[\"metadata\"])\n",
    "    docs.append(doc)\n",
    "\n",
    "faiss_retriever = get_retriever(docs)\n",
    "bm25_retriever = get_bm25_retriever(docs)\n",
    "esenmble_retriever = get_esenmble_retriever(faiss_retriever, bm25_retriever)\n",
    "rerank = get_reranker(esenmble_retriever, user_input)\n",
    "reorder_context = reorder_documents(rerank)\n",
    "prompt = get_prompt_user_request()\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"question\": user_input, \"context\": reorder_context})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9abe3b",
   "metadata": {},
   "source": [
    "# 이 논문이 해결하고자 하는 문제\n",
    "\n",
    "이 논문은 **LSTM(Long Short-Term Memory)** 모델을 사용하여 **입력 시퀀스** $(x_1, \\cdots, x_T)$에 대해 길이가 다를 수 있는 **출력 시퀀스** $(y_1, \\cdots, y_{T'})$의 조건부 확률 $p(y_1, \\cdots, y_{T'} | x_1, \\cdots, x_T)$를 추정하는 문제를 다룹니다. \n",
    "\n",
    "---\n",
    "\n",
    "## 문제의 핵심\n",
    "\n",
    "- **입력 시퀀스와 출력 시퀀스의 길이가 다를 수 있음**  \n",
    "  예를 들어, 영어 문장(입력)이 10단어이고, 그에 대응하는 프랑스어 문장(출력)이 12단어일 수 있습니다.  \n",
    "- **입력 시퀀스를 고정된 차원의 벡터로 요약하는 것**  \n",
    "  LSTM은 입력 시퀀스 전체를 읽고, 마지막 숨겨진 상태(hidden state)를 통해 이 시퀀스를 하나의 벡터 $v$로 요약합니다.  \n",
    "- **요약된 벡터 $v$를 바탕으로 출력 시퀀스를 생성**  \n",
    "  이 벡터 $v$를 초기 상태로 하여, LSTM 언어 모델(LM)이 출력 시퀀스의 각 단어를 순차적으로 예측합니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 문제를 쉽게 이해하기 위한 비유\n",
    "\n",
    "- **입력 시퀀스**: 여러 장의 그림이 순서대로 놓여 있는 책  \n",
    "- **출력 시퀀스**: 그 그림들을 보고 새롭게 이야기를 만드는 것  \n",
    "- LSTM은 먼저 책 전체를 쭉 읽고(입력 시퀀스 요약), 그 내용을 머릿속에 저장합니다(벡터 $v$).  \n",
    "- 그 다음, 머릿속에 저장한 내용을 바탕으로 한 문장씩 차례대로 이야기를 만들어 나갑니다(출력 시퀀스 생성).\n",
    "\n",
    "---\n",
    "\n",
    "## 수식으로 표현된 문제 (출처: 3페이지)\n",
    "\n",
    "$$\n",
    "p(y_{1}, \\cdots, y_{T'} | x_{1}, \\cdots, x_{T}) = \\prod_{t=1}^{T'} p(y_t | v, y_1, \\cdots, y_{t-1})\n",
    "$$\n",
    "\n",
    "- 이 식은 출력 시퀀스의 각 단어 $y_t$가,  \n",
    "  - 입력 시퀀스 전체를 요약한 벡터 $v$와  \n",
    "  - 지금까지 생성된 출력 단어들 $y_1, \\cdots, y_{t-1}$에 조건부로 의존한다는 의미입니다.  \n",
    "- 즉, LSTM은 이전에 생성한 단어들과 입력 시퀀스 요약 정보를 모두 고려하여 다음 단어를 예측합니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 요약\n",
    "\n",
    "| 문제점 | 설명 |\n",
    "|--------|-------|\n",
    "| 입력과 출력 시퀀스 길이 불일치 | 입력과 출력의 길이가 다를 수 있어, 단순한 매핑이 어려움 |\n",
    "| 시퀀스 전체를 고정된 크기로 요약 | LSTM의 마지막 숨겨진 상태를 사용해 입력 시퀀스를 하나의 벡터로 요약 |\n",
    "| 조건부 확률 추정 | 요약 벡터와 이전 출력 단어들을 바탕으로 다음 단어를 예측하는 확률 모델 구축 |\n",
    "\n",
    "이 논문은 위 문제를 해결하기 위해 LSTM 기반의 시퀀스-투-시퀀스(sequence-to-sequence) 모델을 제안하고, 이를 통해 자연어 번역 등 다양한 시퀀스 변환 문제에 적용하고자 합니다.\n",
    "\n",
    "---\n",
    "\n",
    "# 참고  \n",
    "- 문제 정의 및 수식: **3페이지**  \n",
    "- 수식 설명 및 예시: 3페이지 (id: 26)  \n",
    "\n",
    "---\n",
    "\n",
    "필요하면 추가 설명이나 다른 부분도 알려주세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d0172f",
   "metadata": {},
   "source": [
    "# 이 논문이 해결하고자 하는 문제\n",
    "\n",
    "이 논문은 **LSTM(Long Short-Term Memory)** 모델을 사용하여 **입력 시퀀스**에 대응하는 **출력 시퀀스의 조건부 확률**을 추정하는 문제를 다루고 있습니다. \n",
    "\n",
    "---\n",
    "\n",
    "## 문제 설명\n",
    "\n",
    "- 입력 시퀀스 $(x_1, \\cdots, x_T)$가 주어졌을 때, 이에 대응하는 출력 시퀀스 $(y_1, \\cdots, y_{T'})$의 확률 $p(y_1, \\cdots, y_{T'} | x_1, \\cdots, x_T)$를 계산하는 것이 목표입니다.  \n",
    "- 여기서 입력 시퀀스와 출력 시퀀스의 길이 $T$와 $T'$는 서로 다를 수 있습니다.  \n",
    "- LSTM은 입력 시퀀스를 고정된 차원의 벡터 표현 $v$로 변환한 뒤, 이 벡터를 초기 상태로 하여 출력 시퀀스의 확률을 계산합니다.  \n",
    "\n",
    "---\n",
    "\n",
    "## 왜 이 문제가 중요한가?\n",
    "\n",
    "- 시퀀스 데이터(예: 문장, 음성, 동작 등)는 길이가 다양하고, 각 시퀀스의 각 요소가 서로 복잡하게 연결되어 있습니다.  \n",
    "- 전통적인 모델들은 긴 시퀀스에서 앞부분과 뒷부분 사이의 관계를 잘 포착하지 못하는 문제가 있습니다.  \n",
    "- LSTM은 이런 긴 시퀀스 내의 **장기 의존성(long-term dependencies)** 문제를 해결할 수 있는 구조로, 입력과 출력 시퀀스 간의 복잡한 관계를 모델링하는 데 적합합니다.  \n",
    "\n",
    "---\n",
    "\n",
    "## 쉽게 이해하기 위한 비유\n",
    "\n",
    "- **입력 시퀀스**를 여러 문장으로 이루어진 이야기라고 생각해보세요.  \n",
    "- LSTM은 이 이야기를 읽고, 이야기의 핵심 내용을 요약한 메모(벡터 $v$)를 만듭니다.  \n",
    "- 그리고 이 메모를 바탕으로 새로운 이야기를 한 문장씩 차례대로 만들어 나갑니다.  \n",
    "- 각 문장은 이전에 만든 문장들과 이야기의 핵심 메모를 참고해서 만들어집니다.  \n",
    "\n",
    "---\n",
    "\n",
    "## 핵심 수식\n",
    "\n",
    "논문에서 제시한 조건부 확률은 다음과 같습니다 (3페이지):\n",
    "\n",
    "$$\n",
    "p(y_{1}, \\cdots, y_{T^{\\prime}} | x_{1}, \\cdots, x_{T}) = \\prod_{t=1}^{T^{\\prime}} p(y_t | v, y_1, \\cdots, y_{t-1})\n",
    "$$\n",
    "\n",
    "- 즉, 출력 시퀀스의 각 단어 $y_t$는 입력 시퀀스의 요약 $v$와 이전에 생성된 단어들 $y_1, \\cdots, y_{t-1}$에 조건부로 의존합니다.  \n",
    "- 이 수식은 LSTM이 어떻게 순차적으로 출력을 생성하는지 보여줍니다.  \n",
    "\n",
    "---\n",
    "\n",
    "## 요약\n",
    "\n",
    "| 항목 | 설명 |\n",
    "|---|---|\n",
    "| **해결하고자 하는 문제** | 입력 시퀀스에 대해 대응하는 출력 시퀀스의 조건부 확률을 정확히 추정하는 것 |\n",
    "| **주요 도전 과제** | 입력과 출력 시퀀스 길이가 다르고, 긴 시퀀스 내 장기 의존성을 학습하는 어려움 |\n",
    "| **해결 방법** | LSTM을 사용해 입력 시퀀스를 고정 차원 벡터로 요약하고, 이를 초기 상태로 출력 시퀀스를 생성 |\n",
    "| **적용 분야 예시** | 기계 번역, 음성 인식, 텍스트 생성 등 시퀀스-투-시퀀스 문제 |\n",
    "\n",
    "---\n",
    "\n",
    "## 참고 문헌 및 페이지\n",
    "\n",
    "- 문제 정의 및 수식: 3페이지 (id: 25, 26)  \n",
    "- 관련 설명: 3페이지  \n",
    "- 추가 설명 및 현상 분석: 4페이지 (id: 46)  \n",
    "\n",
    "---\n",
    "\n",
    "필요하면 더 구체적인 부분이나 다른 섹션도 설명해 드릴 수 있습니다!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-3tRZr1Gy-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
